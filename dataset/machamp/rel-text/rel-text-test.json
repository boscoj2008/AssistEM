[
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Formation and evolution of galaxies have been a central driving force in the studies of galaxies and cosmology. Recent studies provided a global picture of cosmic star formation history. However, what drives the evolution of star formation activities in galaxies has long been a matter of debate. The key factor of the star formation is the transition of hydrogen from atomic to molecular state, since the star formation is associated with the molecular phase. This transition is also strongly coupled with chemical evolution, because dust grains, i.e., tiny solid particles of heavy elements, play a critical role in molecular formation. Therefore, a comprehensive understanding of neutral-molecular gas transition, star formation and chemical enrichment is necessary to clarify the galaxy formation and evolution.  Entity 2: title toto , we 're not in kansas anymore : on transitioning from research to the real ( invited industrial talk ) authors michael j. carey venue very large data bases year 2000 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: It is a cliche that the Internet has revolutionized many aspects of life in the past decade. Scientific publishing is but one of the many enterprises that have been impacted by the connectivity and high bandwidth afforded by the World Wide Web. Most scientific journals now have a web presence. That said, it is still remarkable the degree to which ACM in general and TODS in particular have embraced the unique capabilities of the web to aid in the propagation of knowledge. Here I summarize the disparate and broad ways in which TODS utilizes the web, in all phases of publishing.  Entity 2: title edgar f. codd : a tribute and personal memoir authors c. j. date venue acm sigmod record year 2003 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In late May, 2008, a group of database researchers, architects, users and pundits met at the Claremont Resort in Berkeley, California to discuss the state of the research field and its impacts on practice. This was the seventh meeting of this sort in twenty years, and was distinguished by a broad consensus that we are at a turning point in the history of the field, due both to an explosion of data and usage scenarios, and to major shifts in computing hardware and platforms. Given these forces, we are at a time of opportunity for research impact, with an unusually large potential for influential results across computing, the sciences and society. This report details that discussion, and highlights the group's consensus view of new focus areas, including new database engine architectures, declarative programming languages, the interplay of structured and unstructured data, cloud data services, and mobile and virtual worlds. We also report on discussions of the community's growth, including suggestions for changes in community processes to move the research agenda forward, and to enhance impact on a broader audience.  Entity 2: title database research : achievements and opportunities into the 1st century authors avi silberschatz , mike stonebraker , jeff ullman venue acm sigmod record year 1996 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The MITRE Corporation provides technical assistance, system engineering, and acquisition support to large organizations, especially U.S. Government agencies. We help our customers to plan complex systems based on emerging technologies, and to implement systems based on commercial-off-the-shelf products. In MITRE's research program, instead of emphasizing concerns of DBMS or CASE vendors, our research emphasizes the issues of organizations who need to use such products. For example, we favor areas where we can build over commercial products, rather than changing their internals.Data management at MITRE goes beyond research, to include technology transition, system engineering, product evaluation, prototypes, tutorials, advice on customers' strategic directions, and participation in standards efforts. We use prototyping to illustrate potential improvements in customer systems, to understand vendors' capabilities, or both. There are close connections with efforts in object management, real-time systems, reengineering, artificial intelligence, and security.  Entity 2: title data management research at the mitre corporation authors arnon rosenthal , len seligman , catherine mccollum , barbara blaustein , bhavani thuraisingham , edward lafferty venue acm sigmod record year 1995 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: A variety of developments combine to highlight the need for respecting order when manipulating relations. For example, new functionality is being added to SQL to support OLAP-style querying in which order is frequently an important aspect. The set- or multiset-based frameworks for query optimization that are currently being taught to database students are increasingly inadequate.This paper presents a foundation for query optimization that extends existing frameworks to also capture ordering. A list-based relational algebra is provided along with three progressively stronger types of algebraic equivalences, concrete query transformation rules that obey the different equivalences, and a procedure for determining which types of transformation rules are applicable for optimizing a query. The exposition follows the style chosen by many textbooks, making it relatively easy to teach this material in continuation of the material covered in the textbooks, and to integrate this material into the textbooks.  Entity 2: title bringing order to query optimization authors giedrius slivinskas , christian s. jensen , richard thomas snodgrass venue acm sigmod record year 2002 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Continuous media servers that provide support for the storage and retrieval of continuous media data (e.g., video, audio) at guaranteed rates are becoming increasingly important. Such servers, typically, rely on several disks to service a large number of clients, and are thus highly susceptible to disk failures. We have developed two fault-tolerant approaches that rely on admission control in order to meet rate guarantees for continuous media requests. The schemes enable data to be retrieved from disks at the required rate even if a certain disk were to fail. For both approaches, we present data placement strategies and admission control algorithms. We also present design techniques for maximizing the number of clients that can be supported by a continuous media server. Finally, through extensive simulations, we demonstrate the effectiveness of our schemes.  Entity 2: title fault-tolerant architectures for continuous media servers authors banu &#214; zden , rajeev rastogi , prashant shenoy , avi silberschatz venue international conference on management of data year 1996 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We present incremental view maintenance algorithms for a data warehouse derived from multiple distributed autonomous data sources. We begin with a detailed framework for analyzing view maintenance algorithms for multiple data sources with concurrent updates. Earlier approaches for view maintenance in the presence of concurrent updates typically require two types of messages: one to compute the view change due to the initial update and the other to compensate the view change due to interfering concurrent updates. The algorithms developed in this paper instead perform the compensation locally by using the information that is already available at the data warehouse. The first algorithm, termed SWEEP, ensures complete consistency of the view at the data warehouse in the presence of concurrent updates. Previous algorithms for incremental view maintenance either required a quiescent state at the data warehouse or required an exponential number of messages in terms of the data sources. In contrast, this algorithm does not require that the data warehouse be in a quiescent state for incorporating the new views and also the message complexity is linear in the number of data sources. The second algorithm, termed Nested SWEEP, attempts to compute a composite view change for multiple updates that occur concurrently while maintaining strong consistency.  Entity 2: title efficient view maintenance at data warehouses authors d. agrawal , a. el abbadi , a. singh , t. yurek venue international conference on management of data year 1997 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: From the standpoint of supporting human-centered discovery of knowledge, the present-day model of mining association rules suffers from the following serious shortcomings: (i) lack of user exploration and control, (ii) lack of focus, and (iii) rigid notion of relationships. In effect, this model functions as a black-box, admitting little user interaction in between. We propose, in this paper, an architecture that opens up the black-box, and supports constraint-based, human-centered exploratory mining of associations. The foundation of this architecture is a rich set of constraint constructs, including domain, class, and SQL-style aggregate constraints, which enable users to clearly specify what associations are to be mined. We propose constrained association queries as a means of specifying the constraints to be satisfied by the antecedent and consequent of a mined association.  Entity 2: title exploratory mining and pruning optimizations of constrained associations rules authors raymond t. ng , laks v. s. lakshmanan , jiawei han , alex pang venue international conference on management of data year 1998 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Building such a database system requires fundamental changes in the architecture of the query processing engine; we present the system-level interfaces of PREDATOR that support E-ADTs, and describe the internal design details.  Entity 2: title the case for enhanced abstract data types authors praveen seshadri , miron livny , raghu ramakrishnan venue very large data bases year 1997 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The use of social media in advocacy, and particularly transnational advocacy, raises concerns of privacy and security for those conducting the advocacy and their contacts on social media. This chapter presents high-level summaries of cases of social media in advocacy and activism from the perspectives of information warfare and information security. From an analysis of these, the impact and relationships of social media in transnational advocacy and information security is discussed. Whilst online advocacy can be considered to be a form of information warfare aligned to a Cyber Macht theory, it can be argued that social media advocacy negatively impacts information security as it encourages various actors to actively attempt to breach security.  Entity 2: title information warfare and security authors h. v. jagadish venue acm sigmod record year 2001 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Internet-based communication defines two main types of services as Pull and Push services, depending on the side that sends the request for transmission of information. In contrast to Pull services, whose request for transmission is initiated by the client, Push service denotes a type of transmission where the request for a given exchange of information is initiated by the publisher or central server. The newly proposed Alert Notification Service (ANS) represents an important implementation of these Push services, for the sites that neither implement it or that think it is not worth to do it economically. In this paper we present details on system functionalities, architecture model, design, and integration of essential modules into a fully working system as a service. The main contribution is in design of a highly scalable solution for an elastic cloud architecture, by separating the static and dynamic parts, correspondingly assigned to different virtual machines.  Entity 2: title data in your face : push technology in perspective authors michael franklin , stan zdonik venue international conference on management of data year 1998 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This article addresses the performance of distributed database systems. Specifically, we present an algorithm for dynamic replication of an object in distributed systems. The algorithm is adaptive in the sence that it changes the replication scheme of the object i.e., the set of processors at which the object inreplicated) as changes occur in the read-write patern of the object (i.e., the number of reads and writes issued by each processor). The algorithm continuously moves the replication scheme towards an optimal one. We show that the algorithm can be combined with the concurrency control and recovery mechanisms of ta distributed database management system. The performance of the algorithm is analyzed theoretically and experimentally. On the way we provide a lower bound on the performance of any dynamic replication algorith.  Entity 2: title adaptive , fine-grained sharing in a client-server oodbms : a callback-based approach authors markos zaharioudakis , michael j. carey , michael j. franklin venue acm transactions on database systems ( tods ) year 1997 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In a recent paper, we proposed adding aSTOP AFTER clause to SQL to permit the cardinality of a query result to be explicitly limited by query writers and query tools. We demonstrated the usefulness of having this clause, showed how to extend a traditional cost-based query optimizer to accommodateit, and demonstrated via DB2-basedsimulations that large performancegains are possible whenSTOP AFTER queries are explicitly supported by the database engine. In this paper, we present several new strategies for efficiently processing STOP AFTER queries. These strategies, based largely on the use of range partitioning techniques, offer significant additional savings for handling STOP AFTER queries that yield sizeable result sets. We describe classes of queries where such savings would indeed arise and present experimental measurements that show the benefits and tradeoffs associated with the new processing strategies  Entity 2: title reducing the braking distance of an sql query engine authors michael j. carey , donald kossmann venue very large data bases year 1998 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The rapid growth of structured data on the Web has created a high demand for making this content more reusable and consumable. Companies are competing not only on gathering structured content and making it public, but also on encouraging people to reuse and profit from this content. Many companies have made their content publicly accessible not only through APIs but also started to widely adopt web metadata standards such as XML, RDF, RDFa, and microformats. This trend of structured data on the Web (Data Web) is shifting the focus of Web technologies towards new paradigms of structured-data retrieval.  Entity 2: title query engines for web-accessible xml data authors leonidas fegaras , ramez elmasri venue very large data bases year 2001 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: To bridge the gap between these two extremes, we propose a new class of replication systems called TRAPP (Tradeoff in Replication Precision and Performance). TRAPP systems give each user fine-grained control over the tradeoff between precision and performance: Caches store ranges that are guaranteed to bound the current data values, instead of storing stale exact values. Users supply a quantitative precision constraint along with each query. To answer a query, TRAPP systems automatically select a combination of locally cached bounds and exact master data stored remotely to deliver a bounded answer consisting of a range that is no wider than the specified precision constraint, that is guaranteed to contain the precise answer, and that is computed as quickly as possible. This paper defines the architecture of TRAPP replication systems and covers some mechanics of caching data ranges.  Entity 2: title offering a precision-performance tradeoff for aggregation queries over replicated data authors chris olston , jennifer widom venue very large data bases year 2000 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The Indian Institute of Technology, Bombay is one of the leading universities in India. Located in Powai, a suburb of the vibrant city of Bombay (which is soon to revert to its original name, Mumbai), it is a scenic campus extending over 500 acres on the shores of Lake Powai. The institute has a faculty strength of about 400, and has about 2500 students. The Department of Computer Science has a faculty strength of 25, and around 150 undergraduate and 70 postgraduate students. The Database Group in the Department of Computer Science and Engineering is the largest database group in India. The group currently has four faculty members, D. B. Phatak, N. L. Sarda, S. Seshadri and S. Sudarshan. The group also currently has three research scholars, ten Masters students, ten undergraduate students and nine project engineers.  Entity 2: title database research at the indian institute of technology , bombay authors d. b. phatak , n. l. sarda , s. seshadri , s. sudarshan venue acm sigmod record year 1996 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: A unifying model for the study of database performance is proposed. Applications of the model are shown to relate and extend important work concerning batched searching, transposed files, index selection, dynamic hash-based files, generalized access path structures, differential files, network databases, and multifile query processing.  Entity 2: title cost models for overlapping and multiversion structures authors yufei tao , dimitris papadias , jun zhang venue acm transactions on database systems ( tods ) year 2002 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The SkyServer provides Internet access to the public Sloan Digital Sky Survey (SDSS) data for both astronomers and for science education. This paper describes the SkyServer goals and architecture. It also describes our experience operating the SkyServer on the Internet. The SDSS data is public and well-documented so it makes a good test platform for research on database algorithms and performance.  Entity 2: title the sdss skyserver : public access to the sloan digital sky server data authors alexander s. szalay , jim gray , ani r. thakar , peter z. kunszt , tanu malik , jordan raddick , christopher stoughton , jan vandenberg venue international conference on management of data year 2002 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O\u2019s state is loaded, similar state for other objects in O\u2019s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.  Entity 2: title context-based prefetch for implementing objects on relations authors philip a. bernstein , shankar pal , david shutt venue very large data bases year 1999 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Physical database design is important for query performance in a shared-nothing parallel database system, in which data is horizontally partitioned among multiple independent nodes. We seek to automate the process of data partitioning. Given a workload of SQL statements, we seek to determine automatically how to partition the base data across multiple nodes to achieve overall optimal (or close to optimal) performance for that workload. Previous attempts use heuristic rules to make those decisions. These approaches fail to consider all of the interdependent aspects of query performance typically modeled by today's sophisticated query optimizers.We present a comprehensive solution to the problem that has been tightly integrated with the optimizer of a commercial shared-nothing parallel database system. Our approach uses the query optimizer itself both to recommend candidate partitions for each table that will benefit each query in the workload, and to evaluate various combinations of these candidates. We compare a rank-based enumeration method with a random-based one. Our experimental results show that the former is more effective.  Entity 2: title automating physical database design in a parallel database authors jun rao , chun zhang , nimrod megiddo , guy lohman venue international conference on management of data year 2002 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: DataMine is a statistical database mining system with strong emphasis on interactiveness and nice graphical representation of information produced. It also supports an offline mode of discovery, and provides an extensive API which allows users to write \"mining applications\" just as easily as routine database applications, The central idea is to perform discovery with a \"human in the loop\" guiding the system using his initial hypothesis and the feedback from the system. Users can pose a rule-query against a rulebase and the system can generate all rules matching their query. The rulebase could either be pregenerated (using offline mode) or could be realized in real-time as the discovery progresses.Rules generated by the system are of the form:Body -&gt; Consequentwhere Body is a conjunction of the elementary predicates of the form (A=a), where A is an attribute and a is a value from the attribute domain of A. Consequent is a single elementary predicate. Each rule can have several parameters like support, confidence, atypicality, color etc. (the definitions have been left out) which can also be used by the user in framing the rule query.For continuous attributes, the system also allows the user some control in deciding how they are discretized. It also allows for the creation of extra attributes at run time which can then be used in queries like the rest.  Entity 2: title datamine-interactive rule discovery system authors t. imielinski , a. virmani venue international conference on management of data year 1995 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The exponential growth of resources on the web, and the wide deployment of devices for multimodal access to the Internet, lead to new problems in information management. In this context, and as part of the European project Vision, we have built an interactive telematic handbook of the culture and the territory of Sardinia. A team of cultural experts browsed the web to get a large collection of Internet resources.The system built for the management of this data uses emerging Internet technologies such as the XML language suite and its applications.  Entity 2: title eiha ?!? : deploying web and wap services using xml technology authors chiara biancheri , jean-christophe pazzaglia , gavino paddeu venue acm sigmod record year 2001 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: he \u03bb-DB OQL compiler is a C++ preprocessor that accepts a language called \u03bb-OQL, which is C++ code with embedded DML commands to perform transactions, queries, updates, etc. The preprocessor translates \u03bb-OQL programs into C++ code that contains calls to the \u03bb-DB evaluation engine. We also provide a visual query formulation interface, called VOODOO, and a translator from visual queries to OQL text, which can be sent to the \u03bb-DB OQL interpreter for evaluation. Even though a lot of effort has been made to make the implementation of our system simple enough for other database researchers to use and extend, our system is quite sophisticated since it employs current state-of-the-art query optimization technologies as well as new advanced experimental optimization techniques which we have developed through the years, such as query unnesting.  Entity 2: title lambda-db : an odmg-based object-oriented dbms authors leonidas fegaras , chandrasekhar srinivasan , arvind rajendran , david maier venue international conference on management of data year 2000 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Government and industry are investing substantial resources in new technologies for accessing heterogeneous information sources, including text-based corpora, structured data, imagery, geo-spatial data, audio, video, and more. Managers of programs that fund relevant research face a difficult problem: they are required to justify investment in certain technologies and approaches versus alternate ones. These program managers recognize a need for good evaluation criteria, but there is little consensus on which criteria to use  Entity 2: title metrics for accessing heterogeneous data : is there any hope ? ( panel ) authors leonard j. seligman , nicholas j. belkin , erich j. neuhold , michael stonebraker , gio wiederhold venue very large data bases year 1995 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Active database systems support mechanisms that enable them to respond automatically to events that are taking place either inside or outside the database system itself. Considerable effort has been directed towards improving understanding of such systems in recent years, and many different proposals have been made and applications suggested. This high level of activity has not yielded a single agreed-upon standard approach to the integration of active functionality with conventional database systems, but has led to improved understanding of active behavior description languages, execution models, and architectures. This survey presents the fundamental characteristics of active database systems, describes a collection of representative systems within a common framework, considers  the consequences for implementations of certain design decisions, and discusses tools for developing active applications.  Entity 2: title declare and sds : early efforts to commercialize deductive database technology authors werner kie &#223; ling , helmut schmidt , werner strau &#223; , gerhard d &#252; nzinger venue the vldb journal -- the international journal on very large data bases year 1994 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Recent research addressed the importance of optimizing L2 cache utilization in the design of main memory indexes and proposed the so-called cache-conscious indexes such as the CSB+-tree. However, none of these indexes took account of concurrency control, which is crucial for running the real-world main memory database applications involving index updates and taking advantage of the off-the-shelf multiprocessor systems for scaling up the performance of such applications. Observing that latching index nodes for concurrency control (CC) incurs the so-called coherence cache misses on shared-memory multiprocessors thus limiting the scalability of the index performance, this paper presents an optimistic, latch-free index traversal (OLFIT) CC scheme based on a pair of consistent node read and update primitives. An experiment with various index CC implementations for the B+tree and CSB+-tree shows that the proposed scheme shows the superior scalability on the multiprocessor system as well as the performance comparable to that of the sequential execution without CC on the uniprocessor system.  Entity 2: title efficient concurrency control in multidimensional access methods authors kaushik chakrabarti , sharad mehrotra venue international conference on management of data year 1999 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Cleaning data of errors in structure and content is important for data warehousing and integration. Current solutions for data cleaning involve many iterations of data \u201cauditing\u201d to find errors, and long-running transformations to fix them. Users need to endure long waits, and often write complex transformation scripts. We present Potter\u2019s Wheel, an interactive data cleaning system that tightly integrates transformation and discrepancy detection. Users gradually build transformations to clean the data by adding or undoing transforms on a spreadsheet-like interface; the effect of a transform is shown at once on records visible on screen. These transforms are specified either through simple graphical operations, or by showing the desired effects on example data values. In the background, Potter\u2019s Wheel automatically infers structures for data values in terms of user-defined domains, and accordingly checks for constraint violations. Thus users can gradually build a transformation as discrepancies are found, and clean the data without writing complex programs or enduring long delays.  Entity 2: title potter 's wheel : an interactive data cleaning system authors vijayshankar raman , joseph m. hellerstein venue very large data bases year 2001 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Various models and languages for describing and manipulating hierarchically structured data have been proposed. Algebraic, calculus-based, and logic-programming oriented languages have all been considered. This article presents a general model for complex values (i.e., values with hierarchical structures), and languages for it based on the three paradigms. The algebraic language generalizes those presented in the literature; it is shown to be related to the functional, style of programming advocated by Backus (1978). The notion of domain independence (from relational databases) is defined, and syntactic restrictions (referred to as safety conditions) on calculus queries are formulated to guarantee domain independence. The main results are: The domain-independent calculus, the safe calculus, the algebra, and the logic-programming oriented language have equivalent expressive power. In particular, recursive queries, such as the transitive closure, can be expressed in each of the languages. For this result, the algebra needs the powerset operation. A more restricted version of safety is presented, such that the restricted safe calculus is equivalent to the algebra without the powerset. The results are extended to the case where arbitrary functions and predicates are used in the languages.  Entity 2: title the power of languages for the manipulation of complex values authors serge abiteboul , catriel beeri venue the vldb journal -- the international journal on very large data bases year 1995 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The amount of scientific and technical information is growing exponentially. As a result, the scientific community has been overwhelmed by the information published in number of new books, journal articles, and conference proceedings. In addition to increasing number of publications, advances in information technology have dramatically reduced the barriers in electronic publishing and distribution of information over networks virtually anywhere in the world. As a result, the scientific community is facing the problem of locating relevant or interesting information. To address the problem of information overload and to sift all available information sources for useful information, recommender systems or filtering systems have emerged. Generally, recommender systems are used online to suggest items that users find interesting, thereby, benefiting both the user and merchant. Recommender systems benefit the user by making him suggestions on items that he is likely to purchase and the business by increase of sales. Filtering information or generation of recommendatio ns by the recommender systems mimic the process of information retrieval systems by incorporating advanced profile building techniques, item/user representation techniques, filtering and recommendation techniques, and profile adaptation techniques. This paper addresses the application domain analysis, functional classification, advantages and disadvantages of various filtering and recommender systems.  Entity 2: title functional properties of information filtering authors rie sawai , masahiko tsukamoto , yin-huei loh , tsutomu terada , shojiro nishio venue very large data bases year 2001 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The theme of the paper is to promote research on asynchronous transactions. We discuss our experience of executing synchronous transactions on a large distributed production system in The Boeing Company. Due to the poor performance of synchronous transactions in our environment, it motivated the exploration of asynchronous transactions as an alternate solution. This paper presents the requirements and benefits/limitations of asynchronous transactions. Open issues related to large scale deployments of asynchronous transactions are also discussed.  Entity 2: title the need for distributed asynchronous transactions authors lyman do , prabhu ram , pamela drew venue international conference on management of data year 1999 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: There are high expectations in all sectors of society for immediate access to biological knowledge of all kinds. To fully exploit and manage the value of biological resources, society must have the intellectual tools to store, retrieve, collate, analyze, and synthesize organism-level and ecological scale information. However, it currently is difficult to discover, access, and use biodiversity data because of the long history of \u201cbottom-up\u201d evolution of scientific biodiversity information, the mismatch between the distribution of biodiversity itself and the distribution of the data about it, and, most importantly, the inherent complexity of biodiversity and ecological data. This stems from, among many factors, numerous data types, the nonexistence of a common underlying (binary) language, and the multiple perceptions of different researchers/data recorders across spatial or temporal distance or both.  Entity 2: title biodiversity informatics : the challenge of rapid development , large databases , and complex data ( keynote ) authors meridith a. lane , james l. edwards , ebbe nielsen venue very large data bases year 2000 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Spatio-temporal data warehouses store enormous amount of data. They are usually exploited by spatiotemporal OLAP systems to extract relevant information. For extracting interesting information, the current user launches spatio-temporal OLAP (ST-OLAP) queries to navigate within a geographic data cube (Geo-cube). Very often choosing which part of the Geo-cube to navigate further, and thus designing the forthcoming ST-OLAP query, is a difficult task. So, to help the current user refine his queries after launching in the geo-cube his current query, we need a ST-OLAP queries suggestion by exploiting a Geo-cube.  Entity 2: title temporal queries in olap authors alberto o. mendelzon , alejandro a. vaisman venue very large data bases year 2000 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: An efficient management of multiversion data with branched evolution is crucial for many applications. It requires database designers aware of tradeoffs among index structures and policies. This paper defines a framework and an analysis method for understanding the behavior of different indexing policies. Given data and query characteristics the analysis allows determining the most suitable index structure. The analysis is validated by an experimental study.  Entity 2: title author index venue very large data bases year 2000 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Our goal is to understand redo recovery. We define an installation graph of operations in an execution, an ordering significantly weaker than conflict ordering from concurrency control. The installation graph explains recoverable system state in terms of which operations are considered installed. This explanation and the set of operations replayed during recovery form an invariant that is the contract between normal operation and recovery. It prescribes how to coordinate changes to system components such as the state, the log, and the cache. We also describe how widely used recovery techniques are modeled in our theory, and why they succeed in providing redo recovery.  Entity 2: title a theory of redo recovery authors david lomet , mark tuttle venue international conference on management of data year 2003 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In a mobile computing system, caching data items at the mobile clients is important to reduce the data access delay in an unreliable and low bandwidth mobile network. However, efficient methods must be used to ensure the coherence between the cached items and the data items at the database server. By exploring the real time properties of the data items, we propose a cache invalidation scheme called: Invalidation by Absolute Validity Interval (IAVI). We define an absolute validate interval (AVI) for each data item based on its real time property, e.g. update interval. A mobile client can verify the validity of a cached item by comparing the last update time and its AVI. A cached item is invalidated if the current time is greater than the last update time by its AVI. With this self-invalidation mechanism, the IAVI scheme uses the invalidation report to inform the mobile clients about the change of AVI rather than the update event of the data item. As a result, the size of invalidation report can be reduced significantly. Performance studies show that the IAVI scheme can significantly reduce the mean response time and invalidation report size under various system parameters.  Entity 2: title cache invalidation scheme for mobile computing systems with real-time data authors joe chun-hung yuen , edward chan , kam-yiu lam , h. w. leung venue acm sigmod record year 2000 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The amount of services and deployed software agents in the most famous o spring of the Internet, the World Wide Web, is exponentially increasing. In addition, the Internet is an open environment, where information sources, communication links and agents themselves may appear and disappear unpredictably. Thus, an e ective, automated search and selection of relevant services or agents is essential for human users and agents as well. We distinguish three general agent categories in the Cyberspace, service providers, service requester, and middle agents. Service providers provide some type of service, such as nding information, or performing some particular domain speci c problem solving. Requester agents need provider agents to perform some service for them.  Entity 2: title dynamic service matchmaking among agents in open information environments authors katia sycara , matthias klusch , seth widoff , jianguo lu venue acm sigmod record year 1999 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Consider a database that represents information about moving objects and their location. For example, for a database representing the location of taxi-cabs a typical query may be: retrieve the free cabs that are currently within 1 mile of 33 N. Michigan Ave., Chicago (to pick-up a customer); or for a trucking company database a typical query may be: retrieve the trucks that are currently within 1 mile of truck ABT312 (which needs assistance); or for a database representing the current location of objects in a battlefield a typical query may be: retrieve the friendly helicopters that are in a given region, or, retrieve the friendly helicopters that are expected to enter the region within the next 10 minutes. The queries may originate from the moving objects, or from stationary users. We will refer to applications with the above characteristics as moving-objects-database (MOD) applications, and to queries as the ones mentioned above as MOD queries.  Entity 2: title domino : databases for moving objects tracking authors ouri wolfson , prasad sistla , bo xu , jutai zhou , sam chamberlain venue international conference on management of data year 1999 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Peer-to-Peer (P2P) systems are becoming increasingly popular as they enable users to exchange digital information by participating in complex networks. Such systems are inexpensive, easy to use, highly scalable and do not require central administration. Despite their advantages, however, limited work has been done on employing database systems on top of P2P networks.Here we propose the PeerOLAP architecture for supporting On-Line Analytical Processing queries. A large number low-end clients, each containing a cache with the most useful results, are connected through an arbitrary P2P network. If a query cannot be answered locally (i.e. by using the cache contents of the computer where it is issued), it is propagated through the network until a peer that has cached the answer is found. An answer may also be constructed by partial results from many peers. Thus PeerOLAP acts as a large distributed cache, which amplifies the benefits of traditional client-side caching. The system is fully distributed and can reconfigure itself on-the-fly in order to decrease the query cost for the observed workload. This paper describes the core components of PeerOLAP and presents our results both from simulation and a prototype installation running on geographically remote peers.  Entity 2: title an adaptive peer-to-peer network for distributed caching of olap results authors panos kalnis , wee siong ng , beng chin ooi , dimitris papadias , kian-lee tan venue international conference on management of data year 2002 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P, we need to estimate the fraction of records in the database that satisfy P. Many commercial database systems maintain histograms to approximate the frequency distribution of values in the attributes of relations.  Entity 2: title wavelet-based histograms for selectivity estimation authors yossi matias , jeffrey scott vitter , min wang venue international conference on management of data year 1998 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In the last few years, several works in the literature have addressed the problem of data extraction from Web pages. The importance of this problem derives from the fact that, once extracted, the d...  Entity 2: title a brief survey of web data extraction tools authors alberto h. f. laender , berthier a. ribeiro-neto , altigran s. da silva , juliana s. teixeira venue acm sigmod record year 2002 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper we present an efficient algorithm for mining association rules that is fundamentally different from known algorithms. Compared to previous algorithms, our algorithm not only reduces the I/O overhead significantly but also has lower CPU overhead for most cases. We have performed extensive experiments and compared the performance of our algorithm with one of the best existing algorithms. It was found that for large databases, the CPU overhead was reduced by as much as a factor of four and I/O was reduced by almost an order of magnitude. Hence this algorithm is especially suitable for very large size databases.  Entity 2: title an efficient algorithm for mining association rules in large databases authors ashoka savasere , edward omiecinski , shamkant b. navathe venue very large data bases year 1995 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper, we present a formal study of dynamic multidimensional histogram structures over continuous data streams. At the heart of our proposal is the use of a dynamic summary data structure (vastly different from a histogram) maintaining a succinct approximation of the data distribution of the underlying continuous stream. On demand, an accurate histogram is derived from this dynamic data structure. We propose algorithms for extracting such an accurate histogram and we analyze their behavior and tradeoffs. The proposed algorithms are able to provide approximate guarantees about the quality of the estimation of the histograms they extract.  Entity 2: title dynamic multidimensional histograms authors nitin thaper , sudipto guha , piotr indyk , nick koudas venue international conference on management of data year 2002 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Sorting is one of the most fundamental algorithms in Computer Science and a common operation in databases not just for sorting query results but also as part of joins (i.e., sort-merge-join) or indexing. In this work, we introduce a new type of distribution sort that leverages a learned model of the empirical CDF of the data. Our algorithm uses a model to efficiently get an approximation of the scaled empirical CDF for each record key and map it to the corresponding position in the output array. We then apply a deterministic sorting algorithm that works well on nearly-sorted arrays (e.g., Insertion Sort) to establish a totally sorted order. We compared this algorithm against common sorting approaches and measured its performance for up to 1 billion normally-distributed double-precision keys. The results show that our approach yields an average 3.38x performance improvement over C++ STL sort, which is an optimized Quicksort hybrid, 1.49x improvement over sequential Radix Sort, and 5.54x improvement over a C++ implementation of Timsort, which is the default sorting function for Java and Python.  Entity 2: title alphasort : a risc machine sort authors chris nyberg , tom barclay , zarka cvetanovic , jim gray , dave lomet venue international conference on management of data year 1994 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: An overview of selected papers related to bone published in 2017 is provided.PurposeThis paper accompanies a lecture at the 2018 Belgian Bone Club annual Clinical Update Symposium held in Brussels on January 20th, discussing the best papers (in the opinion of the author) published in the previous year.MethodsA PubMed search using the keyword \u201cbone\u201d and articles published in 2017.  Entity 2: title foreword by the vldb '98 pc chairmen authors o. shmueli , j. widom venue the vldb journal -- the international journal on very large data bases year 2000 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: A novel execution model for rule application in active databases is developed and applied to the problem of updating derived data in a database represented using a semantic, object-based database model. The execution model is based on the use of \u201climited ambiguity rules\u201d (LARs), which permit disjunction in rule actions. The execution model essentially performs a breadth-first exploration of alternative extensions of a user-requested update. Given an object-based database schema, both integrity constraints and specifications of derived classes and attributes are compiled into a family of limited ambiguity rules. A theoretical analysis shows that the approach is sound: the execution model returns all valid \u201ccompletions\u201d of a user-requested update, or terminates with an appropriate error notification. The complexity of the approach in connection with derived data update is considered.  Entity 2: title an execution model for limited ambiguity rules and its application to derived data update authors i.-min a. chen , richard hull , dennis mcleod venue acm transactions on database systems ( tods ) year 1995 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In many applications, users specify target values for certain attributes, without requiring exact matches to these values in return. Instead, the result to such queries is typically a rank of the top k\" tuples that best match the given attribute values. In this paper, we study the advantages and limitations of processing a top-k query by translating it into a single range query that traditional relational DBMSs can process eciently. In particular, we study how to determine a range query to evaluate a top-k query by exploiting the statistics available to a relational DBMS, and the impact of the quality of these statistics on the retrieval eciency of the resulting scheme.  Entity 2: title evaluating top-k selection queries authors surajit chaudhuri , luis gravano venue very large data bases year 1999 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Deep pretrained transformer networks are effective at various ranking tasks, such as question answering and ad-hoc document ranking. However, their computational expenses deem them cost-prohibitive in practice. Our proposed approach, called PreTTR (Precomputing Transformer Term Representations), considerably reduces the query-time latency of deep transformer networks (up to a 42x speedup on web document ranking) making these networks more practical to use in a real-time ranking scenario. Specifically, we precompute part of the document term representations at indexing time (without a query), and merge them with the query representation at query time to compute the final ranking score. Due to the large size of the token representations, we also propose an effective approach to reduce the storage requirement by training a compression layer to match attention scores. Our compression technique reduces the storage required up to 95% and it can be applied without a substantial degradation in ranking performance.  Entity 2: title effective & ; efficient document ranking without using a large lexicon authors yasushi ogawa venue very large data bases year 1996 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Simple economic and performance arguments suggest appropriate lifetimes for main memory pages and suggest optimal page sizes. The fundamental tradeoffs are the prices and bandwidths of RAMs and dis...  Entity 2: title the five-minute rule ten years later , and other computer storage rules of thumb authors jim gray , goetz graefe venue acm sigmod record year 1997 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper we describe an application of the lexical resource JurWordNet and of the Core Legal Ontology as a descriptive vocabulary for modeling legal domains. It can be viewed as the semantic component of a global standardisation framework for digital governments. A content description model provides a repository of structured knowledge aimed at supporting the semantic interoperability between sectors of Public Administration and the communication processes towards citizen. Specific conceptual models built from this base will act as a cognitive interface able to cope with specific digital government issues and to improve the interaction between citizen and Public Bodies.  Entity 2: title ontology-based support for digital government authors athman bouguettaya , ahmed k. elmagarmid , brahim medjahed , m. ouzzani venue very large data bases year 2001 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: A goal of the Biomedical Informatics Research Network (birn) project is to develop a multi-institution information management system for Neurosciences to gain a deeper understanding of several neurological disorders. Each institution specializes in a different subdiscipline and produces a database of its experimental or computationally derived data; a mediator module performs semantic integration over the databases to enable neuroscientists to perform analyses that could not be done from any single institution\u2019s data. The overall system architecture of the birn system is that of a wrapper-mediator system. The information sources are various relational sources including Oracle 9i having userdefined packages, Oracle 8i with the Spatial Data Cartridge, and databases made available over the web.  Entity 2: title birn-m : a semantic mediator for solving real-world neuroscience problems authors amarnath gupta , bertram lud &#228; scher , maryann e. martone venue international conference on management of data year 2003 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This contribution argues that electronic markets can serve as a powerful mechanism to entice providers to identify their customer base and to offer customer-oriented, high-quality and economical services and to induce customers to a more focused and price-conscious behavior. The paper claims that this should be particularly true for the provision and access to scientific literature where the tradition so far has been mostly free access by customers and non-transparent cost accounting and service procurement by university libraries. We report on a project for developing a technical network infrastructure that allows for a more cost-transparent access to scientific literature by campus users and attempts to add a competitive element to library services. Equally important, it provides added value to the users so that they can orient themselves in the vast expanses of scientific literature much faster and more economically. We cover three major elements of the infrastructure: user agents, traders and source wrappers.  Entity 2: title electronic market : the roadmap for university libraries and members to survive in the information jungle authors michael christoffel , sebastian pulkowski , bethina schmitt , peter c. lockemann venue acm sigmod record year 1998 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Many problems encountered when building applications of database systems involve the manipulation of models. By \"model,\" we mean a complex structure that represents a design artifact, such as a relational schema, object-oriented interface, UML model, XML DTD, web-site schema, semantic network, complex document, or software configuration. Many uses of models involve managing changes in models and transformations of data from one model into another. These uses require an explicit representation of \"mappings\" between models. We propose to make database systems easier to use for these applications by making \"model\" and \"model mapping\" first-class objects with special operations that simplify their use. We call this capability model management.In addition to making the case for model management, our main contribution is a sketch of a proposed data model. The data model consists of formal, object-oriented structures for representing models and model mappings, and of high-level algebraic operations on those structures, such as matching, differencing, merging, selection, inversion and instantiation. We focus on structure and semantics, not implementation.  Entity 2: title a vision for management of complex models authors phillip a. bernstein , alon y. halevy , rachel a. pottinger venue acm sigmod record year 2000 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Vendors of commercial database management system face many challenges in incorporating into their products innovative technologies developed in academia. Pragmatic considerat,ions and operational requirements can limit the viability of applying promising research. Technology leadership in commercial products is often the result of taking unconventional approaches rather than following \u201cconventional wisdom\u201d, as illustrated with several examples of technologies in Oracle8 and its predecessors. The challenge shared by researchers and practitioners alike: \u201cmaking what we do matter.\u201d  Entity 2: title innovation in database management : computer science vs. engineering authors kenneth r. jacobs venue very large data bases year 1997 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: One of the important problems in data mining is discovering association rules from databases of transactions where each transaction consists of a set of items. The most time consuming operation in this discovery process is the computation of the frequency of the occurrences of interesting subset of items (called candidates) in the database of transactions. To prune the exponentially large space of candidates, most existing algorithms, consider only those candidates that have a user defined minimum support. Even with the pruning, the task of finding all association rules requires a lot of computation power and time. Parallel computers offer a potential solution to the computation requirement of this task, provided efficient and scalable parallel algorithms can be designed. In this paper, we present two new parallel algorithms for mining association rules.  Entity 2: title scalable parallel data mining for association rules authors eui-hong han , george karypis , vipin kumar venue international conference on management of data year 1997 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this demonstration, we present a prototype peer-topeer (P2P) application called PeerDB[2] that provides database capabilities. This system has been developed at the National University of Singapore in collaboration with Fudan University, and is being enhanced with more features and applications. The concept behind PeerDB is similar to the analogy of publishing personal web sites, except that it is now applied to personal databases. Unlike personal web sites which are usually hosted together in a central web server, personal databases are stored in the person\u2019s own PC. In addition, it is increasingly common for people to keep their data in common personal DBMS like MySQL, and MSAccess. Therefore, a PeerDB node allows an user to index and publish his/her personal database for other peers to query. PeerDB builds on and extends BestPeer [1] for DBMS applications. Briefly, BestPeer is a generic P2P system designed to serve as a platform to develop P2P applications easily and efficiently. It has the following features: (1) It employs mobile agents; (2) It shares data at a finer granularity as well as computational power; (3) It can dynamically reconfigure the BestPeer network so that a node is always directly connected to peers that provide the best service; (4) It employs a set of location independent global name lookup (LIGLO) servers to uniquely recognize nodes whose IP addresses may change as a result. In the PeerDB network, a set of PeerDB nodes communicate or share resources with each other. Each node comprises four components that are loosely integrated: (a) a data management system (we used MySQL in our implementation) that facilitates storage, manipulation and retrieval of the data at the node, and the associated local and export dictionaries that reflect the meta-data (schema and keywords); (b) a database agent system called DBAgent that provides the environment for mobile agents to operate on; (c) a cache manager for managing remote meta-data and data in secondary storage; and (d) a user-friendly user interface. PeerDB has several distinguishing features. First, it allows users to query data without knowing the schema of data in other nodes.  Entity 2: title peerdb : peering into personal databases authors beng chin ooi , kian-lee tan , aoying zhou , chin hong goh , yingguang li , chu yee liau , bo ling , wee siong ng , yanfeng shu , xiaoyu wang , ming zhang venue international conference on management of data year 2003 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: While the XML Stylesheet Language for Transformations (XSLT) was not designed as a query language, it is well-suited for many query-like operations on XML documents including selecting and restructuring data. Further, it actively fulfills the role of an XML query language in modern applications and is widely supported by application platform software. However, the use of database techniques to optimize and execute XSLT has only recently received attention in the research community. In this paper, we focus on the case where XSL transformations are to be run on XML documents defined as views of relational databases. For a subset of XSLT, we present an algorithm to compose a transformation with an XML view, eliminating the need for the XSLT execution. We then describe how to extend this algorithm to handle several additional features of XSLT, including a proposed approach for handling recursion.  Entity 2: title composing xsl transformations with xml publishing views authors chengkai li , philip bohannon , p. p. s. narayan venue international conference on management of data year 2003 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper, we show how compression can be integrated into a relational database system. Specifically, we describe how the storage manager, the query execution engine, and the query optimizer of a database system can be extended to deal with compressed data. Our main result is that compression can significantly improve the response time of queries if very light-weight compression techniques are used. We will present such light-weight compression techniques and give the results of running the TPC-D benchmark on a so compressed database and a non-compressed database using the AODB database system, an experimental database system that was developed at the Universities of Mannheim and Passau. Our benchmark results demonstrate that compression indeed offers high performance gains (up to 50%) for IO-intensive queries and moderate gains for CPU-intensive queries. Compression can, however, also increase the running time of certain update operations. In all, we recommend to extend today's database systems with light-weight compression techniques and to make extensive use of this feature.  Entity 2: title the implementation and performance of compressed databases authors till westmann , donald kossmann , sven helmer , guido moerkotte venue acm sigmod record year 2000 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Microsoft\u2019s strategic interest in the database field dates from 1993 and the efforts of David Vaskevitch, who is now the Microsoft Vice President in charge of the database and transaction processing product development groups. David\u2019s vision was that the world would need millions of servers, and that this presented a wonderful opportunity to a company like Microsoft that sells software in high volume and at low prices. Database systems played an important role in Vaskevitch\u2019s vision, and, indeed, in Microsoft\u2019s current product plans. David began looking for premier database and transaction processing people in late 1993. The scope of Vaskevitch\u2019s efforts included a desire for Microsoft to establish a database research group. Rick Rashid, Microsoft Research Vice President, collaborated with Vaskevitch in recruiting David Lomet from Digital\u2019s Cambridge Research Lab to initiate the Microsoft Database Research Group. Lomet joined Microsoft Research in January of 1995. Hence, Microsoft\u2019s Database Research Group is now a little over three and a half years old. One person does not a group make. Recruiting efforts continued. Surajit Chaudhuri, a researcher from HP Labs joined the Database Group in February of 1996. Paul Larson, a professor from the University of Waterloo joined in May of that year. Vivek Narasayya was initially an intern as a graduate student from the University of Washington in the summer of 1996, officially joining the group in April of 1997. Roger Barga, the newest member of the group and a new Oregon Graduate Institute Ph.D., joined in December, 1997.  Entity 2: title the microsoft database research group authors david lomet , roger barga , surajit chaudhuri , paul larson , vivek narasayya venue acm sigmod record year 1998 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: A radar transmitter apparatus comprising a radar transmitter equipped with a modulator arranged in an oil-filled housing, the modulator being held in spaced relationship with respect to the inner walls of the housing in order to form an intermediate space for the convection flow of the oil. The housing is substantially trough or vat-shaped and covered by a trough or vat-shaped cover member. In the internal chamber or space between the cover member and the modulator, which internal space is wetted by the oil, there is arranged, on the one hand, a magnetron attached at the cover member and, on the other hand, a thyratron which is mounted directly below an opening at the cover member. This opening is closable by means of oil sealed throughpassage means.  Entity 2: title nodose-a tool for semi-automatically extracting structured and semistructured data from text documents authors brad adelberg venue international conference on management of data year 1998 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The subject of this paper is the creation of knowledge bases by enumerating and organizing all web occurrences of certain subgraphs. We focus on subgraphs that are signatures of web phenomena such as tightly-focused topic communities, webrings, taxonomy trees, keiretsus, etc. For instance, the signature of a webring is a central page with bidirectional links to a number of other pages. We develop novel algorithms for such enumeration problems. A key technical contribution is the development of a model for the evolution of the web graph, based on experimental observations derived from a snapshot of the web. We argue that our algorithms run efficiently in this model, and use the model to explain some statistical phenomena on the web that emerged during our experiments. Finally, we describe the design and implementation of Campfire, a knowledge base of over one hundred thousand web communities.  Entity 2: title extracting large-scale knowledge bases from the web authors ravi kumar , prabhakar raghavan , sridhar rajagopalan , andrew tomkins venue very large data bases year 1999 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Integrity constraint checking for stratifiable deductive databases has been studied by many authors. However, most of these methods may perform unnecessary checking if the update is irrelevant to the constraints. [Lee94] proposed a set called relevant set which can be incorporated in these works to reduce unnecessary checking. [Lee94] adopts a top-down approach and makes use of constants and evaluable functions in the constraints and deductive rules to reduce the search space. In this paper, we further extend this idea to make use of relational predicates, instead of only constants and evaluable functions in [Lee94]. We first show that this extension is not a trivial one as extra database retrieval cost is incurred. We then present a new method to construct a pre-test which can be incorporated in most existing methods to reduce the average checking costs in terms of database accesses by a significant factor. Our method also differs from other partial checking methods as we can handle multiple updates.  Entity 2: title further improvements on integrity constraint checking for stratifiable deductive databases authors sin yeung lee , tok wang ling venue very large data bases year 1996 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: XML has emerged as the standard data exchange format for Internet-based business applications. This has created the need to publish existing business data, stored in relational databases, as XML. A general way to publish relational data as XML is to provide XML views over relational data, and allow business partners to query these views using an XML query language. In this paper, we address the problem of evaluating XML queries over XML views of relational data. This paper makes two main contributions. The first is a general framework for processing arbitrarily complex queries specified using the XQuery query language. The second is a technique for efficiently evaluating XML queries by pushing most of the query computation down to the relational engine.  Entity 2: title querying xml views of relational data authors jayavel shanmugasundaram , jerry kiernan , eugene j. shekita , catalina fan , john funderburk venue very large data bases year 2001 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We develop a simple yet powerful classification of MDBSs based on the nature of integrity constraints and transaction programs. For each of the identified models we show how consistency can be preserved by ensuring that executions are two-level serializable (2LSR). 2LSR is a correctness criterion for MDBS environments weaker than serializability. What makes our approach interesting is that unlike global serializability, ensuring 2LSR in MDBS environments is relatively simple and protocols to ensure 2LSR permit a high degree of concurrency. Furthermore, we believe the range of models we consider cover many practical MDBS environments to which the results of this article can be applied to preserve database consistency.  Entity 2: title ensuring consistency in multidatabases by preserving two-level serializability authors sharad mehrotra , rajeev rastogi , henry f. korth , abraham silberschatz venue acm transactions on database systems ( tods ) year 1998 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: MOTIVATION A large number of useful databases are currently accessible over the Web and within corporate networks. In addition to being frequently updated, this collection of databases tends to be highly dynamic: new databases appear often, and databases (just like Web sites) also disappear. In this environment, the goal of providing flexible, timely and declarative query access over all these databases remains elusive.  Entity 2: title infosleuth : agent-based semantic integration of information in open and dynamic environments authors r. j. bayardo , jr. , w. bohrer , r. brice , a. cichocki , j. fowler , a. helal , v. kashyap , t. ksiezyk , g. martin , m. nodine , m. rashid , m. rusinkiewicz , r. shea , c. unnikrishnan , a. unruh , d. woelk venue international conference on management of data year 1997 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Here we propose the PeerOLAP architecture for supporting On-Line Analytical Processing queries. A large number low-end clients, each containing a cache with the most useful results, are connected through an arbitrary P2P network. If a query cannot be answered locally (i.e. by using the cache contents of the computer where it is issued), it is propagated through the network until a peer that has cached the answer is found. An answer may also be constructed by partial results from many peers. Thus PeerOLAP acts as a large distributed cache, which amplifies the benefits of traditional client-side caching. The system is fully distributed and can reconfigure itself on-the-fly in order to decrease the query cost for the observed workload.  Entity 2: title a survey of approaches to automatic schema matching authors erhard rahm , philip a. bernstein venue the vldb journal -- the international journal on very large data bases year 2001 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper describes issues and solutions related to the creation of a product information database in the enterprise, and using this database as a foundation for deploying an electronic catalog. Today, product information is typically managed in document composition systems and communicated on paper. In the new wired world, these processes are undertaking fundamental changes to cope with the time to market pressure and the need for accurate, complete, and structured presentation of product information. Electronic catalogs are the answer.  Entity 2: title memory-contention responsive hash joins authors diane l. davison , goetz graefe venue very large data bases year 1994 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Given the present cost of memories and the very large storage and bandwidth requirements of large-scale multimedia databases, hierarchical storage servers (which consist of RAM, disk storage, and robot-based tertiary libraries) are becoming increasingly popular. However, related research is scarce and employs tertiary storage for storage augmentation purposes only. This work, exploiting the ever-increasing performance offered by (par. titularly) modern tape library products, aims to utilize tertiary storage in order to augment the system\u2019s performance. We consider the issue of elevating continuous data from its permanent place in tertiary for display purposes. Our primary goals are to save on the secondary storage bandwidth that traditional techniques require for the display of continuous objects, while requiring no additional . RAM buffer space. To this end we develop algorithms for sharing the responsibility for * Research supported by the European Community under the ESPRIT Long Term Research Project HERMES no. 9141. t Research supported by the Canadian government under NSERC grant number 0155218, and by the 1996-97 Going Global STEP program.  Entity 2: title on-demand data elevation in hierarchical multimedia storage servers authors peter triantafillou , thomas papadakis venue very large data bases year 1997 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: It is today widely accepted that \u201cBusiness Rules Independence\u201d is required for information systems to better and more rapidly adjust to changes in the business environment, This paper\u2019 attempts to articulate how logic based database systems provide adequate technology for better \u201cBusiness Rules Independence\u201d. These systems do so by going beyond \u201cData Independence\u201d and by providing \u201cKnowledge Independence\u201d. This paper benefits from the experience gained in developing and marketing the VALIDITY deductive and object-oriented database system during the last few years. Current applications and specific data management techniques to be used are discussed.  Entity 2: title from data independence to knowledge independence : an on-going story authors laurent vieille venue very large data bases year 1998 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We have been developing a mobile passenger guide system for public transports. Passengers can make their travel plans and purchase necessary electronic tickets using mobile terminals via Internet. During the travel, the mobile terminal, which also works as an electronic ticket, compares the stored travel plan with the passenger's actual activities and offers appropriate guide messages. To execute this task, the mobile terminal collects various kinds of information about the travel fields (routes, fares, area maps, station maps, operation schedule, timetables, facilities of stations and vehicles etc.) using multi-channel data communications. The mobile terminal contains a personal database for the passenger by selecting and integrating necessary data according to the user's situation and characteristics.  Entity 2: title integration of electronic tickets and personal guide system for public transport using mobile terminals authors koichi goto , yahiko kambayashi venue international conference on management of data year 2003 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight \"quality\" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues  Entity 2: title reminiscences on influential papers authors kenneth a. ross venue acm sigmod record year 2003 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The ability to store, access and disseminate large amounts of data has altered the social, educational, and governmental landscape throughout the world. The developments in technology, policy, and the economics of computing have been observed by some governments as providing a means to make government agencies more responsive to each others' as well as citizens' needs and also make government actions transparent and accountable to citizens. This panel brings together technocrats who are at the forefront of \u2026  Entity 2: title social , educational , and governmental change enabled through information technology authors krithi ramamritham , yeha el atfi , carlo batini , michael eitan , valerie gregg , d. b. phatak venue very large data bases year 2000 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Database technology is one of the cornerstones for the new millennium\u2019s IT landscape. However, database systems as a unit of code packaging and deployment are at a crossroad: commercial systems have been adding features for a long time and have now reached complexity that makes them a difficult choice, in terms of their \"gain/pain ratio\", as a central platform for value-added information services such as ERP or e-commerce. It is critical that database systems be easy to manage, predictable in their performance characteristics, and ultimately self-tuning. For this elusive goal, RISC-style simplification of server functionality and interfaces is absolutely crucial. We suggest a radical architectural departure in which database technology is packaged into much smaller RISC-style data managers with lean, specialized APIs, and with built-in self-assessment and auto-tuning capabilities 1. The Need for a New Departure Database technology has an extremely successful track record as a backbone of information technology (IT) throughout the last three decades. High-level declarative query languages like SQL and atomic transactions are key assets in the cost-effective development and maintenance of information systems. Furthermore, database technology continues to play a major role in the trends of our modern cyberspace society with applications ranging from webbased applications/services, and digital libraries to information mining on business as well as scientific data. Thus, database technology has impressively proven its benefits and seems to remain crucially relevant in the new millennium as well. Success is a lousy teacher (to paraphrase Bill Gates), and therefore we should not conclude that the database system, as the unit of engineering, deploying, and operating packaged database technology, is in good shape. A closer look at some important application areas and major trends in the software industry strongly indicates that database systems have an overly low \u201cgain/pain ratio\u201d. First, with the dramatic drop of hardware and software prices, the expenses due to human administration and tuning staff dominate the cost of ownership for a database system. The complexity and cost of these feed-and-care tasks is likely to prohibit database systems from further playing their traditionally prominent role in the future IT infrastructure. Next, database technology is more likely to be adopted in unbundled and dispersed form within higher-level application services.  Entity 2: title rethinking database system architecture : towards a self-tuning risc-style database system authors surajit chaudhuri , gerhard weikum venue very large data bases year 2000 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Workflow management systems are among the most interesting concepts for supporting modern organizations with a focus on processes rather than on structure. Workflow management systems offer different degrees of automation of business processes. We classify workflow management systems according to the features they provide and the types of processes they support. Database systems facilitate the realization of workflow management systems in several ways. They can provide the necessary functionality to keep the workflow relevant data, business data as well as process data. The dynamic execution of workflows can be handled by triggers of active database systems. Furthermore, the transaction concept can be extended to develop workflow transactions for consistent execution of workflows and intelligent treatment of exceptions and errors.  Entity 2: title databases and workflow management : what is it all about ? ( panel ) authors andreas reuter , stefano ceri , jim gray , betty salzberg , gerhard weikum venue very large data bases year 1995 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: When building a database, it is mandatory to design a friendly interface, which allo ws the nal user to easily access the data of interest. V ery often,such an interface exploits the pow er of visualization and direct manipulation mechanisms. How ever, it is not su\u00c6cient to associate \\any\" visual represen tation to a database, but the visual representation should be carefully chosen to e ectively con vey all and only the database information content. We are curren tly w orkingon a general theory (see ) for establishing the adequacy of a visual representation, once speci ed the database characteristics, and we are developing a system, called D ARE: Drawing Adequate REpresentations, which implements such a theory.  Entity 2: title the prototype of the dare system authors tiziana catarci , giuseppe santucci venue international conference on management of data year 2001 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: After the successful first International Workshop on Engineering Federated Database Systems (EFDBS'97) in Barcelona in June 1997 [CEH+ 97], the goal of this second workshop was to bring together researchers and practitioners interested in various issues in the development of federated information systems, whereby the scope has been extended to cover database and non-database information sources (the change from EFDBS to EFIS reflects this). This report provides details of the workshop content and the conclusions reached in the final discussion.  Entity 2: title specification and implementation of exceptions in workflow management systems authors fabio casati , stefano ceri , stefano paraboschi , guiseppe pozzi venue acm transactions on database systems ( tods ) year 1999 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper we present a mechanism for approximately translating Boolean query constraints across heterogeneous information sources. Achieving the best translation is challenging because sources support different constraints for formulating queries, and often these constraints cannot be precisely translated. For instance, a query [score > 8] might be \u201cperfectly\u201d translated as [rating > 0.8] at some site, but can only be approximated as [grade = A] at another. Unlike other work, our general framework adopts a customizable \u201ccloseness\u201d metric for the translation that combines both precision and recall. Our results show that for query translation we need to handle interdependencies among both query conjuncts as well as disjuncts. As the basis, we identify the essential requirements of a rule system for users to encode the mappings for atomic semantic units. Our algorithm then translates complex queries by rewriting them in terms of the semantic units.  Entity 2: title approximate query translation across heterogeneous information sources authors kevin chen-chuan chang , hector garcia-molina venue very large data bases year 2000 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Abstract A Real-Time DataBase System (RTDBS) can be viewed as an amalgamation of a conventional DataBase Management System (DBMS) and a real-time system. Like a DBMS, it has to process transactions and guarantee ACID database properties. Furthermore, it has to operate in real-time, satisfying time constraints imposed on transaction commitments. A RTDBS may exist as a stand-alone system or as an embedded component in a larger multidatabase system. The publication in 1988 of a special issue of ACM SIGMOD Record on Real-Time DataBases signaled the birth of the RTDBS research area---an area that brings together researchers from both the database and real-time systems communities. Today, almost eight years later, I am pleased to present in this special section of ACM SIGMOD Record a review of recent advances in RTDBS research. There were 18 submissions to this special section, of which eight papers were selected for inclusion to provide the readers of ACM SIGMOD Record with an overview of current and future research directions within the RTDBS community. In this paper, I will summarize these directions and provide the reader with pointers to other publications for further information.  Entity 2: title real-time index concurrency control authors jayant r. haritsa , s. seshadri venue acm sigmod record year 1996 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In late 2000, work was completed on yet another part of the SQL standard, to which we introduced our readers in an earlier edition of this column.Although SQL database systems manage an enormous amount of data, it certainly has no monopoly on that task. Tremendous amounts of data remain in ordinary operating system files, in network and hierarchical databases, and in other repositories. The need to query and manipulate that data alongside SQL data continues to grow. Database system vendors have developed many approaches to providing such integrated access.In this (partly guested) article, SQL's new part, Management of External Data (SQL/MED), is explored to give readers a better notion of just how applications can use standard SQL to concurrently access their SQL data and their non-SQL data.  Entity 2: title sql and management of external data authors jim melton , jan-eike michels , vanja josifovski , krishna kulkarni , peter schwarz , kathy zeidenstein venue acm sigmod record year 2001 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: XML is becoming the universal format for data exchange between applications. Recently, the emergence of Web services as standard means of publishing and accessing data on the Web introduced a new class of XML documents, which we call intensional documents. These are XML documents where some of the data is given explicitly while other parts are defined only intensionally by means of embedded calls to Web services.When such documents are exchanged between applications, one has the choice of whether or not to materialize the intensional data (i.e., to invoke the embedded calls) before the document is sent. This choice may be influenced by various parameters, such as performance and security considerations. This article addresses the problem of guiding this materialization process.We argue that---like for regular XML data---schemas (\u00e0 la DTD and XML Schema) can be used to control the exchange of intensional data and, in particular, to determine which data should be materialized before sending a document, and which should not. We formalize the problem and provide algorithms to solve it. We also present an implementation that complies with real-life standards for XML data, schemas, and Web services, and is used in the Active XML system. We illustrate the usefulness of this approach through a real-life application for peer-to-peer news exchange.  Entity 2: title exchanging intensional xml data authors tova milo , serge abiteboul , bernd amann , omar benjelloun , fred dang ngoc venue international conference on management of data year 2003 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Databases have employed a schema-based approach to store and retrieve structured data for decades. For peer-to-peer (P2P) networks, similar approaches are just beginning to emerge. While quite a few database techniques can be re-used in this new context, a P2P data management infrastructure poses additional challenges which have to be solved before schema-based P2P networks become as common as schema-based databases. We will describe some of these challenges and discuss approaches to solve them. Our discussion will be based on the design decisions we have employed in our Edutella infrastructure, a schema-based P2P network based on RDF and RDF schemas, and will also point out additional work addressing the issues discussed.  Entity 2: title design issues and challenges for rdf - and schema-based peer-to-peer systems authors wolfgang nejdl , wolf siberski , michael sintek venue acm sigmod record year 2003 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Analysts and decision-makers use what-if analysis to assess the e\u00aeects of hypotheti- cal scenarios. What-if analysis is currently supported by spreadsheets and ad-hoc O L AP tools. Unfortunately, the former lack seam- less integration with the data and the lat- ter lack \u00b0exibility and performance appropri- ate for O L AP applications. To tackle these problems we developed the Sesamesystem, which models an hypothetical scenario as a list of hypothetical modications on the ware- house views and fact data. We provide formal scenario syntax and semantics, which extend view update semantics for accomodating the special requirements of O L AP. We focus on query algebra operators suitable for perform- ing spreadsheet-style computations. Then we present Sesame's optimizer and its corner- stone substitution and rewriting mechanisms. Substitution enables lazy evaluation of the hy- pothetical updates. The substitution module delivers orders-of-magnitude optimizations in cooperation withtherewriterthatusesknowl- edge of arithmetic, relational, \u00afnancial and other operators. Finally we discuss the chal- lenges that the size of the scenario specica- tionsandthe arbitrarynatureof theoperators pose to the rewriter.  Entity 2: title hypothetical queries in an olap environment authors andrey balmin , thanos papadimitriou , yannis papakonstantinou venue very large data bases year 2000 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Data cube enables fast online analysis of large data repositories which is attractive in many applications. Although there are several kinds of available cube-based OLAP products, users may still encounter challenges on effectiveness and efficiency in the exploration of large data cubes due to the huge computation space as well as the huge observation space in a data cube. CubeExplorer is an integrated environment for online exploration of data cubes. It integrates our newly developed techniques on iceberg cube computation, cube-based feature extraction, and gradient analysis, and makes cube exploration effective and efficient. In this demo, we will show the features of CubeExplorer, especially its power and flexibility at exploring and mining of large databases.  Entity 2: title guest editorial authors alon y. halevy venue the vldb journal -- the international journal on very large data bases year 2002 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper describes security issues for federated database management systems set up for managing distributed, heterogeneous and autonomous multilevel databases. It builds on our previous work in multilevel secure distributed database management systems and on the results of others' work in federated database systems. In particular, we define a multilevel secure federated database system and discuss issues on heterogeneity, autonomy, security policy and architecture.  Entity 2: title practical issues with commercial use of federated databases authors jim kleewein venue very large data bases year 1996 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Although many query tree optimization strategies have been proposed in the literature, there still is a lack of a formal and complete representation of all possible permutations of query operations (i.e., execution plans) in a uniform manner. A graph-theoretic approach presented in the paper provides a sound mathematical basis for representing a query and searching for an execution plan. In this graph model, a node represents an operation and a directed edge between two nodes indicates the older of executing these two operations in an execution plan. Each node is associated with a weight and so is an edge. The weight is an expression containing optimization required parameters, such as relation size, tuple size, join selectivity factors. All possible execution plans are representable in this graph and each spanning tree of the graph becomes an execution plan. It is a general model which can be used in the optimizer of a DBMS for internal query representation. On the basis of this model, we devise an algorithm that finds a near optimal execution plan using only polynomial time. The algorithm is compared with a few other popular optimization methods. Experiments show that the proposed algorithm is superior to the others under most circumstances.  Entity 2: title a graph-theoretic model for optimizing queries involving methods authors chiang lee , chi-sheng shih , yaw-huei chen venue the vldb journal -- the international journal on very large data bases year 2001 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The proliferation of data in RDF format has resulted in the emergence of a plethora of specialized management systems. While the ability to adapt to the complexity of a SPARQL query -- given their inherent diversity -- is crucial, current approaches do not scale well when faced with substantially complex, non-selective joins, resulting in exponential growth of execution times. In this demonstration we present H2 RDF+, an RDF store that efficiently performs distributed Merge and Sort-Merge joins using a multiple-index scheme over HBase indexes. Through a greedy planner that incorporates our cost-model, it adaptively commands for either single or multi-machine query execution based on join complexity. In this paper, we present its key scientific contributions and allow participants to interact with an H2RDF+ deployment over a Cloud infrastructure. Using a web-based GUI we allow users to load different datasets (both real and synthetic), apply any query (custom or predefined) and monitor its execution. By allowing real-time inspection of cluster status, response times and committed resources the audience will evaluate the validity of H2RDF+'s claims and perform direct comparisons to two other state-of-the-art RDF stores.  Entity 2: title about quark digital media system authors kamar aulakh venue international conference on management of data year 1998 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Remote data access from disparate sources across a wide-area network such as the Internet is problematic due to the unpredictable nature of the communications medium and the lack of knowledge about the load and potential delays at remote sites. Traditional, static, query processing approaches break down in this environment because they are unable to adapt in response to unexpected delays. Query scrambling has been proposed to address this problem. Scrambling modifies query execution plans on-the-fly when delays are encountered during runtime. In its original formulation, scrambling was based on simple heuristics, which although providing good performance in many cases, were also shown to be susceptible to problems resulting from bad scrambling decisions. In this paper we address these shortcomings by investigating ways to exploit query optimization technology to aid in making intelligent scrambling choices. We propose three different approaches to using query optimization for scrambling. These approaches vary, for example, in whether they optimize for total work or response-time, and whether they construct partial or complete alternative plans. Using a two-phase randomized query optimizer, a distributed query processing simulator, and a workload derived from queries of the TPCD benchmark, we evaluate these different approaches and compare their ability to cope with initial delays in accessing remote sources. The results show that cost-based scrambling can effectively hide initial delays, but that in the absence of good predictions of expected delay durations, there are fundamental tradeoffs between risk aversion and effectiveness.  Entity 2: title cost-based query scrambling for initial delays authors tolga urhan , michael j. franklin , laurent amsaleg venue international conference on management of data year 1998 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Recent research activities in the area of Temporal Databases have revealed some problems related to the definition of time. In this paper we discuss the problem arising from the definition of valid time and the assumptions about valid time, which exist in current Temporal Database approaches. For this problem we propose a solution, while we identify some consistency problems that may appear in Temporal Databases, and which require further investigation.  Entity 2: title on the issue of valid time ( s ) in temporal databases authors stavros kokkotos , efstathios v. ioannidis , themis panayiotopoulos , constantine d. spyropoulos venue acm sigmod record year 1995 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Effective support for XML query languages is becoming increasingly important with the emergence of new applications that access large volumes of XML data. All existing proposals for querying XML (e.g., XQuery) rely on a pattern-specification language that allows path navigation and branching through the XML data graph in order to reach the desired data elements. Optimizing such queries depends crucially on the existence of concise synopsis structures that enable accurate compile-time selectivity estimates for complex path expressions over graph-structured XML data. In this paper, We introduce a novel approach to building and using statistical summaries of large XML data graphs for effective path-expression selectivity estimation. Our proposed graph-synopsis model (termed XSKETCH) exploits localized graph stability to accurately approximate (in limited space) the path and branching distribution in the data graph. To estimate the selectivities of complex path expressions over concise XSKETCH synopses, we develop an estimation framework that relies on appropriate statistical (uniformity and independence) assumptions to compensate for the lack of detailed distribution information. Given our estimation framework, we demonstrate that the problem of building an accuracy-optimal XSKETCH for a given amount of space is \ud835\udca9\ud835\udcab-hard, and propose an efficient heuristic algorithm based on greedy forward selection. Briefly, our algorithm constructs an XSKETCH synopsis by successive refinements of the label-split graph, the coarsest summary of the XML data graph. Our refinement operations act locally and attempt to capture important statistical correlations between data paths. Extensive experimental results with synthetic as well as real-life data sets verify the effectiveness of our approach. To the best of our knowledge, ours is the first work to address this timely problem in the most general setting of graph-structured data and complex (branching) path expressions.  Entity 2: title statistical synopses for graph-structured xml databases authors neoklis polyzotis , minos garofalakis venue international conference on management of data year 2002 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: STREAM is a general-purpose relational Data Stream Management System (DSMS). STREAM supports a declarative query language and flexible query execution plans. It is designed to cope with high data rates and large numbers of continuous queries through careful resource allocation and use, and by degrading gracefully to approximate answers as necessary. A description of language design, algorithms, system design, and implementation as of late 2002 can be found in [3]. The demonstration focuses on two aspects:  Entity 2: title stream : the stanford stream data manager ( demonstration description ) authors arvind arasu , brian babcock , shivnath babu , mayur datar , keith ito , itaru nishizawa , justin rosenstein , jennifer widom venue international conference on management of data year 2003 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper presents a model of a nomadic middleware system with support for temporal consistency of structures semantically associated to XML-documents. Specially defined high-level operations commute with each other in most cases reducing amount of transaction aborts and increasing system availability.  Entity 2: title towards a cooperative transaction model - the cooperative activity model authors marek rusinkiewicz , wolfgang klas , thomas tesch , j &#252; rgen w &#228; sch , peter muth venue very large data bases year 1995 ",
        "output": "The answer is yes",
        "answer": "yes",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: nvited Talk I.- Some Advances in Data-Mining Techniques.- Web Exploration.- Querying Semantically Tagged Documents on the World-Wide Web.- WWW Exploration Queries.- Strategies for Filtering E-mail Messages Combining Content-Based and Sociological Filtering with User-Stereotypes.- Interactive Query Expansion in a Meta-search Engine.- Database Technology.- On the Optimization of Queries containing Regular Path Expressions.  Entity 2: title the tv-tree : an index structure for high-dimensional data authors king ip lin , h. v. jagadish , christos faloutsos venue the vldb journal -- the international journal on very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Abstract This article discusses the challenges for Database Management in the Internet of Things. We provide scenarios to illustrate the new world that will be produced by the Internet of Things, where physical objects are fully integrated into the information highway. We discuss the different types of data that will be part of the Internet of Things. These include identification, positional, environmental, historical, and descriptive data. We consider the challenges brought by the need to manage vast quantities of data across heterogeneous systems. In particular, we consider the areas of querying, indexing, process modeling, transaction handling, and integration of heterogeneous systems. We refer to the earlier work that might provide solutions for these challenges. Finally we discuss a road map for the Internet of Things and respective technical priorities.  Entity 2: title aurora : a data stream management system authors d. abadi , d. carney , u. &#199; etintemel , m. cherniack , c. convey , c. erwin , e. galvez , m. hatoun , a. maskey , a. rasin , a. singer , m. stonebraker , n. tatbul , y. xing , r. yan , s. zdonik venue international conference on management of data year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: METU Object-Oriented DBMS1 includes the implementation of a database kernel, an object-oriented SQL-like language and a graphical user interface. Kernel functions are divided between a SQL Interpreter and a C++ compiler. Thus the interpretation of functions are avoided increasing the efficiency of the system. The compiled by C++ functions are used by the system through the Function Manager. The system is realized on Exodus Storage Manager (ESM), thus exploiting some of the kernel functions readily provided by ESM. The additional functions provided by the MOOD kernel are the optimization and interpretation of SQL statements, dynamic linking of functions, and catalog management.  Entity 2: title query unnesting in object-oriented databases authors leonidas fegaras venue international conference on management of data year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: One important step in integrating heterogeneous databases is matching equivalent attributes: Determining which fields in two databases refer to the same data. The meaning of information may be embodied within a. database model, a conceptual schema, application programs, or data contents. Integration involves extracting semantics, expressing them as metadata, and matching semantically equivalent data elements. We present a procedure using a classifier to categorize attributes according to their field specifications and data values, then train a neural network to recognize similar attributes. In our technique, the knowledge of how to match equivalent data elements is \u201cdiscovered\u201d from metadata , not \u201cpre-programmed\u201d.  Entity 2: title declustering databases on heterogeneous disk systems authors ling tony chen , doron rotem , sidhar seshadri venue very large data bases year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Expectations for change are ubiquitous in education. Teachers who complete education programs and enter classrooms are encouraged to embrace change as new professionals. Instructional change is expected as a result of professional development when teachers examine their instructional practices and reflect on student learning. Two scholars whose work is foundational in teacher education, Lortie (2002) and Britzman (2003) challenged researchers and educators to consider how teachers and teacher education changes and resists change. Lortie (2002), in his sociological study of the profession, speculated on changes and resistance in teaching and argued that the two would \u201cinteract contrapuntally\u201d (p. 219). In other words, these two forces exist in tension so that sustained change would often be thwarted. In a similar vein, Britzman\u2019s (2003) critical analysis of learning to teach examined discourses and discursive structures of teacher preparation and noted contradictions and struggles in the lived experiences of teacher candidates. Near the conclusion of her analysis, she wrote, we should \u201cattend to the possible and acknowledge the uncertainty of our educational lives\u201d (p. 241). Possibilities and uncertainties continue to inform the work of educating teacher candidates and supporting inservice teachers as agents of innovation in schools. To be sure, schools and teaching have changed in terms of student demographics, accountability, standardization, and testing since these two studies were published. And yet, the articles in this issue suggest that exploring sources, motivation, and possible outcomes of change, as well as the nature of resistance, remain paramount. In this issue, one pedagogical innovations article and four empirical studies articles explore change and consider how resistance informs teacher education. In an argument for including data literacy in teacher preparation, Reeves describes six hours of classroom-based training with four cohorts of preservice elementary teachers that he claims will help them more deeply understand how data literacy can inform pedagogy. Reeves\u2019 study is grounded in current research indicating that there is an increased expectation for teachers to use data to make instructional decisions. He contends that teacher preparation programs should include instruction about data literacy. This change could help preservice teachers enter classrooms with detailed knowledge about how data can productively inform their instruction. Reeves recognizes that such a change will require thoughtful consideration about when such instruction should be part of curricula; that these shifts will demand new resources; and, resistance to adding another facet to teacher education programs is possible. Reinhardt investigates how six cooperating teachers understand their roles within contexts of clinical experiences using Vygotskian theory. Her qualitative study focused on four female and two male teacher mentors in a diverse school district and examined how the cooperating teachers understand their role in a clinical experience as contrasted with the reality of teachers\u2019 everyday work.  Entity 2: title editorial authors richard snodgrass venue acm transactions on database systems ( tods ) year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Unfortunately, there is very little money to support the health promotion initiatives in this plan. Health promotion receives very little of the $17 billion NIH research budget and few health promotion procedures are covered by the $400+ billion spent annually for Medicare and Medicaid. The Office of Health Promotion and Disease Prevention has a budget so small that very few health promotion professionals ever encounter this office directly during their careers.  Entity 2: title editorial authors peter apers , stefano ceri , richard snodgrass venue the vldb journal -- the international journal on very large data bases year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: MOTIVATION A large number of useful databases are currently accessible over the Web and within corporate networks. In addition to being frequently updated, this collection of databases tends to be highly dynamic: new databases appear often, and databases (just like Web sites) also disappear. In this environment, the goal of providing flexible, timely and declarative query access over all these databases remains elusive.  Entity 2: title semantic integration of environmental models for application to global information systems and decision-making authors d. scott mackay venue acm sigmod record year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this column, we review these three books.  Entity 2: title book review column authors karl aberer venue acm sigmod record year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight \"quality\" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.  Entity 2: title reminiscences on influential papers authors kenneth a. ross venue acm sigmod record year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases, that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.  Entity 2: title static detection of security flaws in object-oriented databases authors keishi tajima venue international conference on management of data year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Similarity retrieval mechanisms should utilize generalized quadratic form distance functions as well as the Euclidean distance function since ellipsoid queries parameters may vary with the user and situation. In this paper, we present the spatial transformation technique that yields a new search method for adaptive ellipsoid queries with quadratic form distance functions. The basic idea is to transform the bounding rectangles in the original space, wherein distance from a query point is measured by quadratic form distance functions, into spatial objects in a new space wherein distance is measured by Euclidean distance functions. Our method significantly reduces CPU cost due to the distance approximation by the spatial transformation; exact distance evaluations are avoided for most of the accessed bounding rectangles in the index structures. We also present the multiple spatial transformation technique as an extension of the spatial transformation technique. The multiple spatial transformation technique adjusts the tree structures to suit typical ellipsoid queries; the search algorithm utilizes the adjusted structure. This technique reduces both page accesses and CPU time for ellipsoid queries. Experiments using various matrices and index structures demonstrate the superiority of the proposed methods.  Entity 2: title similarity query processing using disk arrays authors apostolos n. papadopoulos , yannis manolopoulos venue international conference on management of data year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: An important challenge to web technologies such as proxy caching, web portals, and application servers is keeping cached data up-to-date. Clients may have different preferences for the latency and recency of their data. Some prefer the most recent data, others will accept stale cached data that can be delivered quickly. Existing approaches to maintaining cache consistency do not consider this diversity and may increase the latency of requests, consume excessive bandwidth, or both. Further, this overhead may be unnecessary in cases where clients will tolerate stale data that can be delivered quickly. This paper introduces latency-recency profiles, a set of parameters that allow clients to express preferences for their different applications. A cache or portal uses profiles to determine whether to deliver a cached object to the client or to download a fresh object from a remote server. We present an architecture for profiles that is both scalable and straightforward to implement at a cache. Experimental results using both synthetic and trace data show that profiles can reduce latency and bandwidth consumption compared to existing approaches, while still delivering fresh data in many cases. When there is insufficient bandwidth to answer all requests at once, profiles significantly reduce latencies for all clients.  Entity 2: title update propagation strategies for improving the quality of data on the web authors alexandros labrinidis , nick roussopoulos venue very large data bases year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: trees that minimize the computation and communication costs of parallel execution. We address the problem of finding parallel plans for SQL queries using the two-phase approach of join ordering and query rewrite (JOQR) followed by parallelization. We focus on the JOQR phase and develop optimization algorithms that account for communication as well as computation costs. Using a model based on representing the partitioning of data as a color, we devise an efficient algorithm for the problem of choosing the partitioning attributes in a query tree so as to minimize total cost. We extend our model and algorithm to incorporate the interaction of data partitioning with conventional optimization choices such as access methods and strategies for computing operators. Our algorithms apply to queries that include operators such as grouping, aggregation, intersection and set difference in addition to joins.  Entity 2: title outerjoin simplification and reordering for query optimization authors c &#233; sar galindo-legaria , arnon rosenthal venue acm transactions on database systems ( tods ) year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We consider an environment where distributed data sources continuously stream updates to a centralized processor that monitors continuous queries over the distributed data. Significant communication overhead is incurred in the presence of rapid update streams, and we propose a new technique for reducing the overhead. Users register continuous queries with precision requirements at the central stream processor, which installs filters at remote data sources. The filters adapt to changing conditions to minimize stream rates while guaranteeing that all continuous queries still receive the updates necessary to provide answers of adequate precision at all times. Our approach enables applications to trade precision for communication overhead at a fine granularity by individually adjusting the precision constraints of continuous queries over streams in a multi-query workload. Through experiments performed on synthetic data simulations and a real network monitoring implementation, we demonstrate the effectiveness of our approach in achieving low communication overhead compared with alternate approaches.  Entity 2: title on computing correlated aggregates over continual data streams authors johannes gehrke , flip korn , divesh srivastava venue international conference on management of data year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The database area has been one of those areas of computerscience which have very directly been driven by applicationrequirements; this is true today in three ways: First, the userswant more application specific support from the database, and theyexpect the DBMS to have more semantic application knowledge.Second, users want database support for new applications which aresometimes far from the traditional database applications andintroduce completely new requirements as well as the need tosmoothly integrate database technology with other advancedtechnologies (e.g. neural nets) in one application. Finally, theembedding of databases into interactive work environments - forinstance, the use of databases in cooperative environments(computer supported cooperative work) - forces the databasecommunity to reconsider some of the traditional beliefs aboutdatabases.  Entity 2: title database research at the university of illinois at urbana-champaign authors m. winslett , k. chang , a. doan , j. han , c. zhai , y. zhou venue acm sigmod record year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Abstract. The view selection problem is to choose a set of views to materialize over a database schema, such that the cost of evaluating a set of workload queries is minimized and such that the views fit into a prespecified storage constraint. The two main applications of the view selection problem are materializing views in a database to speed up query processing, and selecting views to materialize in a data warehouse to answer decision support queries. In addition, view selection is a core problem for intelligent data placement over a wide-area network for data integration applications and data management for ubiquitous computing. We describe several fundamental results concerning the view selection problem. We consider the problem for views and workloads that consist of equality-selection, project and join queries, and show that the complexity of the problem depends crucially on the quality of the estimates that a query optimizer has on the size of the views it is considering to materialize. When a query optimizer has good estimates of the sizes of the views, we show a somewhat surprising result, namely, that an optimal choice of views may involve a number of views that is exponential in the size of the database schema. On the other hand, when an optimizer uses standard estimation heuristics, we show that the number of necessary views and the expression size of each view are polynomially bounded.  Entity 2: title a formal perspective on the view selection problem authors rada chirkova , alon y. halevy , dan suciu venue the vldb journal -- the international journal on very large data bases year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper provides an overview of the current database research activities within the Intelligent Information Systems Group in the Department of Computer Science and Engineering at Arizona State University. The focus of our research is on the integration of data and knowledge management issues, with specific emphasis on multimedia systems, object-oriented databases, active databases, deductive databases, and heterogeneous, distributed database environments.  Entity 2: title research in database engineering at the university of namur authors jean-luc hainaut venue acm sigmod record year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: A data warehouse materializes views derived from data that may not reside at the warehouse. Maintaining these views effciently in response to base updates is diffcult, since it may involve querying external sources where the base data reside. This paper considers the problem of view self-maintenance, where the views are maintained without using all the base data. Without full use of the base data, however, maintaining a view unambiguously is not always possible. Thus, the two critical questions that must be addressed are to determine, in a given situation, whether a view is maintainable, and how to maintain it. W e provide algorithms that answer these questions for a general class of views, and for an important subclass, generate SQL queries that test whether a view is self-maintainable and update the view if it is. We improve significantly on previous work by solving the view self-maintenance problem in the presence of multiple views, with optional access to a subset of the base data, and under arbitrary mixes of insertions and deletions. We provide better insight into the problem by showing that view self-maintainability can be reduced to the problem of deciding query containment.  Entity 2: title incremental clustering for mining in a data warehousing environment authors martin ester , hans-peter kriegel , j &#246; rg sander , michael wimmer , xiaowei xu venue very large data bases year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper provides an overview of the current database research activities within the Intelligent Information Systems Group in the Department of Computer Science and Engineering at Arizona State University. The focus of our research is on the integration of data and knowledge management issues, with specific emphasis on multimedia systems, object-oriented databases, active databases, deductive databases, and heterogeneous, distributed database environments.  Entity 2: title florida international university high performance database research center authors naphtali rishe , wei sun , david barton , yi deng , cyril orji , michael alexopoulos , leonardo loureiro , carlos ordonez , mario sanchez , artyom shaposhnikov venue acm sigmod record year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Given the complexity of many queries over a Data Warehouse (DW), it is interesting to precompute and store in the DW the answer sets of some demanding operations, so called materialized views. In this paper, we present an algorithm, including its experimental evaluation, which allows the materialization of several views simultaneously without losing sight of processing costs for queries using these materialized views.  Entity 2: title report on fqas 2002 : fifth international conference on flexible query answering systems authors amihai motro , troels andreasen venue acm sigmod record year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper, we propose a novel formulation for distance-based outliers that is based on the distance of a point from its kth nearest neighbor. We rank each point on the basis of its distance to its kth nearest neighbor and declare the top n points in this ranking to be outliers. In addition to developing relatively straightforward solutions to finding such outliers based on the classical nested-loop join and index join algorithms, we develop a highly efficient partition-based algorithm for mining outliers. This algorithm first partitions the input data set into disjoint subsets, and then prunes entire partitions as soon as it is determined that they cannot contain outliers. This results in substantial savings in computation. We present the results of an extensive experimental study on real-life and synthetic data sets. The results from a real-life NBA database highlight and reveal several expected and unexpected aspects of the database. The results from a study on synthetic data sets demonstrate that the partition-based algorithm scales well with respect to both data set size and data set dimensionality.  Entity 2: title cure : an efficient clustering algorithm for large databases authors sudipto guha , rajeev rastogi , kyuseok shim venue international conference on management of data year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We develop a precise syntax and semantics of SchemaSQL in a manner that extends traditional SQL syntax and semantics, and demonstrate the following. (1) SchemaSQL retains the flavour of SQL while supporting querying of both data and meta-data. (2) It can be used to represent data in a database in a structure substantially different from original database, in which data and meta-data may be interchanged. (3) It also permits the creation of views whose schema is dynamically dependent on the contents of the input instance. (4) While aggregation in SQL is restricted to values occurring in one column at a time, SchemaSQL permits \u201chorizontal\u201d aggregation and even aggregation over more general \u201cblocks\u201d of information. (5) SchemaSQL provides a great facility for interoperability and data/meta-data management in relational multi-database systems. We provide many examples to illustrate our claims.  Entity 2: title a language based multidatabase system authors eva k &#252; hn , thomas tschernko , konrad schwarz venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper abstracts from the underlying platforms and instead considers the requirements to CRM solutions for the various communication channels, in order to devise a uniform and corporate-wide data architecture for an omni-channel customer view to maximize the business clients' value in customer retention and customer centric analytics. Especially, online customer segmentation integrating channel usage and preferences is presented as a very promising means for constructing a self-energising information loop which will lead to highly improved customer service along the whole customer journey.  Entity 2: title evolution and change in data management - issues and directions authors john f. roddick , lina al-jadir , leopoldo bertossi , marlon dumas , florida estrella , heidi gregersen , kathleen hornsby , jens lufter , federica mandreoli , tomi m &#228; nnist &#246; , enric mayol , lex wedemeijer venue acm sigmod record year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper, we consider a dynamic self-tuning approach to reorganization in a shared nothing system. We introduce a new index-based method that faciliates fast and efficient migration of data. Our solution incorporates a globally height-balanced structure and load tracking at different levels of granularity. We conducted an extensive performance study, and implemented the methods on the Fujitsu AP3000 machine. Both the simulation and empirical results demonstratic that our proposed method is indeed scalable and effective in correcting any deterioration in system throughput.  Entity 2: title dynamic load balancing in hierarchical parallel database systems authors luc bouganim , daniela florescu , patrick valduriez venue very large data bases year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Similarity queries are fundamental operations that are used extensively in many modern applications, whereas disk arrays are powerful storage media of increasing importance. The basic trade-off in similarity query processing in such a system is that increased parallelism leads to higher resource consumptions and low throughput, whereas low parallelism leads to higher response times. Here, we propose a technique which is based on a careful investigation of the currently available data in order to exploit parallelism up to a point, retaining low response times during query processing. The underlying access method is a variation of the R*-tree, which is distributed among the components of a disk array, whereas the system is simulated using event-driven simulation. The performance results conducted, demonstrate that the proposed approach outperforms by factors a previous branch-and-bound algorithm and a greedy algorithm which maximizes parallelism as much as possible. Moreover, the comparison of the proposed algorithm to a hypothetical (non-existing) optimal one (with respect to the number of disk accesses) shows that the former is on average two times slower than the latter.  Entity 2: title improving adaptable similarity query processing by using approximations authors mihael ankerst , bernhard braunm &#252; ller , hans-peter kriegel , thomas seidl venue very large data bases year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight \"quality\" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.  Entity 2: title reminiscences on influential papers authors kenneth a. ross venue acm sigmod record year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this special issue on metadata management, we present a new work on creating, gathering, managing, and understanding metadata. The work in this issue highlights the reality that the lack of metadata and effective techniques for managing them is currently one of the biggest challenges to meaningful use and sharing of the wealth (or should we say glut) of data available today.  Entity 2: title editorial authors richard snodgrass venue acm transactions on database systems ( tods ) year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Together with techniques developed for relational databases, this basis in logic means that deductive databases are capable of handling large amounts of information as well as performing reasoning based on that information. There are many application areas for deductive database technology. One area is that of decision support systems. In particular, the exploitation of an organization's resources requires fi~tbniy sufficient information about the current and future status of the resources themselves, but also a way of reasoning effectively about plans for the future. The present generation of decision support systems are severely deficient when it comes to reasoning about future plans. Deductive database technology is an appropriate solution to this problem. Another fruitful application area is that of expert systems. There are many computing applications in which there are large amounts of information, from which the important facts may be distilled by a simple yet tedious analysis. For example, medical analysis and monitoring can generate a large amount of data, and an error can have disastrous consequences.We demonstrate the COUGAR System, a new distributed data management infrastructure that scales with the growth of sensor interconnectivity and computational power on the sensors over the next decades. Our system resides directly on the sensor nodes and creates the abstraction of a single processing node without centralizing data or computation.  Entity 2: title a language based multidatabase system authors eva k &#252; hn , thomas tschernko , konrad schwarz venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The database community has been researching problems in similarity query for time series databases for many years. The techniques developed in the area might shed light on the query by humming problem. In this demo, we treat both the melodies in the music databases and the user humming input as time series. Such an approach allows us to integrate many database indexing techniques into a query by humming system, improving the quality of such system over the traditional (contour) string databases approach. We design special searching techniques that are invariant to shifting, time scaling and local time warping. This makes the system robust and allows more flexible user humming input.  Entity 2: title cache coherency in oracle parallel server authors boris klots venue very large data bases year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Digital watermarking for relational databases emerged as a candidate solution to provide copyright protection, tamper detection, traitor tracing, maintaining integrity of relational data. Many watermarking techniques have been proposed in the literature to address these purposes. In this paper, we survey the current state-of-theart and we classify them according to their intent, the way they express the watermark, the cover type, the granularity level, and their verifiability.  Entity 2: title managing intervals efficiently in object-relational databases authors hans-peter kriegel , marco p &#246; tke , thomas seidl venue very large data bases year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The extensions support new data types such as point, circle, etc., and functions such as confains, interval, text-contains, etc. Let the tables Policies (policy-id, name, address, location, vehicle-type, . . .) and Claims (policy-id, claim-tag, accident-date, accident-location, accident-report, . . .) represent the partial schema containing both SQL\u201992 and user defined data types (UDTs). Consider a scenario in a targeted marketing application that requires a mailing list of all customers within 5 miles of point L, who have insured a \u2018sports utility vehicle\u2019 and were involved in a \u2018rear-ended\u2019 accident in the past 3 years.  Entity 2: title semantic integration in heterogeneous databases using neural networks authors wen-syan li , chris clifton venue very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Arrays are a common and important class of data. At present, database systems do not provide adequate array support: arrays can neither be easily defined nor conveniently manipulated. Further, array manipulations are not optimized. This paper describes a language called the Array Manipulation Language (AML), for expressing array manipulations, and a collection of optimization techniques for AML expressions.In the AML framework for array manipulation, arbitrary externally-defined functions can be applied to arrays in a structured manner. AML can be adapted to different application domains by choosing appropriate external function definitions. This paper concentrates on arrays occurring in databases of digital images such as satellite or medical images.AML queries can be treated declaratively and subjected to rewrite optimizations. Rewriting minimizes the number of applications of potentially costly external functions required to compute a query result. AML queries can also be optimized for space. Query results are generated a piece at a time by pipelined execution plans, and the amount of memory required by a plan depends on the order in which pieces are generated. An optimizer can consider generating the pieces of the query result in a variety of orders, and can efficiently choose orders that require less space.  Entity 2: title similarity query processing using disk arrays authors apostolos n. papadopoulos , yannis manolopoulos venue international conference on management of data year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: When the schema of an object-oriented database system is modified, the database needs to be changed in such a way that the schema and the database remain consistent with each other. This paper describes the algorithm implemented in the new forthcoming release of the 02 object database for automatically bringing the database to a consistent state after a schema update has been performed. The algorithm, which uses a deferred strategy to update the database, is a revised and extended version of the screening algorithm first sketched in [7].  Entity 2: title implementing lazy database updates for an object database system authors fabrizio ferrandina , thorsten meyer , roberto zicari venue very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We describe the design and implementation of a new data layout scheme, called multi-dimensional clustering, in DB2 Universal Database Version 8. Many applications, e.g., OLAP and data warehousing, process a table or tables in a database using a multi-dimensional access paradigm. Currently, most database systems can only support organization of a table using a primary clustering index. Secondary indexes are created to access the tables when the primary key index is not applicable. Unfortunately, secondary indexes perform many random I/O accesses against the table for a simple operation such as a range query. Our work in multi-dimensional clustering addresses this important deficiency in database systems. Multi-Dimensional Clustering is based on the definition of one or more orthogonal clustering attributes (or expressions) of a table. The table is organized physically by associating records with similar values for the dimension attributes in a cluster. We describe novel techniques for maintaining this physical layout efficiently and methods of processing database operations that provide significant performance improvements. We show results from experiments using a star-schema database to validate our claims of performance with minimal overhead.  Entity 2: title mining multi-dimensional constrained gradients in data cubes authors guozhu dong , jiawei han , joyce m. w. lam , jian pei , ke wang venue very large data bases year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We present a framework which allows the user to access and manipulate data uniformly, regardless of whether it resides in a database or in the file system (or in both). A key issue is the performance of the system. We show that text indexing, combined with newly developed optimization techniques, can be used to provide an efficient high level interface to information stored in files. Furthermore, using these techniques, some queries can be evaluated significantly faster than in standard database implementations. We also study the tradeoff between efficiency and the amount of indexing.  Entity 2: title optimization of queries with user-defined predicates authors surajit chaudhuri , kyuseok shim venue acm transactions on database systems ( tods ) year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The VQBD project addresses the following problem: What is the best way to explore an XML document of unknown structure and content? We focus on XML documents that are too large to browse in their entirety, even with the assistance of pretty-printing software (e.g., multi-megabyte or larger XML documents). In this context, we use the term data exploration to refer to the process by which a user gathers the information needed to use the data for a speci c purpose (e.g., generating a report, writing queries, building user interfaces, writing applications). In a relational or object database, the schema (e.g., table de nitions, class de nitions, integrity constraints, and stored procedures) provides some of the information necessary for writing queries and applications. However, the schema is rarely su cient for these tasks. Typically, one must probe and browse the database to discover data coverage, typical and exceptional values, and other information required to gain a better understanding of the database. In an XML environment, the need for such data exploration is much greater because it is quite likely that the XML data of interest is not accompanied by a schema.  Entity 2: title management of semistructured data authors dan suciu venue acm sigmod record year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Although there are many applications where an object-oriented data model is a good way of representing and querying data, current object database systems are unable to handle objects whose attributes are uncertain. In this article, we extend previous work by Kornatzky and Shimony to develop an algebra to handle object bases with uncertainty. We propose concepts of consistency for such object bases, together with an NP-completeness result, and classes of probabilistic object bases for which consistency is polynomially checkable. In addition, as certain operations involve conjunctions and disjunctions of events, and as the probability of conjunctive and disjunctive events depends both on the probabilities of the primitive events involved as well as on what is known (if anything) about the relationship between the events, we show how all our algebraic operations may be performed under arbitrary probabilistic conjunction and disjunction strategies. We also develop a host of equivalence results in our algebra, which may be used as rewrite rules for query optimization. Last but not least, we have developed a prototype probabilistic object base server on top of ObjectStore. We describe experiments to assess the efficiency of different possible rewrite rules.  Entity 2: title probabilistic temporal databases , i : algebra authors alex dekhtyar , robert ross , v. s. subrahmanian venue acm transactions on database systems ( tods ) year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper we present a novel technique for cost estimation of user-defined methods in advanced database systems. This technique is based on multi-dimensional histograms. We explain how the system collects statistics on the method that a database user defines and adds to the system. From these statistics a multi-dimensional histogram is built. Afterwards, this histrogram can be used for estimating the cost of the target method whenever this method is referenced in a query. This cost estimation is needed by the optimizer of the database system since this cost estimation needs to know the cost of a method in order to place it at its optimal position in the Query Execution Plan (QEP). We explain here how our technique works and we provide an example to better verify its functionality.  Entity 2: title implementation of magic-sets in a relational database system authors inderpal singh mumick , hamid pirahesh venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The results show that current hardware technology trends have significantly changed the performance tradeoffs considered in past studies. A simplistic data placement strategy based on the new results is developed and shown to perform well for a variety of workloads.  Entity 2: title data partitioning and load balancing in parallel disk systems authors peter scheuermann , gerhard weikum , peter zabback venue the vldb journal -- the international journal on very large data bases year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Formulating queries on networked information systems is laden with problems: data diversity, data complexity, network growth, varied user base, and slow network access. This paper proposes a new approach to a network query user interface which consists of two phases: query preview and query refinement. This new approach is based on dynamic queries and tight coupling, guiding users to rapidly and dynamically eliminate undesired items, reduce the data volume to a manageable size, and refine queries locally before submission over a network. A two-phase dynamic query system for NASA's Earth Observing Systems--Data Information Systems (EOSDIS) is presented. The prototype was well received by the team of scientists who evaluated the interface.  Entity 2: title integrating modelling systems for environmental management information systems authors david j. abel , kerry taylor , dean kun venue acm sigmod record year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Nearest neighbor search in high dimensional spaces is an interesting and important problem which is relevant for a wide variety of novel database applications. As recent results show, however, the problem is a very di cult one, not only with regards to the performance issue but also to the quality issue. In this paper, we discuss the quality issue and identify a new generalized notion of nearest neighbor search as the relevant problem in high dimensional space. In contrast to previous approaches, our new notion of nearest neighbor search does not treat all dimensions equally but uses a quality criterion to select relevant dimensions (projections) with respect to the given query.  Entity 2: title the sr-tree : an index structure for high-dimensional nearest neighbor queries authors norio katayama , shin ` ichi satoh venue international conference on management of data year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases, that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.  Entity 2: title object-oriented , rapid application development in a pc database environment authors corporate fox development team microsoft venue international conference on management of data year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.  Entity 2: title book reviews authors karl aberer venue acm sigmod record year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight \"quality\" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.  Entity 2: title reminiscences on influential papers authors kenneth a. ross venue acm sigmod record year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.  Entity 2: title converting relational to object-oriented databases authors joseph fong venue acm sigmod record year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Much of the functionality required to support first class views can be generated semi-automatically, if the derivations between layers are declarative (e.g., SQL, rather than Java). We present a framework where propagation rules can be defined, allowing the flexible and incremental specification of view semantics, even by non-programmers. Finally, we describe research areas opened up by this approach.  Entity 2: title reminiscences on influential papers authors richard snodgrass venue acm sigmod record year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Object-Relational DBMSs have been receiving a great deal of attention from industry analysts and press as the next generation of database management systems. The motivation for a next generation DBMS is driven by the reality of shortened business cycles. This dynamic environment demands fast, cost-effective, time-to-market of new or modified business processes, services, and products. To support this important business need, the next generation DBMS must: 1. leverage the large investments made in existing relational technology, both in data and skill set; 2. Take advantage of the flexibility, productivity, and performance benefits of OO modeling; and 3. Integrate robust DBMS services for production quality systems. The objective of this article is to provide a brief overview of UniSQL's commercial object-relational database management system.  Entity 2: title enhanced abstract data types in object-relational databases authors praveen seshadri venue the vldb journal -- the international journal on very large data bases year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In the early 1980's, researchers recognized that semantic information stored in databases as integrity constraints could be used for query optimization. A new set of techniques called semantic query optimization (SQO) was developed. Some of the ideas developed for SQO have been used commercially, but to the best of our knowledge, no extensive implementations of SQO exist today. In this paper, we describe an implementation of two SQO techniques, Predicate Introduction and Join Elimination, in DB2 Universal Database. We present the implemented algorithms and performance results using the TPCD and APB-1 OLAP benchmarks. Our experiments show that SQO can lead to dramatic query performance improvements. A crucial aspect of our implementation of SQO is the fact that it does not rely on complex integrity constraints (as many previous SQO techniques did); we use only referential integrity constraints and check constraints.  Entity 2: title implementation of magic-sets in a relational database system authors inderpal singh mumick , hamid pirahesh venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.  Entity 2: title book reviews authors karl aberer venue acm sigmod record year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Streams are continuous data feeds generated by such sources as sensors, satellites, and stock feeds. Monitoring applications track data from numerous streams, filtering them for signs of abnormal activity, and processing them for purposes of filtering,  Entity 2: title open object database management systems authors jos &#233; a. blakeley venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Although many of the problems that must be solved by an object-oriented database system are similar to problems solved by relational systems, there are also significant problems that are unique. In particular, an object query can include a path expression to traverse a number of related collections. The order of collection traversals given by the path expression may not be the most efficient to process the query. This generates a critical problem for object query optimizer to select an algorithm to process the query based on direct navigation or various combinations of joins. This paper studies the different algorithms to process path expressions with predicates, including depth first navigation, forward and reverse joins. Using a cost model, it then compares their performances in different cases, according to memory size, selectivity of predicates, fan out between collections, etc.. It also presents a heuristic-based algorithm to find profitable n-ary operators for traversing collections, thus reducing the search space of query plans to process a query with a qualified path expression. An implementation based on the O2 system demonstrates the validity of the results.  Entity 2: title query unnesting in object-oriented databases authors leonidas fegaras venue international conference on management of data year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.  Entity 2: title enhanced abstract data types in object-relational databases authors praveen seshadri venue the vldb journal -- the international journal on very large data bases year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We propose a new client-side data-caching scheme for relational databases with a central server and multiple clients. Data are loaded into each client cache based on queries executed on the central database at the server. These queries are used to form predicates that describe the cache contents. A subsequent query at the client may be satisfied in its local cache if we can determine that the query result is entirely contained in the cache. This issue is called  cache completeness . A separate issue,  cache currency , deals with the effect on client caches of updates committed at the central database. We examine the various performance tradeoffs and optimization issues involved in addressing the questions of cache currency and completeness using predicate descriptions and suggest solutions that promote good dynamic behavior. Lower query-response times, reduced message traffic, higher server throughput, and better scalability are some of the expected benefits of our approach over commonly used relational server-side and object ID-based or page-based client-side caching.  Entity 2: title aries/csa : a method for database recovery in client-server architectures authors c. mohan , inderpal narang venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Sensor networks are typically unattended because of their deployment in hazardous, hostile or remote environments. This makes the problem of conserving energy at individual sensor nodes challenging. S-MAC and PAMAS are two MAC protocols which periodically put nodes (selected at random) to sleep in order to achieve energy savings. Unlike these protocols, we propose an approach in which node duty cycles (i.e sleep and wake schedules) are based on their criticality. A distributed algorithm is used to find sets of winners and losers, who are then assigned appropriate slots in our TDMA based MAC protocol. We introduce the concept of of energy-criticality of a sensor node as a function of energies and traffic rates. Our protocol makes more critical nodes sleep longer, thereby balancing the energy consumption. Simulation results show that the performance of the protocol with increase in traffic load is better than existing protocols with increase in traffic load is better than existing protocols, thereby illustrating the energy balancing nature of the approach.  Entity 2: title power efficient data gathering and aggregation in wireless sensor networks authors h &#252; seyin &#214; zg &#252; r tan , ibrahim k &#246; rpeo &#487; lu venue acm sigmod record year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We address the problem of query rewriting for TSL, a language for querying semistructured data. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting queries, i.e., queries that access the views and produce the same result as q. Our algorithm is based on appropriately generalizing ontainment mappings, the chase, and query composition \u2014 techniques that were developed for structured, relational data. We also develop an algorithm for equivalence checking of TSL queries. We show that the algorithm is sound and complete for TSL, i.e., it always finds every non-trivial TSL rewriting query of q, and we discuss its complexity. We extend the rewriting algorithm to use some forms of structural constraints (such as DTDs) and find more opportunities for query rewriting.  Entity 2: title dataguides : enabling query formulation and optimization in semistructured databases authors roy goldman , jennifer widom venue very large data bases year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Query processing is one of the most, critical issues in Object-Oriented DBMSs. Extensible opt,imizers with efficient, search strategies require a cost model to select the most efficient execution plans. In this paper we propose and partially validate a generic cost-model for Object-Oriented DBMSs. The storage model and its access methods support clust,ered and nested collections, links, and path indexes. Queries may involve complex predicates with qualified path expressions. We propose a, method for estimating the number of block a,ccesses to clustered collections and a paramet,erized execution model for evaluating predicat,es. We estimate the costs of path expression traversals in different cases of physical clustering of the supporting collections. Thr model is validated through experiments with the 02 DBMS.  Entity 2: title repositories and object oriented databases authors philip a. bernstein venue acm sigmod record year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Four different pointer swizzling techniques allowing object replacement are investigated and compared with the performance of an object manager employing no pointer swizzling. The extensive qualitative and quantitative evaluation\u2014only part of which could be presented in this article\u2014demonstrate that there is noone superior pointer swizzling strategy forall application profiles. Therefore, an adaptable object base run-time system is devised that employs the full range of pointer swizzling strategies, depending on the application profile characteristics that are determined by, for example, monitoring in combination with sampling, user specifications, and/or program analysis.  Entity 2: title dual-buffering strategies in object bases authors alfons kemper , donald kossmann venue very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We confront the promises of active database systems with the result of their use by application developers. The main problems encountered are iusufficient methodological support in analysis and design, the lack of standardization, missing development and administration tools for triggers, and weak performance. We concentrate on performance because we discovered it is one the maiu reasons that makes users reluctant to use active rules iu the development of large applications. We show, using simple concrete examples, that optimizing large applications is rendered difficult by the separation of transactions and triggers and the misunderstanding of their subtle interactions. We argue that tools, which provide assistance to programmers, database administrators, and database designers to optimize their applications and master application evolution is strongly needed.  Entity 2: title mpeg-7 and multimedia database systems authors harald kosch venue acm sigmod record year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Regular expressions with capture variables, also known as \"regex formulas,'' extract relations of spans (interval positions) from text. These relations can be further manipulated via the relational Algebra as studied in the context of \"document spanners,\" Fagin et al.'s formal framework for information extraction. We investigate the complexity of querying text by Conjunctive Queries (CQs) and Unions of CQs (UCQs) on top of regex formulas. Such queries have been investigated in prior work on document spanners, but little is known about the (combined) complexity of their evaluation. We show that the lower bounds (NP-completeness and W[1]-hardness) from the relational world also hold in our setting; in particular, hardness hits already single-character text. Yet, the upper bounds from the relational world do not carry over. Unlike the relational world, acyclic CQs, and even gamma-acyclic CQs, are hard to compute. The source of hardness is that it may be intractable to instantiate the relation defined by a regex formula, simply because it has an exponential number of tuples. Yet, we are able to establish general upper bounds. In particular, UCQs can be evaluated with polynomial delay, provided that every CQ has a bounded number of atoms (while unions and projection can be arbitrary). Furthermore, UCQ evaluation is solvable with FPT (Fixed-Parameter Tractable) delay when the parameter is the size of the UCQ.  Entity 2: title information visualization authors tiziana catarci , isabel f. cruz venue acm sigmod record year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Deductive databases generalize relational databases by providing support for recursive views and non-atomic data. Aditi is a deductive system based on the client-server model; it is inherently multi-user and capable of exploiting parallelism on shared-memory multiprocessors. The back-end uses relational technology for efficiency in the management of disk-based data and uses optimization algorithms especially developed for the bottom-up evaluation of logical queries involving recursion. The front-end interacts with the user in a logical language that has more expressive power than relational query languages. We present the structure of Aditi, discuss its components in some detail, and present performance figures.  Entity 2: title tail recursion elimination in deductive databases authors kenneth a. ross venue acm transactions on database systems ( tods ) year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Table of contentsI1 Proceedings of the 4th World Conference on Research IntegrityConcurrent Sessions:1. Countries' systems and policies to foster research integrityCS01.1 Second time around: Implementing and embedding a review of responsible conduct of research policy and practice in an Australian research-intensive universitySusan Patricia O'BrienCS01.2 Measures to promote research integrity in a university: the case of an Asian universityDanny Chan, Frederick Leung2. Examples of research integrity education programmes in different countriesCS02.1 Development of a state-run \u201ccyber education program of research ethics\u201d in KoreaEun Jung Ko, Jin Sun Kwak, TaeHwan Gwon, Ji Min Lee, Min-Ho LeeCS02.3 Responsible conduct of research teachers\u2019 training courses in Germany: keeping on drilling through hard boards for more RCR teachersHelga Nolte, Michael Gommel, Gerlinde Sponholz3. The research environment and policies to encourage research integrity  Entity 2: title workshop report on experiences using object data management in the real-world authors akmal b. chaudhri venue acm sigmod record year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The abstraction approach describes each object in the symbolic image by using a vector consisting of the values of some of its features (e.g., shape, genus, etc.). The approaches differ in the way in which responses to queries are computed. In the classification approach, images are retrieved on the basis of whether or not they contain objects that have the same classification as the objects in the query. On the other hand, in the abstraction approach, retrieval is on the basis of similarity of feature vector values of these objects. Methods of integrating these two approaches into a relational multimedia database management system so that symbolic images can be stored and retrieved based on their content are described. Schema definitions and indices that support query specifications involving spatial as well as contextual constraints are presented. Spatial constraints may be based on both locational information (e.g., distance) and relational information (e.g., north of). Different strategies for image retrieval for a number of typical queries using these approaches are described.  Entity 2: title detection and classification of intrusions and faults using sequences of system calls authors jo &#227; o b. d. cabrera , lundy lewis , raman k. mehra venue acm sigmod record year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Abstract.Due to their expressive power, regular expressions (REs) are quickly becoming an integral part of language specifications for several important application scenarios. Many of these applications have to manage huge databases of RE specifications and need to provide an effective matching mechanism that, given an input string, quickly identifies the REs in the database that match it. In this paper, we propose the RE-tree, a novel index structure for large databases of RE specifications. Given an input query string, the RE-tree speeds up the retrieval of matching REs by focusing the search and comparing the input string with only a small fraction of REs in the database. Even though the RE-tree is similar in spirit to other tree-based structures that have been proposed for indexing multidimensional data, RE indexing is significantly more challenging since REs typically represent infinite sets of strings with no well-defined notion of spatial locality. To address these new challenges, our RE-tree index structure relies on novel measures for comparing the relative sizes of infinite regular languages. We also propose innovative solutions for the various RE-tree operations including the effective splitting of RE-tree nodes and computing a \"tight\" bounding RE for a collection of REs. Finally, we demonstrate how sampling-based approximation algorithms can be used to significantly speed up the performance of RE-tree operations. Preliminary experimental results with moderately large synthetic data sets indicate that the RE-tree is effective in pruning the search space and easily outperforms naive sequential search approaches.  Entity 2: title the hcc-tree : an efficient index structure for object oriented databases authors b. sreenath , s. seshadri venue very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The problem of analyzing and classifying conceptual schemas is becomig increasingly important due to the availability of a large number of schemas related to existing applications. The purposes of schema analysis and classification activities can be different: to extract information on intensional properties of legacy systems in order to restructure or migrate to new architectures; to build libraries of reference conceptual components to be used in building new applications in a given domain; and to identify information flows and possible replication of data in an organization. This article proposes a set of techniques for schema analysis and classification to be used separately or in combination. The techniques allow the analyst to derive significant properties from schemas, with human intervention limited as far as possible. In particular, techniques for associating descriptors with schemas, for abstracting reference conceptual schemas based on schema clustering, and for determining schema similarity are presented. A methodology for systematic schema analysis is illustrated, with the purpose of identifying and abstracting into reference components the similar and potentially reusable parts of a set of schemas. Experiences deriving from the application of the proposed techniques and methodology on a large set of Entity-Relationship conceptual schemas of information systems in the Italian Public Administration domain are described  Entity 2: title bulk-loading techniques for object databases and an application to relational data authors sihem amer-yahia , sophie cluet , claude delobel venue very large data bases year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Integrating data and knowledge from multiple heterogeneous sources -- like databases, knowledge bases or specific software packages -- is often required for answering certain queries. Recently, a powerful framework for defining mediated views spanning multiple knowledge bases by a set of constrained rules was proposed [24, 4, 16]. We investigate the materialization of these views by unfolding the view definition and the efficient maintenance of the resulting materialized mediated view in case of updates. Thereby, we consider two kinds of updates: updates to the view and updates to the underlying sources. For each of these two cases several efficient algorithms maintaining materialized mediated views are given. We improve on previous algorithms like the DRed algorithm [12] and introduce a new fixpoint operator WP which -- opposed to the standard fixpoint operator TP [9] -- allows us to correctly capture the update's semantics without any recomputation of the materialized view.  Entity 2: title incremental maintenance for materialized views over semistructured data authors serge abiteboul , jason mchugh , michael rys , vasilis vassalos , janet l. wiener venue very large data bases year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The paper discusses the LHAM concepts, including concurrency control and recovery, our full-fledged LHAM implementation, and experimental performance results based on this implementation. A detailed comparison with the TSB-tree, both analytically and based on experiments with real implementations, shows that LHAM is highly superior in terms of insert performance, while query performance is in almost all cases at least as good as for the TSB-tree; in many cases it is much better.  Entity 2: title design , implementation , and performance of the lham log-structured history data access method authors peter muth , patrick e. o'neil , achim pick , gerhard weikum venue very large data bases year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Content placement algorithm: An ACDN must decide which applications to deploy where and when. Content placement is solved trivially in traditional CDNs by cache replacement algorithms.  Entity 2: title reminiscences in influential papers authors richard snodgrass venue acm sigmod record year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper presents the design of a read-optimized relational DBMS that contrasts sharply with most current systems, which are write-optimized. Among the many differences in its design are: storage of data by column rather than by row, careful coding and packing of objects into storage including main memory during query processing, storing an overlapping collection of column-oriented projections, rather than the current fare of tables and indexes, a non-traditional implementation of transactions which includes high availability and snapshot isolation for read-only transactions, and the extensive use of bitmap indexes to complement B-tree structures.  Entity 2: title highly concurrent cache consistency for indices in client-server database systems authors markos zaharioudakis , michael j. carey venue international conference on management of data year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Existing and past generations of Prolog compilers have left deduction to run-time and this may account for the poor run-time performance of existing Prolog systems. Our work tries to minimize run-time deduction by shifting the deductive process to compile-time. In addition, we offer an alternative inferencing procedure based on translating logic to mixed integer programming. This makes available for research and implementation in deductive databases, all the theorems, algorithms, and software packages developed by the operations research community over the past 50 years. The method keeps the same query language as for disjunctive deductive databases, only the inferencing procedure changes. The language is purely declarative, independent of the order of rules in the program, and independent of the order in which literals occur in clause bodies. The technique avoids Prolog's problem of infinite looping. It saves run-time by doing primary inferencing at compile-time. Furthermore, it is incremental in nature. The first half of this article translates disjunctive clauses, integrity constraints, and database facts into Boolean equations, and develops procedures to use mixed integer programming methods to compute equations, and develops procedures to use mixed integer programming methods to compute equations, and develops procedures to use mixed integer programming methods to compute equations, and develops procedures to use mixed integer programming methods to compute \u2014least models of definite deductive databases, and \u2014minimal models and the Generalized Closed World Assumption of disjunctive databases.  Entity 2: title implementing database operations using simd instructions authors jingren zhou , kenneth a. ross venue international conference on management of data year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper, we examine presentations in three different domains (heavyweight, middleweight, and lightweight) and provide buffer management and admission control algorithms for the three domains. We propose two improvements (flattening and dynamic-adjustments) on the schedules created for the heavyweight presentations. Results from a simulation environment are presented.  Entity 2: title fault tolerant design of multimedia servers authors steven berson , leana golubchik , richard r. muntz venue international conference on management of data year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Spatio-temporal databases deal with geometries changing over time. The goal of our work is to provide a DBMS data model and query language capable of handling such time-dependent geometries, including those changing continuously that describe moving objects. Two fundamental abstractions are moving point and moving region, describing objects for which only the time-dependent position, or position and extent, respectively, are of interest. We propose to present such time-dependent geometries as attribute data types with suitable operations, that is, to provide an abstract data type extension to a DBMS data model and query language. This paper presents a design of such a system of abstract data types. It turns out that besides the main types of interest, moving point   and moving region, a relatively large number of auxiliary data types are needed. For example, one needs a line type to represent the projection of a moving point into the plane, or a \u201cmoving real\u201d to represent the time-dependent distance of two points. It then becomes crucial to achieve (i) orthogonality in the design of the system, i.e., type constructors can be applied unifomly; (ii) genericity and consistency of operations, i.e., operations range over as many types as possible and behave consistently; and (iii) closure and consistency between structure and operations of nontemporal and related temporal types. Satisfying these goal leads to a simple and expressive system of abstract data types that may be integrated into a query language to yield a powerful language for   querying spatio-temporal data, including moving objects. The paper formally defines the types and operations, offers detailed insight into the considerations that went into the design, and exemplifies the use of the abstract data types using SQL. The paper offers a precise and conceptually clean foundation for implementing a spatio-temporal DBMS extension.  Entity 2: title analysis of locking behavior in three real database systems authors vigyan singhal , alan jay smith venue the vldb journal -- the international journal on very large data bases year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The Moving Picture Experts Group (MPEG) is developing a new standard called the \u201cMultimedia Content Description Interface,\u201d also known as MPEG-7. The goal of MPEG-7 is to enable fast and effective searching and filtering of multimedia content. The effort is being driven by requirements taken from a large number of applications related to multimedia databases, interactive media services (music, TV programs), video libraries, and so forth. MPEG-7 is achieving this goal by developing an XML-Schema based standard for describing features of multimedia content. In this tutorial, we study the emerging MPEG-7 standard and describe the new challenges for MPEG-7 multimedia databases.  Entity 2: title a user-centered interface for querying distributed multimedia databases authors isabel f. cruz , kimberly m. james venue international conference on management of data year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: A major challenge still facing the designers and implementors of database programming languages (DBPLs) is that of query optimisation. In the paper we first give the syntax of our archetypal DBPL and briefly discuss its semantics. We then define a small but powerful algebra of operators over the set data type, provide some key equivalences for expressions in these operators, and list transformation principles for optimising expressions.  Entity 2: title applications of java programming language to database management authors bradley f. burton , victor w. marek venue acm sigmod record year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The Knowledge and Data Base Systems (KDBS) Laboratory was established in 1992 at the National Technical University of Athens. It is recognised internationally, evidenced by its participation as a central node in the Esprit Network of Excellence IDOMENEUS. The Information and Data on Open MEdia for NEtworks of USers, project aims to coordinate and improve European efforts in the development of next-generation information environments which will be capable of maintaining and communicating a largely extended class of information in an open set of media.  Entity 2: title the database group at university of hagen authors gunter schlageter , thomas berkel , eberhard heuel , silke mittrach , andreas scherer , wolfgang wilkes venue acm sigmod record year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper, we present two algorithms for deriving optimal and near-optimal vertical class partitioning schemes. The cost-driven algorithm provides the optimal vertical class partitioning schemes by enumerating, exhaustively, all the schemes and calculating the number of disk accesses required to execute a given set of applications. For this, a cost model for executing a set of methods in an OODB system is developed. Since exhaustive enumeration is costly and only works for classes with a small number of instance variables, a hill-climbing heuristic algorithm (HCHA) is developed, which takes the solution provided by the affinity-based algorithm and improves it, thereby further reducing the total number of disk accesses incurred.  Entity 2: title index configuration in object-oriented databases authors elisa bertino venue the vldb journal -- the international journal on very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Most of new databases are no more built from scratch, but re-use existing data from several autonomous data stores. To facilitate application development, the data to be re-used should preferably be redefined as a virtual database, providing for the logical unification of the underlying data sets. This unification process is called database integration. This chapter provides a global picture of the issues raised and the approaches that have been proposed to tackle the problem.  Entity 2: title the mlpq/gis constraint database system authors peter revesz , rui chen , pradip kanjamala , yiming li , yuguo liu , yonghui wang venue international conference on management of data year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Bitmaps are popular indexes for data warehouse (DW) applications and most database management systems offer them today. This paper proposes query optimization strategies for selections using bitmaps. Both continuous and discrete selection criteria are considered. Query optimization strategies are categorized into static and dynamic. Static optimization strategies discussed are the optimal design of bitmaps, and algorithms based on tree and logical reduction. The dynamic optimization discussed is the approach of inclusion and exclusion for both bit-sliced indexes and encoded bitmap indexes.  Entity 2: title query optimization for xml authors jason mchugh , jennifer widom venue very large data bases year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This article focuses on object-oriented databases (ODBs), providing a method aimed at supporting the designer in the construction of correct ODB schemas. The first necessary condition for schema correctness is the absence of contradictions. A second cause of schema incorrectness is due to the presence of structurally recursive types that, when defined within certain hierarchical patterns, cause the nontermination of the inheritance process. In the article, after the formal definition of a correct schema, two graph-theoretic methods aimed at verifying ODB schema correctness are analyzed. Although the first method is intuitive but inefficient, the second allows schema correctness to be checked in polynomial time, in the size of the schema.  Entity 2: title accessing a relational database through an object-oriented database interface authors jack a. orenstein , d. n. kamber venue very large data bases year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this column, we review these three books: 1. The Problem With Software: Why Smart Engineers Write Bad Code, by Adam Barr. Answers to a question many of us have probably asked; and what we can do about it. Review by Shoshana Marcus. 2. Elements of Parallel Computing, by Eric Aubanel. A textbook on the underpinnings of this important and fascinating area of computer science. Review by Michele Amoretti. 3. Theorems of the 21st Century: Volume I, by Bogdan Grechuk. The background, context, and statement of numerous important (accessible) mathematical theorems from the past 20 years. Review by William Gasarch.  Entity 2: title book review column authors karl aberer venue acm sigmod record year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Multimedia data mining is the mining of high-level multimedia information and knowledge from large multimedia databases. A multimedia data mining system prototype, MultiMediaMiner, has been designed and developed. It includes the construction of a multimedia data cube which facilitates multiple dimensional analysis of multimedia data, primarily based on visual content, and the mining of multiple kinds of knowledge, including summarization, comparison, classification, association, and clustering.  Entity 2: title metadata for multimedia documents authors klemens b &#246; hm , thomas c. rakow venue acm sigmod record year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper describes the version and workspace features of Microsoft Repository, a layer that implements fine-grained objects and relationships on top of Microsoft SQL Server. It supports branching and merging of versions, delta storage, checkout-checkin, and single-version views for version-unaware applications.  Entity 2: title the microsoft repository authors philip a. bernstein , brian harry , paul sanders , david shutt , jason zander venue very large data bases year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O\u2019s state is loaded, similar state for other objects in O\u2019s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.  Entity 2: title an efficient method for checking object-oriented database schema correctness authors a. formica , h. d. groger , m. missikoff venue acm transactions on database systems ( tods ) year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Much of business XML data has accompanying XSD specifications. In many scenarios, \"shredding such XML data into a relational storage is a popular paradigm. Optimizing evaluation of XPath queries over such XML data requires paying careful attention to both the logical and physical designs of the relational database where XML data is shredded. None of the existing solutions has taken into account physical design of the generated relational database. In this paper, we study the interplay of logical and physical design and conclude that 1) solving them independently leads to suboptimal performance and 2) there is substantial overlap between logical and physical designs: some well-known logical design transformations generate the same mappings as physical design. Furthermore, existing search algorithms are inefficient to search the extremely large space of logical and physical design combinations. We propose a search algorithm that carefully avoids searching duplicated mappings and utilizes the workload information to further prune the search space. Experimental results confirm the effectiveness of our approach.  Entity 2: title storing and querying ordered xml using a relational database system authors igor tatarinov , stratis d. viglas , kevin beyer , jayavel shanmugasundaram , eugene shekita , chun zhang venue international conference on management of data year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.  Entity 2: title index nesting - an efficient approach to indexing in object-oriented databases authors beng chin ooi , jiawei han , hongjun lu , kian lee tan venue the vldb journal -- the international journal on very large data bases year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The main components of MIND are a global query processor, a global transaction manager, a schema integrator, interfaces to supported database systems and a user graphical interface.In MIND all local databases are encapsulated in a generic database object with a well defined single interface. This approach hides the differences between local databases from the rest of the system. The integration of export schemas is currently performed manually by using an object definition language (ODL) which is based on OMG's interface definition language. The DBA builds the integrated schema as a view over export schemas. the functionalities of ODL allow selection and restructuring of schema elements from existing local schemas.MIND global query optimizer aims at maximizing the parallel execution of the intersite joins of the global subqueries. Through MIND global transaction manager, the serializable execution of the global transactions are provided.  Entity 2: title languages for multi-database interoperability authors fr &#233; d &#233; ric gingras , laks v. s. lakshmanan , iyer n. subramanian , despina papoulis , nematollaah shiri venue international conference on management of data year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The integration of object-oriented programming concepts with databases is one of the most significant advances in the evolution of database systems. Many aspects of such a combination have been studied, but there are few models to provide security for this richly structured information. We develop an authorization model for object-oriented databases. This model consists of a set of policies, a structure for authorization rules, and algorithms to evaluate access requests against the authorization rules. User access policies are based on the concept of inherited authorization applied along the class structure hierarchy. We propose also a set of administrative policies that allow the control of user access and its decentralization. Finally, we study the effect of class structuring changes on authorization. >  Entity 2: title accessing a relational database through an object-oriented database interface authors jack a. orenstein , d. n. kamber venue very large data bases year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Views as a means to describe parts of a given data collection play an important role in many database applications. In dynamic environments where data is updated, not only information provided by v...  Entity 2: title querying xml views of relational data authors jayavel shanmugasundaram , jerry kiernan , eugene j. shekita , catalina fan , john funderburk venue very large data bases year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We present MOCHA, a new self-extensible database middleware system designed to interconnect distributed data sources. MOCHA is designed to scale to large environments and is based on the idea that some of the user-defined functionality in the system should be deployed by the middleware system itself. This is realized by shipping Java code implementing either advanced data types or tailored query operators to remote data sources and have it executed remotely. Optimized query plans push the evaluation of powerful data-reducing operators to the data source sites while executing data-inflating operators near the client's site. The Volume Reduction Factor is a new and more explicit metric introduced in this paper to select the best site to execute query operators and is shown to be more accurate than the standard selectivity factor alone. MOCHA has been implemented in Java and runs on top of Informix and Oracle. We present the architecture of MOCHA, the ideas behind it, and a performance study using scientific data and queries. The results of this study demonstrate that MOCHA provides a more flexible, scalable and efficient framework for distributed query processing compared to those in existing middleware solutions.  Entity 2: title mocha : a database middleware system featuring automatic deployment of application-specific functionality authors manuel rodr &#237; guez-mart &#237; nez , nick roussopoulos , john m. mcgann , stephen kelley , vadim katz , zhexuan song , joseph j &#225; j &#225; venue international conference on management of data year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper describes the methodology and implementation of a data management system for highly distributed systems, which was built to solve the scalability and reliability problems faced in a wide area postal logistics application developed at Siemens. The core of the approach is to borrow from Internet routing protocols, and their proven scalability and robustness, to build a network-embedded dynamic database index, and to augment schema definition to optimize the use of this index. The system was developed with an eye toward future applications in the area of sensor networks.  Entity 2: title database systems management and oracle8 authors c. gregory doherty venue international conference on management of data year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Lotus Notes is a commercial product that empowers individuals and organizations to collaborate and share information [1].Notes enables the easy development of applications such as messaging, document management, workflow, and asynchronous conferencing. Notes applications can be deployed globally, across independent organizations, among a heterogeneous network of loosely coupled computers that range in size from small notebooks to large multi-processor systems.The third major release of Lotus Notes occurred in May 1993.  Entity 2: title the naos system authors c. collet , t. coupaye venue international conference on management of data year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper describes the methodology and implementation of a data management system for highly distributed systems, which was built to solve the scalability and reliability problems faced in a wide area postal logistics application developed at Siemens. The core of the approach is to borrow from Internet routing protocols, and their proven scalability and robustness, to build a network-embedded dynamic database index, and to augment schema definition to optimize the use of this index. The system was developed with an eye toward future applications in the area of sensor networks.  Entity 2: title data management for earth system science authors james frew , jeff dozier venue acm sigmod record year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this article, we review pairwise spatial join algorithms and show how they can be combined for multiple inputs. In addition, we explore the application of synchronous traversal (ST), a methodology that processes synchronously all inputs without producing intermediate results. Then, we integrate the two approaches in an engine that includes ST and pairwise algorithms, using dynamic programming to determine the optimal execution plan. The results show that, in most cases, multiway spatial joins are best processed by combining ST with pairwise methods. Finally, we study the optimization of very large queries by employing randomized search algorithms.  Entity 2: title iterative spatial join authors edwin h. jacox , hanan samet venue acm transactions on database systems ( tods ) year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Semistructured data is not strictly typed like relational or object-oriented data and may be irregular or incomplete. It often arises in practice, e.g., when heterogeneous data sources are integrated or data is taken from the World Wide Web. Views over semistructured data can be used to filter the data and to restructure (or provide structure to) it. To achieve fast query response time, these views are often materialized. This paper proposes an incremental maintenance algorithm for materialized views over semistructured data. We use the graph-based data model OEM and the query language Lorel, developed at Stanford, as the framework for our work. our algorithm produces a set of queries that compute the updates to the view based upon an update of the source. We develop an analytic cost model and compare the cost of executing our incremental maintenance algorithm to that of recomputing the view. We show that for nearly all types of database updates, it is more efficient to apply our incremental maintenance algorithm to the view than to recompute the view from the database, even when there are thousands of updates.  Entity 2: title efficient maintenance of materialized mediated views authors james j. lu , guido moerkotte , joachim schue , v. s. subrahmanian venue international conference on management of data year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Due to the wide use of object-oriented technology in software development and the existence of many relational databases, reverse engineering of relational schemas to object-oriented schemas is gaining in interest. One of the major problems with existing approaches for this schema mapping is that they fail to take into consideration many modern relational database design alternatives (e.g., use of binary data to store multiple-valued attributes). This paper presents a schema mapping procedure that can be applied on existing relational databases without changing their schema. The procedure maps a relational schema that is at least in 2NF into an object-oriented schema by taking into consideration various types of relational database design optimizations.  Entity 2: title exact : an extensible approach to active object-oriented databases authors oscar d &#237; az , arturo jaime venue the vldb journal -- the international journal on very large data bases year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle \"noise\" (data points that are not part of the underlying pattern) effectively.  Entity 2: title wavecluster : a multi-resolution clustering approach for very large spatial databases authors gholamhosein sheikholeslami , surojit chatterjee , aidong zhang venue very large data bases year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Data Grids are being built across the world as the next generation data handling systems to manage peta-bytes of inter organizational data and storage space. A data grid (datagrid) is a logical name space consisting of storage resources and digital entities that is created by the cooperation of autonomous organizations and its users based on the coordination of local and global policies. Data Grid Management Systems (DGMSs) provide services for the confluence of organizations and management of inter-organizational data and resources in the datagrid.The objective of the tutorial is to provide an introduction to the opportunities and challenges of this emerging technology.  Entity 2: title on supporting containment queries in relational database management systems authors chun zhang , jeffrey naughton , david dewitt , qiong luo , guy lohman venue international conference on management of data year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Due to the wide use of object-oriented technology in software development and the existence of many relational databases, reverse engineering of relational schemas to object-oriented schemas is gaining in interest. One of the major problems with existing approaches for this schema mapping is that they fail to take into consideration many modern relational database design alternatives (e.g., use of binary data to store multiple-valued attributes). This paper presents a schema mapping procedure that can be applied on existing relational databases without changing their schema. The procedure maps a relational schema that is at least in 2NF into an object-oriented schema by taking into consideration various types of relational database design optimizations.  Entity 2: title integrating a structured-text retrieval system with an object-oriented database system authors tak w. yan , jurgen annevelink venue very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this column, we review these three books.  Entity 2: title book review column authors karl aberer venue acm sigmod record year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Similarity search in large time series databases has attracted much research interest recently. It is a difficult problem because of the typically high dimensionality of the data. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and the Discrete Wavelet Transform (DWT). In this article, we introduce a new dimensionality reduction technique, which we call Adaptive Piecewise Constant Approximation (APCA). While previous techniques (e.g., SVD, DFT and DWT) choose a common representation for all the items in the database that minimizes the global reconstruction error, APCA approximates each time series by a set of constant value segments of varying lengths such that their individual reconstruction errors are minimal. We show how APCA can be indexed using a multidimensional index structure. We propose two distance measures in the indexed space that exploit the high fidelity of APCA for fast searching: a lower bounding Euclidean distance approximation, and a non-lower-bounding, but very tight, Euclidean distance approximation, and show how they can support fast exact searching and even faster approximate searching on the same index structure. We theoretically and empirically compare APCA to all the other techniques and demonstrate its superiority.  Entity 2: title a foundation for multi-dimensional databases authors marc gyssens , laks v. s. lakshmanan venue very large data bases year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Our initial studies of Broadcast Disks focused on the performance of the mechanism when the data being broadcast did not change. In this paper, we extend those results to incorporate the impact of updates. We first propose several alternative models for updates and examine the fundamental tradeoff that arises between the currency of data and performance. We then propose and analyze mechanisms for implementing these various models. The performance results show that, even in a model where updates must be transmitted immediately, the performance of the Broadcast Disks technique can be made quite mbust through the use of simple techniques for propagating and prefetching data items.  Entity 2: title exploiting versions for handling updates in broadcast disks authors evaggelia pitoura , panos k. chrysanthis venue very large data bases year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper provides an overview of the current database research activities within the Intelligent Information Systems Group in the Department of Computer Science and Engineering at Arizona State University. The focus of our research is on the integration of data and knowledge management issues, with specific emphasis on multimedia systems, object-oriented databases, active databases, deductive databases, and heterogeneous, distributed database environments.  Entity 2: title database research at columbia university authors shih-fu chang , luis gravano , gail e. kaiser , kenneth a. ross , salvatore j. stolfo venue acm sigmod record year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Here we survey the compactness and geometric stability conjectures formulated by the participants at the 2018 IAS Emerging Topics Workshop on Scalar Curvature and Convergence. We have tried to survey all the progress towards these conjectures as well as related examples, although it is impossible to cover everything. We focus primarily on sequences of compact Riemannian manifolds with nonnegative scalar curvature and their limit spaces.  Entity 2: title an introduction to spatial database systems authors ralf hartmut g &#252; ting venue the vldb journal -- the international journal on very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study  Entity 2: title constructing the next 100 database management systems : like the handyman or like the engineer ? authors andreas geppert , klaus r. dittrich venue acm sigmod record year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Several alternatives to manage large XML document collections exist, ranging from file systems over relational or other database systems to specifically tailored XML base management systems. In this paper we give a tour of Natix, a database management system designed from scratch for storing and processing XML data. Contrary to the common belief that management of XML data is just another application for traditional databases like relational systems, we illustrate how almost every component in a database system is affected in terms of adequacy and performance.  Entity 2: title anatomy of a real e-commerce system authors anant jhingran venue international conference on management of data year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We consider the problem of substring searching in large databases. Typical applications of this problem are genetic data, web data, and event sequences. Since the size of such databases grows exponentially, it becomes impractical to use inmemory algorithms for these problems. In this paper, we propose to map the substrings of the data into an integer space with the help of wavelet coefficients. Later, we index these coefficients using MBRs (Minimum Bounding Rectangles). We define a distance function which is a lower bound to the actual edit distance between strings. We experiment with both nearest neighbor queries and range queries. The results show that our technique prunes significant amount of the database (typically 50-95%), thus reducing both the disk I/O cost and the CPU cost significantly .  Entity 2: title cure : an efficient clustering algorithm for large databases authors sudipto guha , rajeev rastogi , kyuseok shim venue international conference on management of data year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: After revealing the strong performance shortcomings of the state-of-the-art algorithm for k-nearest neighbor search [Korn et al. 1996], we present a novel multi-step algorithm which is guaranteed to produce the minimum number of candidates. Experimental evaluations demonstrate the significant performance gain over the previous solution, and we observed average improvement factors of up to 120 for the number of candidates and up to 48 for the total runtime.  Entity 2: title fast nearest neighbor search in medical image databases authors flip korn , nikolaos sidiropoulos , christos faloutsos , eliot siegel , zenon protopapas venue very large data bases year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: One way to overcome those problems is to move towards less restricted scenarios. In this context we present a large-scale real-world dataset designed to evaluate learning techniques for human action recognition beyond hand-crafted datasets. To this end we put the process of collecting data on its feet again and start with the annotation of a test set of 250 cooking videos. The training data is then gathered by searching for the respective annotated classes within the subtitles of freely available videos. The uniqueness of the dataset is attributed to the fact that the whole process of collecting the data and training does not involve any human intervention. To address the problem of semantic inconsistencies that arise with this kind of training data, we further propose a semantical hierarchical structure for the mined classes.  Entity 2: title extracting schema from semistructured data authors svetlozar nestorov , serge abiteboul , rajeev motwani venue international conference on management of data year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Queries navigate semistructured data via path expressions, and can be accelerated using an index. Our solution encodes paths as strings, and inserts those strings into a special index that is highly optimized for long and complex keys. We describe the Index Fabric, an indexing structure that provides the efficiency and flexibility we need. We discuss how \"raw paths\" are used to optimize ad hoc queries over semistructured data, and how \"refined paths\" optimize specific access paths. Although we can use knowledge about the queries and structure of the data to create refined paths, no such knowledge is needed for raw paths.  Entity 2: title capturing and querying multiple aspects of semistructured data authors curtis e. dyreson , michael h. b &#246; hlen , christian s. jensen venue very large data bases year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: One of the important problems in data mining is discovering association rules from databases of transactions where each transaction consists of a set of items. The most time consuming operation in this discovery process is the computation of the frequency of the occurrences of interesting subset of items (called candidates) in the database of transactions. To prune the exponentially large space of candidates, most existing algorithms, consider only those candidates that have a user defined minimum support. Even with the pruning, the task of finding all association rules requires a lot of computation power and time. Parallel computers offer a potential solution to the computation requirement of this task, provided efficient and scalable parallel algorithms can be designed. In this paper, we present two new parallel algorithms for mining association rules.  Entity 2: title association rules over interval data authors r. j. miller , y. yang venue international conference on management of data year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We present techniques for computing small space representations of massive data streams. These are inspired by traditional wavelet-based approximations that consist of specific linear projections of the underlying data. We present general \u201csketch\u201d based methods for capturing various linear projections of the data and use them to provide pointwise and rangesum estimation of data streams. These methods use small amounts of space and per-item time while streaming through the data, and provide accurate representation as our experiments with real data streams show.  Entity 2: title approximating multi-dimensional aggregate range queries over real attributes authors dimitrios gunopulos , george kollios , vassilis j. tsotras , carlotta domeniconi venue international conference on management of data year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In the last few years, many active database models have been proposed. Some of them have been implemented as research prototypes. The use and study of these prototypes shows that it is difficult to get a clear idea of the proposed approaches and to compare them. More generally there are some unquestionable difficulties in understanding, reasoning about and teaching behavior of active database systems. We think there is a need for formal descriptions of the semantics of such systems in order to describe and to understand them with less ambiguities, to compare them and to come up with some progress in defining standard concepts and functionalities for active databases.  Entity 2: title the prototype of the dare system authors tiziana catarci , giuseppe santucci venue international conference on management of data year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Internet, Web and distributed computing infrastructures continue to gain in popularity as a means of communication for organizations, groups and individuals alike. In such an environment, characterized by large distributed, autonomous, diverse, and dynamic information sources, access to relevant and accurate information is becoming increasingly complex. This complexity is exacerbated by the evolving system, semantic and structural heterogeneity of these potentially global, cross-disciplinary, multicultural and rich-media technologies. Clearly, solutions to these challenges require addressing directly a variety of interoperability issues.  Entity 2: title semantic interoperability in information services : experiencing with coopware authors avigdor gal venue acm sigmod record year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper, we propose the novel idea of hierarchical subspace sampling in order to create a reduced representation of the data. The method is naturally able to estimate the local implicit dimensionalities of each point very effectively, and thereby create a variable dimensionality reduced representation of the data. Such a technique has the advantage that it is very adaptive about adjusting its representation depending upon the behavior of the immediate locality of a data point. An interesting property of the subspace sampling technique is that unlike all other data reduction techniques, the overall efficiency of compression improves with increasing database size. This is a highly desirable property for any data reduction system since the problem itself is motivated by the large size of data sets. Because of its sampling approach, the procedure is extremely fast and scales linearly both with data set size and dimensionality. Furthermore, the subspace sampling technique is able to reveal important local subspace characteristics of high dimensional data which can be harnessed for effective solutions to problems such as selectivity estimation and approximate nearest neighbor search.  Entity 2: title multi-dimensional substring selectivity estimation authors h. v. jagadish , olga kapitskaia , raymond t. ng , divesh srivastava venue very large data bases year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: As teachers, if we believe content knowledge matters the most for successful instruction, we may not understand that getting to know students and discovering their strengths as learners are equally important. Teachers are instructional islands with a lot of content to share but perhaps unconnected to the learners that make up the classroom. If as teachers, we describe students by saying, \u201cshe is a math wizard,\u201d \u201cshe is a science ace,\u201d or \u201che is a sponge for historical facts,\u201d we can communicate a lot about students with minimal language. These metaphors help us make comparisons that evoke multiple layers of meaning, and yet thinking metaphorically is also an aspect of everyday life. Cognitive scientists Lakoff and Johnson (2008) argued that our conceptualizations of the world around us are metaphorical and provided examples of metaphors such as \u201ctime is money\u201d and suggested that the way we construe argument is conceived in metaphors of war when we \u201cattack a position,\u201d for example, to support a philosophical claim. From an educational philosophy perspective, Greene contended learning is a landscape (1973) and teachers are philosophers working to help learners resist the forces that limit and oppress them (1988) to attain freedom to think for themselves. These theorists recognized the epistemological power of metaphor and challenged us to see its educational potential. Comparisons through metaphoric thinking afford different perspectives and open imaginative possibilities, challenging us to see familiar relationships in new ways. Metaphors can push us to think about teacher education differently as well and move beyond familiar views of clinical experiences, teacher interns, teacher preparation programs, and who we are as educators to see these concepts more complexly. The pedagogical innovation study and six empirical studies in this issue evoke metaphors of exploration, dialogue, windows, building, partnering, and innovating to inform and complicate our understanding teacher education.  Entity 2: title editorial authors richard snodgrass venue acm transactions on database systems ( tods ) year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The rapid emergence of XML as a standard for data exchange over the Web has led to considerable interest in the problem of securing XML documents. In this context, query evaluation engines need to ensure that user queries only use and return XML data the user is allowed to access. These added access control checks can considerably increase query evaluation time. In this paper, we consider the problem of optimizing the secure evaluation of XML twig queries.    We focus on the simple, but useful, multi-level access control model, where a security level can be either specified at an XML element, or inherited from its parent. For this model, secure query evaluation is possible by rewriting the query to use a recursive function that computes an element's security level. Based on security information in the DTD, we devise efficient algorithms that optimally determine when the recursive check can be eliminated, and when it can be simplified to just a local check on the element's attributes, without violating the access control policy. Finally, we experimentally evaluate the performance benefits of our techniques using a variety of XML data and queries.  Entity 2: title evaluating top-k selection queries authors surajit chaudhuri , luis gravano venue very large data bases year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Abstract.This paper contains an overview of the technology used in the query processing and optimization component of Oracle Rdb, a relational database management system originally developed by Digital Equipment Corporation and now under development by Oracle Corporation. Oracle Rdb is a production system that supports the most demanding database applications, runs on multiple platforms and in a variety of environments.  Entity 2: title including group-by in query optimization authors surajit chaudhuri , kyuseok shim venue very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The tutorial surveys state-of-the-art methods for storing and retrieving multimedia data from large databases. Records (= documents) may consist of formatted fields, text, images, voice, animation etc. .4 sample query that we would like to support is \u2018in a collection of 2-d color images, find images that are similar to a sunset photograph\u2019. Indexing for images and other media is a new, active area of research; the tutorial will present recent approaches and prototype systems, for 2-d and 3-d medical image databases, 2-d color image databases, and l-d time series databases.  Entity 2: title standard for multimedia databases authors john r. smith venue international conference on management of data year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: XML is an emerging standard for data representation and exchange on the World-Wide Web. Due to the nature of information on the Web and the inherent flexibility of XML, we expect that much of the data encoded in XML will be semistructured: the data may be irregular or incomplete, and its structure may change rapidly or unpredictably. This paper describes the query processor of Lore, a DBMS for XML-based data supporting an expressive query language. We focus primarily on Lore's cost-based query optimizer. While all of the usual problems associated with cost-based query optimization apply to XML-based query languages, a number of additional problems arise, such as new kinds of indexing, more complicated notions of database statistics, and vastly different query execution strategies for different databases. We define appropriate logical and physical query plans, database statistics, and a cost model, and we describe plan enumeration including heuristics for reducing the large search space. Our optimizer is fully implemented in Lore and preliminary performance results are reported. This is a short version of the paper Query Optimization for Semistructured Data which is available at: http://www-db.stanford.edu/~mchughj/publications/qo.ps  Entity 2: title rainbow : multi-xquery optimization using materialized xml views authors xin zhang , katica dimitrova , ling wang , maged el sayed , brian murphy , bradford pielech , mukesh mulchandani , luping ding , elke a. rundensteiner venue international conference on management of data year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In the bottom-up evaluation of logic programs and recursively defined views on databases, all generated facts are usually assumed to be stored until the end of the evaluation. Discarding facts during the evaluation, however, can considerably improve the efficiency of the evaluation: the space needed to evaluate the program, the I/O costs, the costs of maintaining and accessing indices, and the cost of eliminating duplicates may all be reduced. Given an evaluation method that is sound, complete, and does not repeat derivation steps, we consider how facts can be discarded during the evaluation without compromising these properties. We show that every such space optimization method has certain components, the first to ensure soundness and completeness, the second to avoid redundancy (i.e., repetition of derivations), and the third to reduce \u201cfact lifetimes\u201d (i.e., the time period for which each fact must be retained during evaluation). We present new techniques based on providing bounds on the number of derivations and uses of facts, and using monotonicity constraints for each of the first two components, and provide novel synchronization techniques for the third component of a space optimization method.  Entity 2: title xsb as a deductive database authors konstantinos sagonas , terrance swift , david s. warren venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Infomaster is an information integration system that provides integrated access to multiple distributed heterogeneous information sources on the Internet, thus giving the illusion of a centralized, homogeneous information system. We say that Infomaster creates a virtual data warehouse. The core of Infomaster is a facilitator that dynamically determines an efficient way to answer the user's query using as few sources as necessary and harmonizes the heterogeneities among these sources. Infomaster handles both structural and content translation to resolve differences between multiple data sources and the multiple applications for the collected data. Infomaster connects to a variety of databases using wrappers, such as for Z39.50, SQL databases through ODBC, EDI transactions, and other World Wide Web (WWW) sources. There are several WWW user interfaces to Infomaster, including forms based and textual. Infomaster also includes a programmatic interface and it can download results in structured form onto a client computer. Infomaster has been in production use for integrating rental housing advertisements from several newspapers (since fall 1995), and for meeting room scheduling (since winter 1996). Infomaster is also being used to integrate heterogeneous electronic product catalogs.  Entity 2: title spotfire : an information exploration environment authors christopher ahlberg venue acm sigmod record year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The increasing usage of location-aware devices, such as GPS and RFID, has made moving object management an important task. Especially, being demanded in real-world applications, continuous query processing on moving objects has attracted significant research efforts. However, little attention has been given to the design of concurrent continuous query processing for multi-user environments. In this paper, we propose a concurrency control protocol to efficiently process continuous queries over moving objects on a B-tree-based framework. The proposed protocol integrates link-based and lock-coupling strategies, and is proven to assure serializable isolation, data consistency, and deadlock-free for continuous query processing. Concurrent operations including continuous query, object movement, and query movement are protected under this protocol. Experimental results on benchmark data sets demonstrated the scalability and efficiency of the proposed concurrent framework.  Entity 2: title transactional information systems : theory , algorithms , and the practice of concurrency control and recovery authors marc h. scholl venue acm sigmod record year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.  Entity 2: title lambda-db : an odmg-based object-oriented dbms authors leonidas fegaras , chandrasekhar srinivasan , arvind rajendran , david maier venue international conference on management of data year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: From the Publisher:  The focus of Data Management for Mobile Computing is on the impact of mobile computing on data management beyond the networking level. The purpose is to provide a thorough and cohesive overview of recent advances in wireless and mobile data management. The book is written with a critical attitude. This volume probes the new issues introduced by wireless and mobile access to data and what are both their conceptual and practical consequences. Data Management for Mobile Computing provides a single source for researchers and practitioners who want to keep current on the latest innovations in the field. It can also serve as a textbook for an advanced course on mobile computing or as a companion text for a variety of courses including courses on distributed systems, database management, transaction management, operating or file systems, information retrieval or dissemination, and web computing.  Entity 2: title digital library services in mobile computing authors bharat bhargava , melliyal annamalai , evaggelia pitoura venue acm sigmod record year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Although many of the problems that must be solved by an object-oriented database system are similar to problems solved by relational systems, there are also significant problems that are unique. In particular, an object query can include a path expression to traverse a number of related collections. The order of collection traversals given by the path expression may not be the most efficient to process the query. This generates a critical problem for object query optimizer to select an algorithm to process the query based on direct navigation or various combinations of joins. This paper studies the different algorithms to process path expressions with predicates, including depth first navigation, forward and reverse joins. Using a cost model, it then compares their performances in different cases, according to memory size, selectivity of predicates, fan out between collections, etc.. It also presents a heuristic-based algorithm to find profitable n-ary operators for traversing collections, thus reducing the search space of query plans to process a query with a qualified path expression. An implementation based on the O2 system demonstrates the validity of the results.  Entity 2: title converting relational to object-oriented databases authors joseph fong venue acm sigmod record year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Incremental refresh of a materialized join view is often less expensive than a full, non-incremental refresh. However, it is still a potentially costly atomic operation. This paper presents an algorithm that performs incremental view maintenance as a series of small, asynchronous steps. The size of each step can be controlled to limit contention between the refresh process and concurrent operations that access the materialized view or the underlying relations. The algorithm supports point-in-time refresh, which allows a materialized view to be refreshed to any time between the last refresh and the present.  Entity 2: title incremental maintenance of views with duplicates authors timothy griffin , leonid libkin venue international conference on management of data year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Association rule mining recently attracted strong attention. Usually, the classification hierarchy over the data items is available. Users are interested in generalized association rules that span different levels of the hierarchy, since sometimes more interesting rules can be derived by taking the hierarchy into account.  Entity 2: title algorithms for mining association rules for binary segmentations of huge categorical databases authors yasuhiko morimoto , takeshi fukuda , hirofumi matsuzawa , takeshi tokuyama , kunikazu yoda venue very large data bases year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In the database framework of Kanellakis et al. [1990] it was argued that constraint query languages should take constraint databases as input and give other constraint databases that use the same type of atomic constraints as output. This closed-form requirement has been difficult to realize in constraint query languages that contain the negation symbol. This paper describes a general approach to restricting constraint query languages with negation to safe subsets that contain only programs that are evaluable in closed-form on any valid constraint database input.  Entity 2: title introduction to constraint databases authors bart kuijpers venue acm sigmod record year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Business-oriented workflows have been studied since the 70's under various names (office automation, workflow management, business process management) and by different communities, including the database community. Much basic and applied research has been conducted over the years, e.g. theoretical studies of workflow languages and models (based on Petri-nets or process calculi), their properties, transactional behavior, etc.  Entity 2: title guest editorial authors matthias jarke venue the vldb journal -- the international journal on very large data bases year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Formulating queries on networked information systems is laden with problems: data diversity, data complexity, network growth, varied user base, and slow network access. This paper proposes a new approach to a network query user interface which consists of two phases: query preview and query refinement. This new approach is based on dynamic queries and tight coupling, guiding users to rapidly and dynamically eliminate undesired items, reduce the data volume to a manageable size, and refine queries locally before submission over a network. A two-phase dynamic query system for NASA's Earth Observing Systems--Data Information Systems (EOSDIS) is presented. The prototype was well received by the team of scientists who evaluated the interface.  Entity 2: title www-udk : a web-based environmental meta-information system authors ralf kramer , ralf nicholai , arne koschel , claudia rolker , peter lockemann , andree keitel , rudolf legat , konrad tirm venue acm sigmod record year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We propose to extend database systems by a Skyline operation. This operation filters out a set of interesting points from a potentially large set of data points. A point is interesting if it is not dominated by any other point. For example, a hotel might be interesting for somebody traveling to Nassau if no other hotel is both cheaper and closer to the beach. We show how SSL can be extended to pose Skyline queries, present and evaluate alternative algorithms to implement the Skyline operation, and show how this operation can be combined with other database operations, e.g., join.  Entity 2: title cache conscious algorithms for relational query processing authors ambuj shatdal , chander kant , jeffrey f. naughton venue very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Traditional query processors generate full, accurate query results, either in batch or in pipelined fashion. We argue that this strict model is too rigid for exploratory queries over diverse and distributed data sources, such as sources on the Internet. Instead, we propose a looser model of querying in which a user submits a broad initial query outline, and the system continually generates partial result tuples that may contain values for only some of the output fields. The user can watch these partial results accumulate at the user interface, and accordingly refine the query by specifying their interest in different kinds of partial results.After describing our querying model and user interface, we present a query processing architecture for this model which is implemented in the Telegraph dataflow system. Our architecture is designed to generate partial results quickly, and to adapt query execution to changing user interests. The crux of this architecture is a dataflow operator that supports two kinds of reorderings: reordering of intermediate tuples within a dataflow, and reordering of query plan operators through which tuples flow. We study reordering policies that optimize for the quality of partial results delivered over time, and experimentally demonstrate the benefits of our architecture in this context.  Entity 2: title online query processing : a tutorial authors peter j. haas , joseph m. hellerstein venue international conference on management of data year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Relational databases provide the ability to store user-defined functions and predicates which can be invoked in SQL queries. When evaluation of a user-defined predicate is relatively expensive, the traditional method of evaluating predicates as early as possible is no longer a sound heuristic. There are two previous approaches for optimizing such queries. However, neither is able to guarantee the optimal plan over the desired execution space. We present efficient techniques that are able to guarantee the choice of an optimal plan over the desired execution space. The optimization algorithm with complete rank-ordering improves upon the naive optimization algorithm by exploiting the nature of the cost formulas for join methods and is polynomial in the number of  user-defined predicates (for a given number of relations.) We also  propose pruning rules that significantly reduce the cost of searching the execution space for both the naive algorithm as well as for the optimization algorithm with complete rank-ordering, without compromising optimality. We also propose a conservative local heuristic that is simpler and has low optimization overhead. Although it is not always guaranteed to find the optimal plans, it produces close to optimal plans in most cases. We discuss how, depending on application requirements, to determine the algorithm of choice. It should be emphasized that our optimization algorithms handle user-defined selections as well as user-defined join predicates uniformly. We present complexity analysis  and experimental comparison of the algorithms.  Entity 2: title minimization of tree pattern queries authors sihem amer-yahia , sungran cho , laks v. s. lakshmanan , divesh srivastava venue international conference on management of data year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Although many of the problems that must be solved by an object-oriented database system are similar to problems solved by relational systems, there are also significant problems that are unique. In particular, an object query can include a path expression to traverse a number of related collections. The order of collection traversals given by the path expression may not be the most efficient to process the query. This generates a critical problem for object query optimizer to select an algorithm to process the query based on direct navigation or various combinations of joins. This paper studies the different algorithms to process path expressions with predicates, including depth first navigation, forward and reverse joins. Using a cost model, it then compares their performances in different cases, according to memory size, selectivity of predicates, fan out between collections, etc.. It also presents a heuristic-based algorithm to find profitable n-ary operators for traversing collections, thus reducing the search space of query plans to process a query with a qualified path expression. An implementation based on the O2 system demonstrates the validity of the results.  Entity 2: title static detection of security flaws in object-oriented databases authors keishi tajima venue international conference on management of data year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Managing the transactions in real time distributed computing system is not easy, as it has heterogeneously networked computers to solve a single problem. If a transaction runs across some different sites, it may commit at some sites and may failure at another site, leading to an inconsistent transaction. The complexity is increase in real time applications by placing deadlines on the response time of the database system and transactions processing. Such a system needs to process transactions before these deadlines expired. A series of simulation study have been performed to analyze the performance under different transaction management under conditions such as different workloads, distribution methods, execution mode-distribution and parallel etc. The scheduling of data accesses are done in order to meet their deadlines and to minimize the number of transactions that missed deadlines. A new concept is introduced to manage the transactions in database size for originating site and remote site rather than database size computing parameters. With this approach, the system gives a significant improvement in performance.  Entity 2: title secure buffering in firm real-time database systems authors binto george , jayant r. haritsa venue the vldb journal -- the international journal on very large data bases year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Clustering of large data bases is an important research area with a large variety of applications in the data base context. Missing in most of the research efforts are means for guiding the clustering process and understanding the results, which is especially important for high dimensional data. Visualization technology may help to solve this problem since it provides effective support of different clustering paradigms and allows a visual inspection of the results. The HD-Eye (high-dim. eye) system shows that a tight integration of advanced clustering algorithms and state-of-the-art visualization techniques is powerful for a better understanding and effective guidance of the clustering process, and therefore can help to significantly improve the clustering results.  Entity 2: title multi-dimensional clustering : a new data layout scheme in db2 authors sriram padmanabhan , bishwaranjan bhattacharjee , tim malkemus , leslie cranston , matthew huras venue international conference on management of data year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The contribution of this project is threefold: (1) efficient generation for large itemsets by hash method (2) effective reduction on itemsets scan required by the division approach and (3) the option of reducing the number of database scans required Our proposed hash and division-based techniques, HD algorithm, is very efficient for the generation of candidate large itemsets where the number of candidate large itemsets generated by HD is, smaller than that by many methods such as the Apriori algorithm, DHP algorithm and DIC algorithm According to our simulation results, the proposed approach is more efficient than any existing algorithms.  Entity 2: title mining fuzzy association rules in databases authors chan man kuok , ada fu , man hon wong venue acm sigmod record year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Over the last decades, improvements in CPU speed have outpaced improvements in main memory and disk access rates by orders of magnitude, enabling the use of data compression techniques to improve the performance of database systems. Previous work describes the benefits of compression for numerical attributes, where data is stored in compressed format on disk. Despite the abundance of string-valued attributes in relational schemas there is little work on compression for string attributes in a database context. Moreover, none of the previous work suitably addresses the role of the query optimizer: During query execution, data is either eagerly decompressed when it is read into main memory, or data lazily stays compressed in main memory and is decompressed on demand only  Entity 2: title rate-based query optimization for streaming information sources authors stratis d. viglas , jeffrey f. naughton venue international conference on management of data year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.  Entity 2: title cost-based selection of path expression processing algorithms in object-oriented databases authors georges gardarin , jean-robert gruser , zhao-hui tang venue very large data bases year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We have implemented a compressor (XMilI) and decompressor (XDemill) for XML data, to be used in data exchange and archiving, which can be downloaded from http://www.research.att.com/sw/tools/xmill. XMill compresses about twice as good as gzip, at about the same speed. It does not need a DTD in order to compress, and preserves the input XML file faithfully, including element order, attributes order, PI 's, comments, the DTD, etc. A novelty in XMill is that it allows users to combine existing compressors in order to compress heterogeneous XML data: by default it uses zlib , a library function implementing gz ip ' s functionality, and includes some standard compression techniques for simple data types.  Entity 2: title xpress : a queriable compression for xml data authors jun-ki min , myung-jae park , chin-wan chung venue international conference on management of data year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The UniSQL/X unified relational and object--cmented database system is designed to support application development in either a convenuonal host programming Ianguagc (such as C), or an Object-aiented programming language (such as C++ or Smalltalk). In particular, C++ programmers can take advantage of all the capabilities of UniSQL/X in C++ programming style by using the UniSQL/X C++ Interfi~ce. C programmers can access the UniSQL/X database by using the Embedded SQL/X (objectaiented SQL) Preprocessor and/or the UniSQL/X AH (call levcl interface).  Entity 2: title olap , relational , and multidimensional database systems authors george colliat venue acm sigmod record year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This dynamic, data-centered approach opens up opportunities for personalizations: each user can be mapped to an individual hypertextual view of the Web site (called site view), and business rules may be used to change site views, both statically and dynamically. We argue that personalization of Web access (also called oneto-one Web delivery) is naturally supported by the proposed data-driven approach, and is We acknowledge the support of ESPRIT Project 28771 W3I3, MURST Project Interdata, CNR-CESTIA, and the HP Internet Philanthropic Initiative.  Entity 2: title design principles for data-intensive web sites authors stefano ceri , piero fraternali , stefano paraboschi venue acm sigmod record year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Publisher Summary eXtensible Markup Language (XML) is becoming the predominant data exchange format in a variety of application domains (supply-chain, scientific data processing, telecommunication infrastructure, etc.). Not only is an increasing amount of XML data now being processed, but XML is also increasingly being used in business-critical applications. Efficient and reliable storage is an important requirement for these applications. By relying on relational engines for this purpose, XML developers can benefit from a complete set of data management services (including concurrency control, crash recovery, and scalability) and from the highly optimized relational query processors. Strategies that automate the process of generating XML to relational mappings have been proposed in the literature. Due to the flexibility of the XML infrastructure, different XML applications exhibit widely different characteristics (for example, permissive vs. strict schemas, different access patterns).  Entity 2: title efficiently publishing relational data as xml documents authors jayavel shanmugasundaram , eugene j. shekita , rimon barr , michael j. carey , bruce g. lindsay , hamid pirahesh , berthold reinwald venue very large data bases year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Current State of Health Promotion The science and art of health promotion has made very impressive progress in the past two decades. It has evolved from :in innovative idea that made conceptual sense, but had no scientific backing to a maturing field supported by over ],000 empirical studies which demonstrate the positive health and financial impact of programs, and practiced by virtually all major employers in the US. Despite this progress, health promotion is not a part of mainstream medicine. Only a small fraction, probably less than 1%, of the $1.149 trillion spent annually on medical care is spent on health promotion. Despite the progress we h~.ve made on developing the science of health promotion, it is not recognized as a mature science by any respected scientific group. Repeated analyses conclude that roughly half of all prematu:ve deaths in the United States are from lifestyle related causes. Indeed, conservative estimates are that tobacco kills 450,000; obesity kills 300,000; and alcohol kills 100,000.  Entity 2: title editorial authors peter apers , stefano ceri , richard snodgrass venue the vldb journal -- the international journal on very large data bases year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight \"quality\" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.  Entity 2: title influential papers authors ken ross venue acm sigmod record year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Columbia University has a number of projects that touch on database systems issues. In this report, we describe the Columbia Fast Query Project (Section 2), the JAM project (Section 3), the CARDGIS project (Section 4), the Columbia Internet Information Searching Project (Section 5), the Columbia Content-Based Visual Query project (Section 6), and projects associated with Columbia\u2019s Programming Systems Laboratory (Section 7).  Entity 2: title the database research group at eth zurich authors moira c. norrie , stephen m. blott , hans-j &#246; rg schek , gerhard weikum venue acm sigmod record year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Conduct of scientific and engineering research is becoming critically dependent on effective management of scientific and engineering data and technical information. The rapid advances in scientific instrumentation, computer and communication technologies enable the scientists to collect, generate, process, and share unprecedented volumes of data. For example, the Earth Observing System Data and Information System (EOSDIS) has the task to manage the data from NASA\u2019s Earth science research satellites and field measurement programs, and other data essential for the interpretation of these measurements in support of global change research. Apart from being able to handle a stream of 1 terabyte of data daily by the year 2000, EOSDIS will also need to provide transparent access to heterogeneous data held in the archives of several US government agencies, organizations and countries. A single graphical user interface employing the Global Change Master Directory needs to help users locate data sets of interest among massive and diverse data sets, or find the appropriate data analysis tools, regardless of their location. Another major international effort in the area of human genome research faces some similar, as well as unique issues due to the complexity of the genome data, special querying requirements and much more heterogeneous collections of data.  Entity 2: title component-based e-commerce : assessment of current practices and future directions authors martin bichler , arie segev , j. leon zhao venue acm sigmod record year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: As teachers, if we believe content knowledge matters the most for successful instruction, we may not understand that getting to know students and discovering their strengths as learners are equally important. Teachers are instructional islands with a lot of content to share but perhaps unconnected to the learners that make up the classroom. If as teachers, we describe students by saying, \u201cshe is a math wizard,\u201d \u201cshe is a science ace,\u201d or \u201che is a sponge for historical facts,\u201d we can communicate a lot about students with minimal language. These metaphors help us make comparisons that evoke multiple layers of meaning, and yet thinking metaphorically is also an aspect of everyday life. Cognitive scientists Lakoff and Johnson (2008) argued that our conceptualizations of the world around us are metaphorical and provided examples of metaphors such as \u201ctime is money\u201d and suggested that the way we construe argument is conceived in metaphors of war when we \u201cattack a position,\u201d for example, to support a philosophical claim. From an educational philosophy perspective, Greene contended learning is a landscape (1973) and teachers are philosophers working to help learners resist the forces that limit and oppress them (1988) to attain freedom to think for themselves. These theorists recognized the epistemological power of metaphor and challenged us to see its educational potential. Comparisons through metaphoric thinking afford different perspectives and open imaginative possibilities, challenging us to see familiar relationships in new ways. Metaphors can push us to think about teacher education differently as well and move beyond familiar views of clinical experiences, teacher interns, teacher preparation programs, and who we are as educators to see these concepts more complexly.  Entity 2: title editorial authors richard snodgrass venue acm transactions on database systems ( tods ) year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The XQuery formalization is an ongoing effort of the W3C XML Query working group to define a precise formal semantics for XQuery. This paper briefly introduces the current state of the formalization and discusses some of the more demanding remaining challenges in formally describing an expressive query language for XML.  Entity 2: title medical information systems : characterization and challenges authors jorge c. g. ramirez , lon a. smith , lynn l. peterson venue acm sigmod record year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Browsing ANd Keyword Searching (BANKS) enables almost effortless Web publishing of relational and eXtensible Markup Language (XML) data that would otherwise remain (at least partially) invisible to the Web. Relational databases store large amounts of data that are queried using structured query languages. A user needs to know the underlying schema and the query language in order to make meaningful ad hoc queries on the data. This is a substantial barrier for casual users, such as users of Web-based information systems. HTML forms can be provided for predefined queries. A university Website may provide a form interface to search for faculty and students. Searching for departments would require yet another form, as would search for courses offered. However, creating an interface for each such task is laborious, and is also confusing to users since they must first expend effort finding which form to use. search can provide a very simple and easy-to-use mechanism for casual users to get information from databases.  Entity 2: title storing and querying ordered xml using a relational database system authors igor tatarinov , stratis d. viglas , kevin beyer , jayavel shanmugasundaram , eugene shekita , chun zhang venue international conference on management of data year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: When building a database, it is mandatory to design a friendly interface, which allo ws the nal user to easily access the data of interest. V ery often,such an interface exploits the pow er of visualization and direct manipulation mechanisms. How ever, it is not su\u00c6cient to associate \\any\" visual represen tation to a database, but the visual representation should be carefully chosen to e ectively con vey all and only the database information content. We are curren tly w orkingon a general theory (see ) for establishing the adequacy of a visual representation, once speci ed the database characteristics, and we are developing a system, called D ARE: Drawing Adequate REpresentations, which implements such a theory.  Entity 2: title the oasis multidatabase prototype authors mark roantree , john murphy , wilhelm hasselbring venue acm sigmod record year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We describe the integration of a structured-text retrieval system (TextMachine) into an object-oriented database system (OpOur approach is a light-weight one, using the external function capability of the database system to encapsulate the text retrieval system as an external information source. Yet, we are able to provide a tight integration in the query language and processing; the user can access the text retrieval system using a standard database query language. The effcient and effective retrieval of structured text performed by the text retrieval system is seamlessly combined with the rich modeling and general-purpose querying capabilities of the database system, resulting in an integrated system with querying power beyond those of the underlying systems. The integrated system also provides uniform access to textual data in the text retrieval system and structured data in the database system, thereby achieving information fusion. We discuss the design and implementation of our prototype system, and address issues such as the proper framework for external integration, the modeling of complex categorization and structure hierarchies of documents (under automatic document schema impand techniques to reduce the performance overhead of accessing an external source.  Entity 2: title the trigs active object-oriented database system - an overview authors g. kappel , w. retschitzegger venue acm sigmod record year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Structural matching and discovery in documents such as SGML and HTML is important for data warehousing [6], version management [7, 11], hypertext authoring, digital libraries [4] and Internet databases. As an example, a user of the World Wide Web may be interested in knowing changes in an HTML document [2, 5, 10]. Such changes can be detected by comparing the old and new version of the document (referred to as structural matching of documents). As another example, in hypertext authoring, a user may wish to find the common portions in the history list of a document or in a database of documents (referred to as structural discovery of documents). In SIGMOD 95 demo sessions, we exhibited a software package, called TreeDiff [13], for comparing two latex documents and showing their differences. Given two documents, the tool represents the documents as ordered labeled trees and finds an optimal sequence of edit operations to transform one document (tree) to the other. An edit operation could be an insert, delete, or change of a node in the trees. The tool is so named because documents are represented and compared using approximate tree matching techniques [9, 12, 14].  Entity 2: title efficient index structures for string databases authors tamer kahveci , ambuj k. singh venue very large data bases year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We briefly outline the main characteristics of an efficient server-based algorithm for garbage collecting object-oriented databases in a client-server environment. The algorithm is incremental and runs concurrently with client transactions. Unlike previous algorithms, it does not hold any locks on data and does not require callbacks to clients. It is fault tolerant, but performs very little logging. The algorithm has been designed to be integrated into existing OODB systems, and therefore it works with standard implementation techniques such as two-phase locking and write-ahead-logging. In addition, it supports client-server performance optimizations such as client caching and flexible management of client buffers. The algorithm has been implemented in the EXODUS storage manager before being evaluated.  Entity 2: title garbage collection in object oriented databases using transactional cyclic reference counting authors srinivas ashwin , prasan roy , s. seshadri , abraham silberschatz , s. sudarshan venue very large data bases year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Deductive databases generalize relational databases by providing support for recursive views and non-atomic data. Aditi is a deductive system based on the client-server model; it is inherently multi-user and capable of exploiting parallelism on shared-memory multiprocessors. The back-end uses relational technology for efficiency in the management of disk-based data and uses optimization algorithms especially developed for the bottom-up evaluation of logical queries involving recursion. The front-end interacts with the user in a logical language that has more expressive power than relational query languages. We present the structure of Aditi, discuss its components in some detail, and present performance figures.  Entity 2: title applying database technology in the adsm mass storage system authors luis-felipe cabrera , robert rees , wayne hineman venue very large data bases year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Mobility demands the systems be adaptive. One approach is to make adaptation transparent to applications, allowing them to remain unchanged. An alternative approach views adaptation as a collaborative partnership between applications and the system. This paper is a status report on our research on both fronts. We report on our considerable experience with application-transparent adaptation in the Coda File System. We also describe our ongoing work on application-aware adaptation in Odyssey.  Entity 2: title data replication for mobile computers authors yixiu huang , prasad sistla , ouri wolfson venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Some aggregate and grouping queries are conceptually simple, but difficult to express in SQL. This difficulty causes both conceptual and implementation problems for the SQLbased database system. Complicated queries and views are hard to understand and maintain. Further, the code produced is sometimes unnecessarily inefficient, as we demonstrate experimentally using a commercial database system. In this paper, we examine a class of queries involving (potentially repeated) selection, grouping and aggregation over the same groups, and propose an extension of SQL syntax that allows the succinct representation of these queries. We propose a new relational algebra operation that represents several levels of aggregation over the same groups in an operand relation. We demonstrate that the extended relational operator can be evaluated using efficient algorithms. We describe a translation from the extended SQL language into our algebraic language. We have implemented a preprocessor that evaluates our extended language on top of a commercial database system. We demonstrate that on a variety of examples, our implementation improves performance over standard SQL representations of the same examples by orders of magnitude.  Entity 2: title declarative updates of relational databases authors weidong chen venue acm transactions on database systems ( tods ) year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The elapsed time for external mergesort is normally dominated by I/O time. This paper is focused on reducing I/O time during the merge phase. Three new buffering and readahead strategies are proposed, called equal buffering, extended forecasting and clustering. They exploit the fact that virtually all modern disks perform caching and sequential readahead. The latter two also collect information during run formation (the last key of each run block) which is then used to preplan reading. For random input data, extended forecasting and clustering were found to reduce merge time by 30% compared with traditional double buffering.  Entity 2: title dynamic memory adjustment for external mergesort authors weiye zhang , per - &#197; ke larson venue very large data bases year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We provide an overview of query processing in parallel database systems and discuss several open issues in the optimization of queries for parallel machines.  Entity 2: title optimization of dynamic query evaluation plans authors richard l. cole , goetz graefe venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Similarity retrieval mechanisms should utilize generalized quadratic form distance functions as well as the Euclidean distance function since ellipsoid queries parameters may vary with the user and situation. In this paper, we present the spatial transformation technique that yields a new search method for adaptive ellipsoid queries with quadratic form distance functions. The basic idea is to transform the bounding rectangles in the original space, wherein distance from a query point is measured by quadratic form distance functions, into spatial objects in a new space wherein distance is measured by Euclidean distance functions. Our method significantly reduces CPU cost due to the distance approximation by the spatial transformation; exact distance evaluations are avoided for most of the accessed bounding rectangles in the index structures. We also present the multiple spatial transformation technique as an extension of the spatial transformation technique. The multiple spatial transformation technique adjusts the tree structures to suit typical ellipsoid queries; the search algorithm utilizes the adjusted structure. This technique reduces both page accesses and CPU time for ellipsoid queries. Experiments using various matrices and index structures demonstrate the superiority of the proposed methods.  Entity 2: title approximate query processing using wavelets authors kaushik chakrabarti , minos garofalakis , rajeev rastogi , kyuseok shim venue the vldb journal -- the international journal on very large data bases year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: A weighted sample is used to preserve the densities of the original data. Density biased sampling naturally includes uniform sampling as a special case. A memory efficient algorithm is proposed that approximates density biased sampling using only a single scan of the data. We empirically evaluate density biased sampling using synthetic data sets that exhibit varying cluster size distributions finding up to a factor of six improvement over uniform sampling.  Entity 2: title efficient and effective clustering methods for spatial data mining authors raymond t. ng , jiawei han venue very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Some aggregate and grouping queries are conceptually simple, but difficult to express in SQL. This difficulty causes both conceptual and implementation problems for the SQLbased database system. Complicated queries and views are hard to understand and maintain. Further, the code produced is sometimes unnecessarily inefficient, as we demonstrate experimentally using a commercial database system. In this paper, we examine a class of queries involving (potentially repeated) selection, grouping and aggregation over the same groups, and propose an extension of SQL syntax that allows the succinct representation of these queries. We propose a new relational algebra operation that represents several levels of aggregation over the same groups in an operand relation. We demonstrate that the extended relational operator can be evaluated using efficient algorithms. We describe a translation from the extended SQL language into our algebraic language. We have implemented a preprocessor that evaluates our extended language on top of a commercial database system. We demonstrate that on a variety of examples, our implementation improves performance over standard SQL representations of the same examples by orders of magnitude.  Entity 2: title on supporting containment queries in relational database management systems authors chun zhang , jeffrey naughton , david dewitt , qiong luo , guy lohman venue international conference on management of data year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: To meet the needs of many real-world control applications, concepts from Temporal, Real-Time, and Active Databases must be integrated:  Entity 2: title coalescing in temporal databases authors michael h. b &#246; hlen , richard thomas snodgrass , michael d. soo venue very large data bases year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Data mining services require accurate input data for their results to be meaningful, but privacy concerns may influence users to provide spurious information. We investigate here, with respect to mining association rules, whether users can be encouraged to provide correct information by ensuring that the mining process cannot, with any reasonable degree of certainty, violate their privacy. We present a scheme, based on probabilistic distortion of user data, that can simultaneously provide a high degree of privacy to the user and retain a high level of accuracy in the mining results. The performance of the scheme is validated against representative real and synthetic datasets.  Entity 2: title scalable parallel data mining for association rules authors eui-hong han , george karypis , vipin kumar venue international conference on management of data year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: A major challenge still facing the designers and implementors of database programming languages (DBPLs) is that of query optimisation. In the paper we first give the syntax of our archetypal DBPL and briefly discuss its semantics. We then define a small but powerful algebra of operators over the set data type, provide some key equivalences for expressions in these operators, and list transformation principles for optimising expressions.  Entity 2: title safe query languages for constraint databases authors peter z. revesz venue acm transactions on database systems ( tods ) year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In a temporal OODB, an OID index (OIDX) is needed to map from OID to the physical location of the object. In a transaction time temporal OODB, the OIDX should also index the object versions. In this case, the index entries, which we call object descriptors (OD), also include the commit timestamp of the transaction that created the object version. The OIDX in a non-temporal OODB only needs to be updated when an object is created, but in a temporal OODB, the OIDX has to be updated every time an object is updated. This has previously been shown to be a potential bottleneck, and in this paper, we present the Persistent Cache (PCache), a novel approach which reduces the index update and lookup costs in temporal OODBs.  Entity 2: title unisql/x unified relational and object-oriented database system authors won kim venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: To overcome current bottlenecks in business-to-business (B2B) electronic commerce, we need intelligent solutions for mechanizing the process of structuring, standardizing, aligning and personalizing data. This article surveys the overall content management process and discusses requirements for its scalable support.  Entity 2: title data management for pervasive computing authors mitch cherniack , michael j. franklin , stanley b. zdonik venue very large data bases year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper we present a novel technique for cost estimation of user-defined methods in advanced database systems. This technique is based on multi-dimensional histograms. We explain how the system collects statistics on the method that a database user defines and adds to the system. From these statistics a multi-dimensional histogram is built. Afterwards, this histrogram can be used for estimating the cost of the target method whenever this method is referenced in a query. This cost estimation is needed by the optimizer of the database system since this cost estimation needs to know the cost of a method in order to place it at its optimal position in the Query Execution Plan (QEP). We explain here how our technique works and we provide an example to better verify its functionality.  Entity 2: title object-relational database systems ( tutorial ) : principles , products and challenges authors michael j. carey , nelson m. mattos , anil k. nori venue international conference on management of data year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: To avoid this problem, we introduce a new organization of the directory which uses a split algorithm minimizing overlap and additionally utilizes the concept of supernodes. The basic idea of overlap-minimizing split and supernodes is to keep the directory as hierarchical as possible, and at the same time to avoid splits in the directory that would result in high overlap  Entity 2: title independence is good : dependency-based histogram synopses for high-dimensional data authors amol deshpande , minos garofalakis , rajeev rastogi venue international conference on management of data year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We present a framework for designing, in a declarative and flexible way, efficient migration programs and an undergoing implementation of a migration tool called RelOO whose targets are any ODBC compliant system on the relational side and the 02 system on the object side. The framework consists of (i) a declarative language to specify database transformations from relations to objects, but also physical properties on the object database (clustering and sorting) and (ii) an algebrabased program rewriting technique which optimizes the migration processing time while taking into account physical properties and transaction decomposition.  Entity 2: title application of oodb and sgml techniques in text database : an electronic dictionary system authors jian zhang venue acm sigmod record year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In the DBCache project, we are incorporating a database cache feature in DB2 UDB by modifying the engine code and leveraging existing federated database functionality. This allows us to take advantage of DB2's sophisticated distributed query processing power for database caching. As a result, the user queries can be executed at either the local database cache or the remote backend server, or more importantly, the query can be partitioned and then distributed to both databases for cost optimum execution.DBCache also includes a cache initialization component that takes a backend database schema and SQL queries in the workload, and generates a middle-tier database schema for the cache.  Entity 2: title caching technologies for web applications authors c. mohan venue very large data bases year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We investigate the optimization and evaluation of queries with universal quantification in the context of the object-oriented and object-relational data models. The queries are classified into 16 categories depending on the variables referenced in the so-called range and quantifier predicates. For the three most important classes we enumerate the known query evaluation plans and devise some new ones. These alternative plans are primarily based on anti-semijoin, division, generalized grouping with count aggregation, and set difference. In order to evaluate the quality of the many different evaluation plans a thorough performance analysis on some sample database configurations was carried out.  Entity 2: title relational databases for querying xml documents : limitations and opportunities authors jayavel shanmugasundaram , kristin tufte , chun zhang , gang he , david j. dewitt , jeffrey f. naughton venue very large data bases year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The problem of answering queries using views is to find efficient methods of answering a query using a set of previously materialized views over the database, rather than accessing the database relations. The problem has received significant attention because of its relevance to a wide variety of data management problems, such as data integration, query optimization, and the maintenance of physical data independence. To date, the performance of proposed algorithms has received very little attention, and in particular, their scale up in the presence of a large number of views is unknown. We first analyze two previous algorithms, the bucket algorithm and the inverse-rules, and show their deficiencies. We then describe the MiniCon, a novel algorithm for finding the maximally-contained rewriting of a conjunctive query using a set of conjunctive views. We present the first experimental study of algorithms for answering queries using views.  Entity 2: title optimizing queries using materialized views : a practical , scalable solution authors jonathan goldstein , per - &#197; ke larson venue international conference on management of data year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Research and development of multidatabase systems was triggered by the need to integrate data from heterogeneous and physically distributed information sources Issues on multidatabase architectures and semantic heterogeneity have been explored. This paper presents the multidatabase systems done and currently being done at De La Salle University. These research projects focus on multidatabase architectures, and on identifying methods in resolving various forms of schema conflicts.  Entity 2: title the mlpq/gis constraint database system authors peter revesz , rui chen , pradip kanjamala , yiming li , yuguo liu , yonghui wang venue international conference on management of data year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this article we present DynaMat, a system that manages dynamic collections of materialized aggregate views in a data warehouse. At query time, DynaMat utilizes a dedicated disk space for storing computed aggregates that are further engaged for answering new queries. Queries are executed independently or can be bundled within a multiquery expression. In the latter case, we present an execution mechanism that exploits dependencies among the queries and the materialized set to further optimize their execution. During updates, DynaMat reconciles the current materialized view selection and refreshes the most beneficial subset of it within a given maintenance window. We show how to derive an efficient update plan with respect to the available maintenance window, the different update policies for the views and the dependencies that exist among them.  Entity 2: title tam : a system for dynamic transactional activity management authors tong zhou , ling liu , calton pu venue international conference on management of data year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We discuss data mining based on association rules for two numeric attributes and one Boolean attribute. For example, in a database of bank customers, Age and Balance are two numeric attributes, and CardLoan is a Boolean attribute. Taking the pair (Age, Balance) as a point in two-dimensional space, we consider an association rule of the form  Entity 2: title scalable parallel data mining for association rules authors eui-hong han , george karypis , vipin kumar venue international conference on management of data year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Model-driven engineering technologies offer a promising approach to address the inability of third-generation languages to alleviate the complexity of platforms and express domain concepts effectively.  Entity 2: title guest editorial authors vijay atluri , anupam joshi , yelena yesha venue the vldb journal -- the international journal on very large data bases year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: New telecommunication services and mobility networks have introduced databases in telecommunication networks. Compared with traditional use of databases, telecom databases must fulfill very tough requirements on response time, throughput, and availability. ClustRa is a telecom database prototype developed to run on standard workstations interconnected by an ATM switch. To meet the throughput and real-time response requirements, ClustRa is a main memory database with neighbor main, memory logging. Transactions are executed in parallel. To meet the availability requirements, we use a 2-safe replication scheme over two sites with independent failure modes, a novel declustering strategy, early detection of failures with fast takeover, and by on-line self-repair and maintenance. This paper gives an overview of ClustRa and includes a set of performance measurements.  Entity 2: title database principles authors leonid libkin venue acm sigmod record year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Relational databases provide the ability to store user-defined functions and predicates which can be invoked in SQL queries. When evaluation of a user-defined predicate is relatively expensive, the traditional method of evaluating predicates as early as possible is no longer a sound heuristic. There are two previous approaches for optimizing such queries. However, neither is able to guarantee the optimal plan over the desired execution space. We present efficient techniques that are able to guarantee the choice of an optimal plan over the desired execution space. The optimization algorithm with complete rank-ordering improves upon the naive optimization algorithm by exploiting the nature of the cost formulas for join methods and is polynomial in the number of user-defined predicates (for a given number of relations.) We also propose pruning rules that significantly reduce the cost of searching the execution space for both the naive algorithm as well as for the optimization algorithm with complete rank-ordering, without compromising optimality. We also propose a conservative local heuristic that is simpler and has low optimization overhead. Although it is not always guaranteed to find the optimal plans, it produces close to optimal plans in most cases.  Entity 2: title query optimization by predicate move-around authors alon y. levy , inderpal singh mumick , yehoshua sagiv venue very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Data mining services require accurate input data for their results to be meaningful, but privacy concerns may influence users to provide spurious information. We investigate here, with respect to mining association rules, whether users can be encouraged to provide correct information by ensuring that the mining process cannot, with any reasonable degree of certainty, violate their privacy. We present a scheme, based on probabilistic distortion of user data, that can simultaneously provide a high degree of privacy to the user and retain a high level of accuracy in the mining results. The performance of the scheme is validated against representative real and synthetic datasets.  Entity 2: title data mining with optimized two-dimensional association rules authors takeshi fukuda , yasuhiko morimoto , shimichi morishita , takeshi tokuyama venue acm transactions on database systems ( tods ) year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources.  Entity 2: title tavant system architecture for sell-side channel management authors srinivasa narayanan , subbu n. subramanian venue very large data bases year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.  Entity 2: title building knowledge base management systems authors john mylopoulos , vinay chaudhri , dimitris plexousakis , adel shrufi , thodoros topologlou venue the vldb journal -- the international journal on very large data bases year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper, we present and thoroughly evaluate a new class of query optimization algorithms that are based on a principle that we call iterative dynamic programming, or IDP for short. IDP has several important advantages: First, IDP-algorithms produce the best plans of all known algorithms in situations in which dynamic programming is not viable because of its high complexity. Second, some IDP variants are adaptive and produce as good plans as dynamic programming if dynamic programming is viable and as good-as possible plans if dynamic programming turns out to be not viable. Three, all IDP-algorithms can very easily be integrated into an existing optimizer which is based on dynamic programming.  Entity 2: title sql query optimization : reordering for a general class of queries authors piyush goel , bala iyer venue international conference on management of data year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: A new access method, called M-tree, is proposed to organize and search large data sets from a generic \u201cmetric space\u201d, i.e. where object proximity is only defined by a distance function satisfying the positivity, symmetry, and triangle inequality postulates. We detail algorithms for insertion of objects and split management, which keep the M-tree always balanced - several heuristic split alternatives are considered and experimentally evaluated. Algorithms for similarity (range and k-nearest neighbors) queries are also described. Results from extensive experimentation with a prototype system are reported, considering as the performance criteria the number of page I/O\u2019s and the number of distance computations. The results demonstrate that the Mtree indeed extends the domain of applicability beyond the traditional vector spaces, performs reasonably well in high-dimensional data spaces, and scales well in case of growing files.  Entity 2: title index-driven similarity search in metric spaces authors gisli r. hjaltason , hanan samet venue acm transactions on database systems ( tods ) year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Currently, the Internet provides access to a very large number and wide variety of information sources (e.g., textual databases, sites containing technical reports, directory listings), and systems to access these sources (e.g., World Wide Web, Gopher, WAIS). The challenge is to provide easy, efficient, robust and secure access to this information and other kinds (e.g., relational and object oriented databases). This aim of this panel is to explore whether there are any new technical problems, relevant to the Database field, that need to be solved in order to realize such global information systems. In particular, we debate whether existing techniques from database systems (e.g., multidatabases and distributed databases) can be applied or straigtitforwardly extended to global information systems. Furthermore, we attempt to establish realistic goals for database technologies in global information systems. Some of the specific issues discussed are the following:  Entity 2: title integrating modelling systems for environmental management information systems authors david j. abel , kerry taylor , dean kun venue acm sigmod record year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Scientific data of importance to biologists in the Humitn Genome Project resides not only in conventional da.tabases, but in structured files maintained in a number of different formats (e.g. ASN.1 a.nd ACE) as well a.s sequence analysis packages (e.g. BLAST and FASTA). These formats and packages contain a number of data types not found in conventional databases, such as lists and variants, and may be deeply nested. We present in this paper techniques for querying and transforming such data, and illustrate their use in a prototype system developed in conjunction with the Human Genome Center for Chromosome 22. We also describe optimizations performed by the system, a crucial issue for bulk data.  Entity 2: title lineage tracing for general data warehouse transformations authors yingwei cui , jennifer widom venue very large data bases year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This is a bibliography on active databases and active database systems which reflects the various research activities in this field. We compiled this bibliography for our own use, but hopefully it might be useful to other people as well. All papers that appear in the following list, are generally accessible.We do not claim that the bibliography is exhaustive and covers the complete range of literature that deals with activities. We decided to focus on central approaches, concepts, methods, and systems in the area of active databases. It does not contain entries in the area of \"pure\" real-time, object-oriented, temporal, and deductive databases. But we did include publications related to those approaches, as long as they discuss active databases.We divided the material into various sections following our own personal perception of the field. The sections provide an overview on different projects in the area of active databases, followed by sections on relevant research topics. Each section contains a few remarks followed by a list of cross references into the annotated bibliography. Papers might appear in more than one section in case they discuss different topics relevant to different sections.Additionally, when relevant we also included unpublished, but publicly available material. For those papers we included information how to obtain them from the authors or from the organizations where the were produced.The beauty of our work is the individual annotation to almost all publications. Due to space limitations we are forced to leave out those annotations in the version published here. For a complete annotated bibliography we refer to the entry in our WWW server.The effort to build up such a bibliography is an endless task. Since we believe it now provides a comprehensive overview on the existing literature in the field, we decided to publish it. However, we invite all readers to add remarks, corrections, updates, additions (including further annotations).Part of this work was done while we were associated with the FORWISS Institute of the Technical University of Munich. We would like to thank our student, Markus Blaschka, who compiled many references during his master's thesis.  Entity 2: title promises and realities of active database systems authors eric simon , angelika kotz dittrich venue very large data bases year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The analysis of time series in financial and scientific applications requires database functionality with complex specialized modeling capabilities and at the same time an easy-to-use interface. We present the time series management system CALANDA which combines both, a powerful dedicated data model and an intuitive GUI. The focus of this paper and the demonstration is to show how CALANDA is accessed by end users.  Entity 2: title research perspectives for time series management systems authors werner dreyer , angelika kotz dittrich , duri schmidt venue acm sigmod record year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper we propose a distributed case-based approach to the problem of rewriting queries. According to this approach we use a case memory instead of static views, i.e. views that are deened a priori. As a consequence, the mediated schema is dynamically updated, strongly innuenced by the queries submitted by a consumer. This approach allows a mediator to face systems where consumers may change their customization needs and information sources may become unavailable, may be added, or may modify their schemas.  Entity 2: title instance-based attribute identification in database integration authors cecil eng h. chua , roger h. l. chiang , ee-peng lim venue the vldb journal -- the international journal on very large data bases year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Indexing a class hierarchy, in order to efficiently search or update the objects of a class according to a (range of) value(s) of an attribute, impacts OODB performance heavily. For this indexing problem, most systems use the class hierarchy index (CH) technique of [15] implemented using B+-trees. Other techniques, such as those of [14, 18,31], can lead to improved average-case performance but involve the implementation of new data-structures. As a special form of external dynamic two-dimensional range searching, this OODB indexing problem is solvable within reasonable worst-case bounds [12]. Based on this insight, we have developed a technique, called indexing by class-division (CD), which we believe can be used as a practical alternative to CH. We present an optimized implementation and experimental validation of CD's average-case performance. The main advantages of the CD technique are: (1) CD is an extension of CH that provides a significant speed-up over CH for a wide spectrum of range queries--this speed-up is at least linear in the number of classes queried for uniform data and larger otherwise; and (2) CD queries, updates and concurrent use are implementable using existing B+-tree technology. The basic idea of class-division involves a time-space tradeoff and CD requires some space and update overhead in comparison to CH. In practice, this overhead is a small factor (2 to 3) and, in worst-case, is bounded by the depth of the hierarchy and the logarithm of its size.  Entity 2: title very large databases in a commercial application environment authors karl-heinz hess venue very large data bases year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: With the increasing importance of XML, LDAP directories, and text-based information sources on the Internet, there is an ever-greater need to evaluate queries involving (sub)string matching. In many cases, matches need to be on multiple attributes/dimensions, with correlations between the multiple dimensions. Effective query optimization in this context requires good selectivity estimates. In this paper, we use pruned count-suffix trees (PSTs) as the basic data structure for substring selectivity estimation. For the 1-D problem, we present a novel technique called MO (Maximal Overlap). We then develop and analyze two 1-D estimation algorithms, MOC and MOLC, based on MO and a constraint-based characterization of all possible completions of a given PST. For the k-D problem, we first generalize PSTs to multiple dimensions and develop a space- and time-efficient probabilistic algorithm to construct k-D PSTs directly. We then show how to extend MO to multiple dimensions. Finally, we demonstrate, both analytically and experimentally, that MO is both practical and substantially superior to competing algorithms.  Entity 2: title hierarchical subspace sampling : a unified framework for high dimensional data reduction , selectivity estimation and nearest neighbor search authors charu c. aggarwal venue international conference on management of data year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We enunciate the need for watermarking database relations to deter their piracy, identify the unique characteristics of relational data which pose new challenges for watermarking, and provide desirable properties of a watermarking system for relational data. A watermark can be applied to any database relation having attributes which are such that changes in a few of their values do not affect the applications.  Entity 2: title data page layouts for relational databases on deep memory hierarchies authors anastassia ailamaki , david j. dewitt , mark d. hill venue the vldb journal -- the international journal on very large data bases year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The paper shows how modern architectures can be used to speed up ranking algorithms. In the paper \u201cOn optimality-ratio and coverage in ranking of joined search results\u201d, the authors study a novel ranking problem. Instead of ranking individual items, they consider ranking of combinations of items, e.g., a combination of a hotel and two restaurants. They study the semantics and query processing algorithms in this context. The paper shows the kind of new ranking problems that emerge in these new-age applications. The paper titled \u201cDistributed top-k query processing by exploiting skyline summaries\u201d studies top-k processing in distributed environments. With increasing volumes of data, the data is typically distributed over multiple servers.  Entity 2: title guest editorial authors malcolm p. atkinson venue the vldb journal -- the international journal on very large data bases year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Editor's note: For this issue's \"From the Editors,\" I invited Robert Gephart of the University of Al-berta to reflect on his observations as a long-serving , award-winning reviewer of qualitative research for A!vII Over the past two and a half years, I have developed a tremendous respect for Bob's keen eye for evaluating qualitative research submissions , and great admiration for the painstaking advice he provides authors about how to improve their work. As a world-renowned qualitative author himself, Bob is in an excellent position to provide observations about how authors might increase the chances of having their qualitative research accepted for publication at AMI In a three-way electronic mail conversation about the challenges and opportunities of qualitative research , Bob, Tom Lee, and I all concluded that many authors with potentially very interesting data sets don't seem to know how to analyze them to their full potential. This is perhaps not surprising, gi ven the clear predominance of quantitative methods and statistics courses over qualitative ones, particularly in North America, as well as the inherently greater subjectivity involved in designing and analyzing qualitative research. As such, we encouraged Bob to provide a bit of a minitutorial-complete with reference citations and examples of high-quality papers that use particular qualitative approaches-in addition to his observations about qualitative research submitted to AMI The result is a longer-than-usual \"From the Edi-tors\" column. but one that we believe is well worth the extra reading time for anyone interested in producing , reviewing, or attempting to coax greater insights from qualitative research. We are fortunate to have someone with Bob's expertise share his observations, and we hope that his thoughts will prove useful to researchers for many years to come.  Entity 2: title editorial authors richard snodgrass venue acm transactions on database systems ( tods ) year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Support for virtual states and deltas between them is useful for a variety of database applications, including hypothetical database access, version management, simulation, and active databases. The Heraclitus paradigm elevates delta values to be \"first-class citizens\" in database programming languages, so that they can be explicitly created, accessed and manipulated.A fundamental issue concerns the trade-off between the \"accuracy\" or \"robustness\" of a form of delta representation, and the ease of access and manipulation of that form. At one end of the spectrum, code-blocks could be used to represent delta values, resulting in a more accurate capture of the intended meaning of a proposed update, at the cost of more expensive access and manipulation. In the context of object-oriented databases, another point on the spectrum is \"attribute-granularity\" deltas which store the net changes to each modified attribute value of modified objects.This paper introduces a comprehensive framework for specifying a broad array of forms for representing deltas for complex value types (tuple, set, bag, list, o-set and dictionary). In general, the granularity of such deltas can be arbitrarily deep within the complex value structure. Applications of this framework in connection with hypothetical access to, and \"merging\" of, proposed updates are discussed.  Entity 2: title a cost model for clustered object-oriented databases authors georges gardarin , jean-robert gruser , zhao-hui tang venue very large data bases year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: There is not appropriate testing method for the purchasing process of sealing washer in hydraulic support producing company at present.In order to solve the problem,by using the performance testing system of sealing washer worked upright column for hydraulic support,the author designed a test bed used to test the seal performance of hydraulic cylinder in the mine hydraulic support to provide database supports for purchasing sealing washer for hydraulic support manufacturers.In this paper,there will be the introduction of the principles,constitutions and functions of the test bed.It is reflected by the practical applications that the test bed is operating stably,accurately and efficiently which could be used by testing sealing washer.  Entity 2: title database principles authors leonid libkin venue acm sigmod record year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Previous work on functional joins was constrained in two ways: (1) all approaches we know assume references being implemented as physical object identifiers (OIDs) and (2) most approaches are, in addition, limited to single-valued reference attributes. Both are severe limitations since most object-relational and all object-oriented database systems do support nested reference sets and many object systems do implement references as location-independent (logical) OIDs. In this work, we develop a new functional join algorithm that can be used for any realization form for OIDs (physical or logical) and is particularly geared towards supporting functional joins along nested reference sets. The algorithm can be applied to evaluate joins along arbitrarily long path expressions which may include one or more reference sets. The new algorithm generalizes previously proposed partition-based pointer joins by repeatedly applying partitioning with interleaved re-merging before evaluating the next functional join. Consequently, the algorithm is termed P(PM)*M where P stands for partitioning and M denotes merging. Our prototype implementation as well as an analytical assessment based on a cost model prove that this new algorithm performs superior in almost all database configurations. *This work was supported in part by the German National Research Foundation DFG under contracts Ke 401/6-2 and Ke 40117-I.  Entity 2: title cost-based selection of path expression processing algorithms in object-oriented databases authors georges gardarin , jean-robert gruser , zhao-hui tang venue very large data bases year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: My purpose here at this conference is to provide a background, first for meetings like this on subjects dealing with computers, and second, for this particular meeting. My qualifications for keynoting are, I think, as good as those of anyone here: as far as this particular meeting is concerned, I am not now directly engaged in working on computers of the type that we are going to discuss; and the organization that I represent, the Bell Telephone Laboratories, is not in the business of making such computers. So I speak as a relatively innocent bystander.  Entity 2: title keynote address authors robert s. epstein venue international conference on management of data year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We witness a rapid increase in the number of structured information sources that are available online, especially on the WWW. These sources include commercial databases on product information, stock market information, real estate, automobiles, and entertainment. We would like to use the data stored in these databases to answer complex queries that go beyond keyword searches. We face the following challenges: (1) Several information sources store interrelated data, and any query-answering system must understand the relationships between their contents. (2) Many sources are not full-featured database systems and can answer only a small set of queries over their data (for example, forms on the WWW restrict the set of queries one can (3) Since the number of sources is very large, effective techniques are needed to prune the set of information sources accessed to answer a query. (4) The details of interacting with each source vary greatly. We describe the Information Manifold, an implemented system that provides uniform access to a heterogeneous collection of more than 100 information sources, many of them on the WWW. IM tackles the above problems by providing a mechanism to describe declaratively the contents and query capabilities of available information sources. There is a clean separation between the declarative source description and the actual details of interacting with an information source. We describe algorithms that use the source descriptions to prune effciently the set of information sources for a given query and practical algorithms to generate executable query plans. The query plans we generate can inolve querying several information sources and combining their answers. We also present experimental studies that indicate that the architecture and algorithms used in the Information Manifold scale up well to several hundred information sources  Entity 2: title rate-based query optimization for streaming information sources authors stratis d. viglas , jeffrey f. naughton venue international conference on management of data year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We present the PPOST-architecture (Persistent Parallel Object Store) for main-memory database systems on parallel computers, that is suited for applications with challenging performance requirements. The architecture takes full advantage of parallelism, large main memories and fast switching networks. An important property of this architecture is its excellent scaling behavior.  Entity 2: title towards self-tuning data placement in parallel database systems authors mong li lee , masaru kitsuregawa , beng chin ooi , kian-lee tan , anirban mondal venue international conference on management of data year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The present paper introduces techniques that solve this problem. Experience with a working prototype optimizer demonstrates (i) that the additional optimization and start-up overhead of dynamic plans compared to static plans is dominated by their advantage at run-time, (ii) that dynamic plans are as robust as the \u201cbrute-force\u201d remedy of run-time optimization, i.e., dynamic plans maintain their optimality even if parameters change between compile-time and run-time, and (iii) that the start-up overhead of dynamic plans is significantly less than the time required for complete optimization at run-time. In other words, our proposed techniques are superior to both techniques considered to-date, namely compile-time optimization into a single static plan as well as run-time optimization. Finally, we believe that the concepts and technology described can be transferred to commercial query optimizers in order to improve the performance of embedded queries with host variables in the query predicate and to adapt to run-time system loads unpredictable at compile time.  Entity 2: title adaptable query optimization and evaluation in temporal middleware authors giedrius slivinskas , christian s. jensen , richard thomas snodgrass venue international conference on management of data year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Data mining on large data warehouses is becoming increasingly important. In support of this trend, we consider a spectrum of architectural alternatives for coupling mining with database systems. These alternatives include: loose-coupling through a SQL cursor interface; encapsulation of a mining algorithm in a stored procedure; caching the data to a file system on-the-fly and mining; tight-coupling using primarily user-defined functions; and SQL implementations for processing in the DBMS. We comprehensively study the option of expressing the mining algorithm in the form of SQL queries using Association rule mining as a case in point. We consider four options in SQL-92 and six options in SQL enhanced with object-relational extensions (SQL-OR). Our evaluation of the different architectural alternatives shows that from a performance perspective, the Cache-Mine option is superior, although the performance of the SQL-OR option is within a factor of two. Both the Cache-Mine and the SQL-OR approaches incur a higher storage penalty than the loose-coupling approach which performance-wise is a factor of 3 to 4 worse than Cache-Mine. The SQL-92 implementations were too slow to qualify as a competitive option. We also compare these alternatives on the basis of qualitative factors like automatic parallelization, development ease, portability and inter-operability.  Entity 2: title querying network directories authors h. v. jagadish , laks v. s. lakshmanan , tova milo , divesh srivastava , dimitra vista venue international conference on management of data year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We present a framework which allows the user to access and manipulate data uniformly, regardless of whether it resides in a database or in the file system (or in both). A key issue is the performance of the system. We show that text indexing, combined with newly developed optimization techniques, can be used to provide an efficient high level interface to information stored in files. Furthermore, using these techniques, some queries can be evaluated significantly faster than in standard database implementations. We also study the tradeoff between efficiency and the amount of indexing.  Entity 2: title inferring function semantics to optimize queries authors mitch cherniack , stanley b. zdonik venue very large data bases year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Integrated access to information that is spread over multiple, distributed, and heterogeneous sources is an important problem in many scienti c and commercial domains. While much work has been done on query processing and choosing plans under cost criteria, very little is known about the important problem of incorporating the information quality aspect into query planning. In this paper we describe a framework for multidatabase query processing that fully includes the quality of information in many facets, such as completeness, timeliness, accuracy, etc. We seamlessly include information quality into a multidatabase query processor based on a view-rewriting mechanism. We model information quality at di erent levels to ultimately nd a set of high-quality queryanswering plans.  Entity 2: title semantic interoperability in global information systems authors a. m. ouksel , a. sheth venue acm sigmod record year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Discovery of association rules .is an important database mining problem. Current algorithms for finding association rules require several passes over the analyzed database, and obviously the role of I/O overhead is very significant for very large databases. We present new algorithms that reduce the database activity considerably. The idea is to pick a Random sample, to find using this sample all association rules that probably hold in the whole database, and then to verify the results with the rest of the database. The algorithms thus produce exact association rules, not approximations based on a sample. The approach is, however, probabilistic, and in those rare cases where our sampling method does not produce all association rules, the missing rules can be found in a second pass. Our experiments show that the proposed algorithms can find association rules very efficiently in only one database  Entity 2: title large databases for remote sensing and gis authors a. r. dasgupta venue very large data bases year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Spatio-temporal databases deal with geometries changing over time. The goal of our work is to provide a DBMS data model and query language capable of handling such time-dependent geometries, including those changing continuously that describe moving objects. Two fundamental abstractions are moving point and moving region, describing objects for which only the time-dependent position, or position and extent, respectively, are of interest. We propose to present such time-dependent geometries as attribute data types with suitable operations, that is, to provide an abstract data type extension to a DBMS data model and query language. This paper presents a design of such a system of abstract data types. It turns out that besides the main types of interest, moving point   and moving region, a relatively large number of auxiliary data types are needed. For example, one needs a line type to represent the projection of a moving point into the plane, or a \u201cmoving real\u201d to represent the time-dependent distance of two points. It then becomes crucial to achieve (i) orthogonality in the design of the system, i.e., type constructors can be applied unifomly; (ii) genericity and consistency of operations, i.e., operations range over as many types as possible and behave consistently; and (iii) closure and consistency between structure and operations of nontemporal and related temporal types. Satisfying these goal leads to a simple and expressive system of abstract data types that may be integrated into a query language to yield a powerful language for   querying spatio-temporal data, including moving objects. The paper formally defines the types and operations, offers detailed insight into the considerations that went into the design, and exemplifies the use of the abstract data types using SQL. The paper offers a precise and conceptually clean foundation for implementing a spatio-temporal DBMS extension.  Entity 2: title an introduction to deductive database languages and systems authors kotagiri ramamohanarao , james harland venue the vldb journal -- the international journal on very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Although research on temporal database systems has been active for about 20 years, implementations have not appeared until recently. This is one reason why current commercial database systems provide only limited temporal functionality. This paper summarizes extant state of the art of temporal database implementations. Rather than being very specific about each system we have attempted to provide an indication of the functionality together with pointers to additional information. It is hoped that this leads to more efforts pushing the implementation of temporal database systems in the near future.  Entity 2: title the ores temporal database management system authors babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We introduce a rich language of descriptions for semistructured tree-like data, and we explain how such descriptions relate to the data they describe. Various query languages and data schemas can b...  Entity 2: title capturing and querying multiple aspects of semistructured data authors curtis e. dyreson , michael h. b &#246; hlen , christian s. jensen venue very large data bases year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We consider data to be semistructured when there is no schema fixed or known in advance and when the data may be incomplete or irregular. For example, HTML files on the World-Wide Web usually contain some structure, but often the data is irregular or In addition, data integrated from multiple, heterogeneous information sources often is semistructured. Storing and querying semistructured data poses considerably different problems and requirements than those for traditional databases, where data storage and query processing are dependent upon structured data. Relational, nested-relational, and object-oriented database systems, for example, all depend upon the data having a known and regular schema.  Entity 2: title storing semistructured data with stored authors alin deutsch , mary fernandez , dan suciu venue international conference on management of data year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources.  Entity 2: title data management for earth system science authors james frew , jeff dozier venue acm sigmod record year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Advanced applications in fields such as CAD, software engineering, real-time process control, corporate repositories and digital libraries require the construction, efficient access and management of large, shared knowledge bases. Such knowledge bases cannot be built using existing tools such as expert system shells, because these do not scale up, nor can they be built in terms of existing database technology, because such technology does not support the rich representational structure and inference mechanisms required for knowledge-based systems. This paper proposes a generic architecture for a knowledge base management system intended for such applications. The architecture assumes an object-oriented knowledge representation language with an assertional sublanguage used to express constraints and rules. It also provides for general-purpose deductive inference and special-purpose temporal reasoning. Results reported in the paper address several knowledge base management issues. For storage management, a new method is proposed for generating a logical schema for a given knowledge base. Query processing algorithms are offered for semantic and physical query optimization, along with an enhanced cost model for query cost estimation. On concurrency control, the paper describes a novel concurrency control policy which takes advantage of knowledge base structure and is shown to outperform two-phase locking for highly structured knowledge bases and update-intensive transactions. Finally, algorithms for compilation and efficient processing of constraints and rules during knowledge base operations are described. The paper describes original results, including novel data structures and algorithms, as well as preliminary performance evaluation data. Based on these results, we conclude that knowledge base management systems which can accommodate large knowledge bases are feasible.  Entity 2: title database systems management and oracle8 authors c. gregory doherty venue international conference on management of data year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Commercial parallel database systems such as DB2 Parallel Edition (DB2 PE) [l, 21 are delivering the ability to execute complex queries on very large databases. However, the serial application interface to these database systems can become a bottleneck for a growing list of applications such as mailing list generation and data propagation from a warehouse to smaller data marts. In this abstract, we describe the CURRENT NODE and NODENUMBER functions provided by DB2 PE and show how these two functions can be used to retrieve data in parallel in a linearly scalable manner with respect to the number of nodes in the system. Before proceeding further, we should point out that DB2 PE uses a hash partitioning strategy to distribute rows of a table to nodes in a nodegroup which is a user-specified subset of system nodes. We apply a system-specified hashing function on the user-specified partitioning key values to generate a partition number. This number is used as an index into a partition map (which can be modified by users) to find the node number where the row will be stored.  Entity 2: title managing a db2 parallel edition database authors gilles fecteau venue very large data bases year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In the bottom-up evaluation of logic programs and recursively defined views on databases, all generated facts are usually assumed to be stored until the end of the evaluation. Discarding facts during the evaluation, however, can considerably improve the efficiency of the evaluation: the space needed to evaluate the program, the I/O costs, the costs of maintaining and accessing indices, and the cost of eliminating duplicates may all be reduced. Given an evaluation method that is sound, complete, and does not repeat derivation steps, we consider how facts can be discarded during the evaluation without compromising these properties. We show that every such space optimization method has certain components, the first to ensure soundness and completeness, the second to avoid redundancy (i.e., repetition of derivations), and the third to reduce \u201cfact lifetimes\u201d (i.e., the time period for which each fact must be retained during evaluation). We present new techniques based on providing bounds on the number of derivations and uses of facts, and using monotonicity constraints for each of the first two components, and provide novel synchronization techniques for the third component of a space optimization method.  Entity 2: title dataguides : enabling query formulation and optimization in semistructured databases authors roy goldman , jennifer widom venue very large data bases year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: As we approach the next century, the software industry landscape is undergoing massive technology and business changes. The client/server revolution has barely reached its half-life and it is already being eclipsed by the Internet revolution. Software development is moving away from the direction of being labour-intensive. Customers are buying more pre-packaged software solutions or software components that can easily be assembled on site by in-house personnel or systems integrators. Except for a handful of players like Microsoft, Oracle and Computer Associates, very few leading software companies of the seventies and eighties have survived into the nineties. A whole new generation of software companies have emerged that are focussed on selling advanced\u2019 software components based on industry standards. For Indian software companies with superior technology development skills, the Internet will open up opportunities to build products that have never been built before and to enter global markets on a scale that was never attempted before.  Entity 2: title the evolution of the web and implications for an incremental crawler authors junghoo cho , hector garcia-molina venue very large data bases year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this article, we first introduce the syntax of Temporal-Probabilistic (TP) relations and then show how they can be converted to an explicit, significantly more space-consuming form, called Annotated Relations. We then present a theoretical annotated temporal algebra (TATA). Being explicit, TATA is convenient for specifying how the algebraic operations should behave, but is impractical to use because annotated relations are overwhelmingly large. Next, we present a temporal probabilistic algebra (TPA). We show that our definition of the TP-algebra provides a correct implementation of TATA despite the fact that it operates on implicit, succinct TP-relations instead of overwhemingly large annotated relations. Finally, we report on timings for an implementation of the TP-Algebra built on top of ODBC.  Entity 2: title a probabilistic relational model and algebra authors debabrata dey , sumit sarkar venue acm transactions on database systems ( tods ) year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper deals with finding outliers (exceptions) in large, multidimensional datasets. The identification of outliers can lead to the discovery of truly unexpected knowledge in areas such as electronic commerce, credit card fraud, and even the analysis of performance statistics of professional athletes. Existing methods that we have seen for finding outliers in large datasets can only deal efficiently with two dimensions/attributes of a dataset. Here, we study the notion of DB- (DistanceBased) outliers. While we provide formal and empirical evidence showing the usefulness of DB-outliers, we focus on the development of algorithms for computing such outliers. First, we present two simple algorithms, both having a complexity of O(k N\u2019), k being the dimensionality and N being the number of objects in the dataset. These algorithms readily support datasets with many more than two attributes. Second, we present an optimized cell-based algorithm that has a complexity that is linear wrt N, but exponential wrt k. Third, for datasets that are mainly disk-resident, we present another version of the cell-based algorithm that guarantees at most 3 passes over a dataset. We provide  Entity 2: title fast algorithms for mining association rules in large databases authors rakesh agrawal , ramakrishnan srikant venue very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way.  Entity 2: title query processing in tertiary memory databases authors sunita sarawagi venue very large data bases year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The automatic reclamation of storage for unreferenced objects is very important in object databases. Existing language system algorithms for automatic storage reclamation have been shown to be inappropriate. In this paper, we investigate methods to improve the performance of algorithms for automatic for automatic storage reclamation of object databases. These algorithms are based on a technique called partitioned garbage collection, in which a subset of the entire database is collected independently of the rest. Specifically, we investigate the policy that is used to select what partition in the database should be collected. The policies that we propose and investigate are based on the intuition that the values of overwritten pointers provide good hints about  where to find garbage. Using trace-driven simulation, we show that one of our policies requires less I/O to collect more garbage than any existing implementable policy and performs close to a near-optimal policy over a wide range of database sizes and object connectivities.  Entity 2: title efficient incremental garbage collection for client-server object database systems authors laurent amsaleg , michael j. franklin , olivier gruber venue very large data bases year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Although \u201cnow<\u201d is expressed in SQL and CURRENT_TIMESTAMP within queries, this value cannot be stored in the database. How ever, this notion of an ever-increasing current-time value has been reflected in some temporal data models by inclusion of database-resident variables, such as \u201cnow<\u201d \u201cuntil-changed,< \u201d \u201c**,\u201d \u201c@,\u201d and \u201c-\u201d. Time variables are very desirable, but their used also leads to a new type of database, consisting of tuples with variables, termed a variable database.<  Entity 2: title a structured approach for the definition of the semantics of active databases authors piero fraternali , letizia tanca venue acm transactions on database systems ( tods ) year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Garbage collection is important in object-oriented databases to free the programmer from explicitly deallocating memory. In this paper, we present a garbage collection algorithm, called Transactional Cyclic Reference Counting (TCRC), for object-oriented databases. The algorithm is based on a variant of a reference-counting algorithm proposed for functional programming languages The algorithm keeps track of auxiliary reference count information to detect and collect cyclic garbage. The algorithm works correctly in the presence of concurrently running transactions, and system failures. It does not obtain any long-term locks, thereby minimizing interference with transaction processing. It uses recovery subsystem logs to detect pointer updates; thus, existing code need not be rewritten. Finally, it exploits schema information, if available, to reduce costs. We have implemented the TCRC algorithm and present results of a performance study of the implementation.  Entity 2: title efficient incremental garbage collection for client-server object database systems authors laurent amsaleg , michael j. franklin , olivier gruber venue very large data bases year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Managing the transactions in real time distributed computing system is not easy, as it has heterogeneously networked computers to solve a single problem. If a transaction runs across some different sites, it may commit at some sites and may failure at another site, leading to an inconsistent transaction. The complexity is increase in real time applications by placing deadlines on the response time of the database system and transactions processing. Such a system needs to process transactions before these deadlines expired. A series of simulation study have been performed to analyze the performance under different transaction management under conditions such as different workloads, distribution methods, execution mode-distribution and parallel etc. The scheduling of data accesses are done in order to meet their deadlines and to minimize the number of transactions that missed deadlines. A new concept is introduced to manage the transactions in database size for originating site and remote site rather than database size computing parameters. With this approach, the system gives a significant improvement in performance.  Entity 2: title relaxed transaction processing authors munindar p. singh , christine tomlinson , darrell woelk venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The University of Ulm was founded in 1967 with focus on medicine and natural sciences. In 1989 the University established two new faculties: Engineering Sciences and Computer Science. This enlargement took place within the framework of the so-called Science City Ulm. In a joint effort, the State of Baden-W\u00fcrttemberg, industrial companies, the University, and the City of Ulm successfully established a research and development infrastructure at or nearby the university campus consisting of the university's research labs, university-related research institutes like the Research Institute for Applied Knowledge Processing (FAW), and industrial research and development labs, especially a large research center of Daimler-Benz AG.  Entity 2: title information systems research at rwth aachen authors matthias jarke venue acm sigmod record year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Data warehousing systems integrate information from operational data sources into a central repository to enable analysis and mining of the integrated information. During the integration process, source data typically undergoes a series of transformations, which may vary from simple algebraic operations or aggregations to complex \u201cdata cleansing \u201d procedures. In a warehousing environment, the data lineage problem is that of tracing warehouse data items back to the original source items from which they were derived. We formally define the lineage tracing problem in the presence of general data warehouse transformations, and we present algorithms for lineage tracing in this environment. Our tracing procedures take advantage of known structure or properties of transformations when present, but also work in the absence of such information. Our results can be used as the basis for a lineage tracing tool in a general warehousing setting, and also can guide the design of data warehouses that enable efficient lineage tracing.  Entity 2: title lineage tracing for general data warehouse transformations authors y. cui , j. widom venue the vldb journal -- the international journal on very large data bases year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Many real-time database applications arise in electronic financial services, safety-critical installations and military systems where enforcing security is crucial to the success of the enterprise. We investigate here the performance implications, in terms of killed transactions, of guaranteeing multi-level secrecy in a real-time database system supporting applications with firm deadlines. In particular, we focus on the buffer management aspects of this issue.Our main contributions are the following. First, we identify the importance and difficulties of providing secure buffer management in the real-time database environment. Second, we present SABRE, a novel buffer management algorithm that provides covert-channel-free security. SABRE employs a fully dynamic one-copy allocation policy for efficient usage of buffer resources.  Entity 2: title deeds towards a distributed and active real-time database system authors s. f. andler , j. hansson , j. eriksson , j. mellin , m. berndtsson , b. eftring venue acm sigmod record year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: George Mason University began as an independent state university in 1972. Its development has been marked by rapid growth and innovative planning, resulting in an enrollment of more than 24,000 students in 1997. It is located in Fairfax, Virginia\u2014about fifteen miles southwest of Washington, DC\u2014near many governmental agencies and industrial firms specializing in information-intensive products and services. Information and Software Systems Engineering (ISSE) is one of six departments in GMU's School of Information Technology and Engineering (SITE). Established in 1985, SITE has approximately 90 faculty and ISSE has 13 full time faculty. ISSE is a rapidly growing department with wide-ranging teaching and research interests.  Entity 2: title research and practice in federated information systems authors w. hasselbring , w.-j . van den heuvel , g. j. houben , r.-d . kutsche , b. rieger , m. roantree , k. subieta venue acm sigmod record year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper presents the design of a read-optimized relational DBMS that contrasts sharply with most current systems, which are write-optimized. Among the many differences in its design are: storage of data by column rather than by row, careful coding and packing of objects into storage including main memory during query processing, storing an overlapping collection of column-oriented projections, rather than the current fare of tables and indexes, a non-traditional implementation of transactions which includes high availability and snapshot isolation for read-only transactions, and the extensive use of bitmap indexes to complement B-tree structures.  Entity 2: title an aspect of query optimization in multidatabase systems authors chiang lee , chia-jung chen , hongjun lu venue acm sigmod record year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper is a survey of work and issues on multidimensional search trees. We provide a classification of such methods, we describe the related algorithms, we present performance analysis efforts, and finally outline future research directions. Multi-dimensional search trees and Spatial Access Methods, in general, are designed to handle spatial objects, like points, line segments, polygons, polyhedra etc. The goal is to support spatial queries, such as nearest neighbors queries (find all cities within 10 miles from Washington D.C.), or range queries (find all the lakes on earth, within 30 and 40 degrees of latitude), and so on. The applications are numerous, including traditional database multi-attribute indexing, Geographic Information Systems and spatial database systems, and indexing multimedia databases by content. $\u2018rom the spatial databases viewpoint we can dist,inguish between two major classes of access methods:  Entity 2: title efficient concurrency control in multidimensional access methods authors kaushik chakrabarti , sharad mehrotra venue international conference on management of data year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Business-oriented workflows have been studied since the 70's under various names (office automation, workflow management, business process management) and by different communities, including the database community. Much basic and applied research has been conducted over the years, e.g. theoretical studies of workflow languages and models (based on Petri-nets or process calculi), their properties, transactional behavior, etc.  Entity 2: title guest editorial authors philip a. bernstein , yannis ioannidis , raghu ramakrishnan venue the vldb journal -- the international journal on very large data bases year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: METU Object-Oriented DBMS1 includes the implementation of a database kernel, an object-oriented SQL-like language and a graphical user interface. Kernel functions are divided between a SQL Interpreter and a C++ compiler. Thus the interpretation of functions are avoided increasing the efficiency of the system. The compiled by C++ functions are used by the system through the Function Manager. The system is realized on Exodus Storage Manager (ESM), thus exploiting some of the kernel functions readily provided by ESM. The additional functions provided by the MOOD kernel are the optimization and interpretation of SQL statements, dynamic linking of functions, and catalog management.  Entity 2: title lambda-db : an odmg-based object-oriented dbms authors leonidas fegaras , chandrasekhar srinivasan , arvind rajendran , david maier venue international conference on management of data year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This annotated bibliography presents a collection of published papers, technical reports, Master's and PhD Theses that have investigated various aspects of object database performance.  Entity 2: title an annotated bibliography on active databases ( short version ) authors ulrike jaeger , johann christoph freytag venue acm sigmod record year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The Eighth International Workshop on Knowledge Representation Meets Databases (KRDB) was held at the Ponti cia Universit a Urbaniana, in Rome, right after VLDB 2001. KRDB was initiated in 1994 to provide an opportunity for researchers and practitioners from the two areas to exchange ideas and results. This year's focus was on Modeling, Querying andManaging Semistructured Data. The one day program included ten research papers, one invited talk, and a panel. Eight of the accepted papers addressed various topics related to representation of information and reasoning in XML, one was on data integration and one on transaction processing.  Entity 2: title intelligent access to heterogeneous information sources : report on the 4th workshop on knowledge representation meets databases authors franz baader , manfred a. jeusfeld , werner nutt venue acm sigmod record year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper considers this issue with respect to spatially distributed environmental models. A method of measuring the semantic proximity between components of large, integrated models is presented, along with an example illustrating its application. It is concluded that many of the issues associated with weak model semantics can be resolved with the addition of self-evaluating logic and context-based tools that present the semantic weaknesses to the end-user.  Entity 2: title optimization of dynamic query evaluation plans authors richard l. cole , goetz graefe venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper, we present two algorithms for deriving optimal and near-optimal vertical class partitioning schemes. The cost-driven algorithm provides the optimal vertical class partitioning schemes by enumerating, exhaustively, all the schemes and calculating the number of disk accesses required to execute a given set of applications. For this, a cost model for executing a set of methods in an OODB system is developed. Since exhaustive enumeration is costly and only works for classes with a small number of instance variables, a hill-climbing heuristic algorithm (HCHA) is developed, which takes the solution provided by the affinity-based algorithm and improves it, thereby further reducing the total number of disk accesses incurred.  Entity 2: title a logical foundation for deductive object-oriented databases authors mengchi liu , gillian dobbie , tok wang ling venue acm transactions on database systems ( tods ) year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This second Long Range Planning special issue on PLS-SEMin strategic management research and practice seeks to further progress towards this goal. The journal received 41 articles for its special issue on PLS-SEM, twelve of which completed a thorough review process successfully. Based on the number of high quality manuscripts, a decision was made to split the special issue. In the first Long Range Planning special issue on PLS-SEM in strategic management (Hair et al., 2012a; Robins, 2012), the focus was on methodological developments and their application (Becker et al., 2012; Furrer et al., 2012; Gudergan et al., 2012; Hair et al., 2012a,b,c; Money et al., 2012; Rigdon, 2012).  Entity 2: title guest editorial authors matthias jarke venue the vldb journal -- the international journal on very large data bases year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The chair of ACRP\u2019s Association Board of Trustees recounts how he left manufacturing and research and development a decade ago to take a position as a clinical research administrator. It was a move that place him into a role he knew little about, having not been engaged in clinical research beforehand. If this sounds like a familiar experience to others, the lessons shared in this column highlight the importance of the individual\u2019s ongoing will to learn, and of organizational support for that learning.  Entity 2: title treasurer 's message authors joachim hammer venue acm sigmod record year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We describe the implementation of the magic-sets transformation in the Starburst extensible relational database system. To our knowledge this is the first implementation of the magic-sets transformation in a relational database system. The Starburst implementation has many novel features that make our implementation especially interesting to database practitioners (in addition to database researchers).  Entity 2: title a general technique for querying xml documents using a relational database system authors jayavel shanmugasundaram , eugene shekita , jerry kiernan , rajasekar krishnamurthy , efstratios viglas , jeffrey naughton , igor tatarinov venue acm sigmod record year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The goal of STRUDEL project is to extend and adapt these concepts to the problem of Web-site management. Consider several tasks required of a Web-site manager. Site managers often want to manage a single repository of site data, but present different browsable \u201cviews\u201d of the site based on criteria such as the type of user accessing the site, e.g., external or internal, expert or novice. Morever, a manager might want to modify the data repository by editing simple text files or by updating external databases, to reorganize the structure of the pages by manipulating graphs that represent the linked pages, or to design multiple presentations of a single page by editing HTML files or by using a WYSIWYG HTML generator.  Entity 2: title the ores temporal database management system authors babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Large organizations need to exchange information among many separately developed systems. In order for this exchange to be useful, the individual systems must agree on the meaning of their exchanged data. That is, the organization must ensure semantic interoperability. This paper provides a theory of semantic values as a unit of exchange that facilitates semantic interoperability betweeen heterogeneous information systems. We show how semantic values can either be stored explicitly or be defined by environments. A system architecture is presented that allows autonomous components to share semantic values. The key component in this architecture is called the context mediator, whose job is to identify and construct the semantic values being sent, to determine when the exchange is meaningful, and to convert the semantic values to the form required by the receiver. Our theory is then applied to the relational model. We provide an interpretation of standard SQL queries in which context conversions and manipulations are transparent to the user. We also introduce an extension of SQL, called Context-SQL (C-SQL), in which the context of a semantic value can be explicitly accessed and updated. Finally, we describe the implementation of a prototype context mediator for a relational C-SQL system.  Entity 2: title quality-driven integration of heterogenous information systems authors felix naumann , ulf leser , johann christoph freytag venue very large data bases year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: OdeFS is a file-like interface to the Ode objectoriented database. OdeFS allows database objects to be accessed and manipulated with standard commands, just like files in a traditional file system. No recompilation is required, so proprietary applications can access Ode objects. OdeFS is implemented as a network file server, using the NFS protocol. This paper describes OdeFS and its implementation.  Entity 2: title a database interface for file update authors serge abiteboul , sophie cluet , tova milo venue international conference on management of data year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Mining for associations between items in large transactional databases is a central problem in the field of knowledge discovery. When the database is partitioned among several share-nothing machines, the problem can be addressed using distributed data mining algorithms. One such algorithm, called CD, was proposed by Agrawal and Shafer in [1] and was later enhanced by the FDM algorithm of Cheung, Han et al. [5]. The main problem with these algorithms is that they do not scale well with the number of partitions. They are thus impractical for use in modern distributed environments such as peer-to-peer systems, in which hundreds or thousands of computers may interact. In this paper we present a set of new algorithms that solve the Distributed Association Rule Mining problem using far less communication. In addition to being very efficient, the new algorithms are also extremely robust. Unlike existing algorithms, they continue to be efficient even when the data is skewed or the partition sizes are imbalanced. We present both experimental and theoretical results concerning the behavior of these algorithms and explain how they can be implemented in different settings.  Entity 2: title data mining with optimized two-dimensional association rules authors takeshi fukuda , yasuhiko morimoto , shimichi morishita , takeshi tokuyama venue acm transactions on database systems ( tods ) year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper reports on experience obtained during the design, implementation and use of a multi-paradigm query interface to an object-oriented database. The specific system which has been developed allows equivalent data retrieval tasks to be expressed using textual, form-based and graph-based notations, and supports automatic translation of queries between these three paradigms. The motivation behind the development of such an interface is presented, as is the software architecture which supports the multi-paradigm functionality.  Entity 2: title a user-centered interface for querying distributed multimedia databases authors isabel f. cruz , kimberly m. james venue international conference on management of data year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: A range query applies an aggregation operation over all selected cells of an OLAP data cube where the selection is specified by providing ranges of values for numeric dimensions. We present fast algorithms for range queries for two types of aggregation operations: SUM and MAX. These two operations cover techniques required for most popular aggregation operations, such as those supported by SQL.  Entity 2: title temporal queries in olap authors alberto o. mendelzon , alejandro a. vaisman venue very large data bases year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Object-Relational DBMSs have been receiving a great deal of attention from industry analysts and press as the next generation of database management systems. The motivation for a next generation DBMS is driven by the reality of shortened business cycles. This dynamic environment demands fast, cost-effective, time-to-market of new or modified business processes, services, and products. To support this important business need, the next generation DBMS must: 1. leverage the large investments made in existing relational technology, both in data and skill set; 2. Take advantage of the flexibility, productivity, and performance benefits of OO modeling; and 3. Integrate robust DBMS services for production quality systems. The objective of this article is to provide a brief overview of UniSQL's commercial object-relational database management system.  Entity 2: title the mariposa distributed database management system authors jeff sidell venue acm sigmod record year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Some significant progress related to multidimensional data analysis has been achieved in the past few years, including the design of fast algorithms for computing datacubes, selecting some precomputed group-bys to materialize, and designing efficient storage structures for multidimensional data. However, little work has been carried out on multidimensional query optimization issues. Particularly the response time (or evaluation cost) for answering several related dimensional queries simultaneously is crucial to the OLAP applications. Recently, Zhao et al. first exploited this problem by presenting three heuristic algorithms. In this paper we first consider in detail two cases of the problem in which all the queries are either hash-based star joins or index-based star joins only. In the case of the hash-based star join, we devise a polynomial approximation algorithm which delivers a plan whose evaluation cost is $ O(n^{\\epsilon }$) times the optimal, where n is the number of queries and $ \\epsilon $ is a fixed constant with $0<\\epsilon \\leq 1$. We also present an exponential algorithm which delivers a plan with the optimal evaluation cost. In the case of the index-based star join, we present a heuristic algorithm which delivers a plan whose evaluation cost is n times the optimal, and an exponential algorithm which delivers a plan with the optimal evaluation cost. We then consider a general case in which both hash-based star-join and index-based star-join queries are included. For this case, we give a possible improvement on the work of Zhao et al., based on an analysis of their solutions. We also develop another heuristic and an exact algorithm for the problem. We finally conduct a performance study by implementing our algorithms. The experimental results demonstrate that the solutions delivered for the restricted cases are always within two times of the optimal, which confirms our theoretical upper bounds. Actually these experiments produce much better results than our theoretical estimates. To the best of our knowledge, this is the only development of polynomial algorithms for the first two cases which are able to deliver plans with deterministic performance guarantees in terms of the qualities of the plans generated. The previous approaches including that of [ZDNS98] may generate a feasible plan for the problem in these two cases, but they do not provide any performance guarantee, i.e., the plans generated by their algorithms can be arbitrarily far from the optimal one.  Entity 2: title optimizing queries across diverse data sources authors laura m. haas , donald kossmann , edward l. wimmers , jun yang venue very large data bases year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Database systems for real-time applications must satisfy timing constraints associated with transactions, while maintaining data consistency. In addition to real-time requirements, security is usually required in many applications. Multilevel security requirements introduce a new dimension to transaction processing in real-time database systems. In this paper, we argue that because of the complexities involved, trade-offs need to be made between security and timeliness. We briefly present the secure two-phase locking protocol and discuss an adaptive method to support trading off security for timeliness, depending on the current state of the system. The performance of the adaptive secure two-phase locking protocol shows improved timeliness. We also discuss future research direction to improve timeliness of secure database systems.  Entity 2: title an annotated bibliography on real-time database systems authors &#214; zg &#252; r ulusoy venue acm sigmod record year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: An information retrieval (IR) engine can rank documents based on textual proximity of keywords within each document. In this paper we apply this notion to search across an entire database for objects that are \"near\" other relevant objects. Proximity search enables simple \"focusing\" queries based on general relationships among objects, helpful for interactive query sessions. We view the database as a graph, with data in vertices (objects) and relationships indicated by edges. Proximity is defined based on shortest paths between objects. We have implemented a prototype search engine that uses this model to enable keyword searches over databases, and we have found it very effective for quickly finding relevant information. Computing the distance between objects in a graph stored on disk can be very expensive. Hence, we show how to build compact indexes that allow us to quickly find the distance between objects at search time. Experiments show that our algorithms are effcient and scale well.  Entity 2: title efficient geometry-based similarity search of 3d spatial databases authors daniel a. keim venue international conference on management of data year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We propose a new client-side data-caching scheme for relational databases with a central server and multiple clients. Data are loaded into each client cache based on queries executed on the central database at the server. These queries are used to form predicates that describe the cache contents. A subsequent query at the client may be satisfied in its local cache if we can determine that the query result is entirely contained in the cache. This issue is called  cache completeness . A separate issue,  cache currency , deals with the effect on client caches of updates committed at the central database. We examine the various performance tradeoffs and optimization issues involved in addressing the questions of cache currency and completeness using predicate descriptions and suggest solutions that promote good dynamic behavior. Lower query-response times, reduced message traffic, higher server throughput, and better scalability are some of the expected benefits of our approach over commonly used relational server-side and object ID-based or page-based client-side caching.  Entity 2: title highly concurrent cache consistency for indices in client-server database systems authors markos zaharioudakis , michael j. carey venue international conference on management of data year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Finding approximate answers to multi-dimensional range queries over real valued attributes has significant applications in data exploration and database query optimization. In this paper we consider the following problem: given a table of d attributes whose domain is the real numbers, and a query that specifies a range in each dimension, find a good approximation of the number of records in the table that satisfy the query.  Entity 2: title on the computation of multidimensional aggregates authors sameet agarwal , rakesh agrawal , prasad deshpande , ashish gupta , jeffrey f. naughton , raghu ramakrishnan , sunita sarawagi venue very large data bases year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Satisfiability, implication, and equivalence problems involving conjunctive inequalities are important and widely encountered database problems that need to be efficiently and effectively processed. In this article we consider two popular types of arithmetic inequalities, (XopY) and (X op C), where X and Y are attributes, C is a constant of the domain or X, and op \u2208{<, \u2264, =, \u2260, >, \u2265). These inequalities are most frequently used in a database system, inasmuch as the former type of inequality represents a 0-join, and the latter is a selection. We study the satisfiability and implication problems under the integer domain and the real domain, as well as under two different operator sets ({<, \u2264, =, \u2265, >} and {<, \u2264, =, \u2260, \u2265, >}). Our results show that solutions under different domains and/or different operator sets are quite different. Out of these eight cases, excluding two cases that had been shown to be NP-hard, we either report the first necessary and sufficient conditions for these problems as well as their efficient algorithms with complexity analysis (for four cases), or provide an improved algorithm (for two cases). These iff conditions and algorithms are essential to database designers, practitioners, and researchers. These algorithms have been implemented and an experimental study comparing the proposed algorithms and those previously known is conducted. Our experiments show that the proposed algorithms are more efficient than previously known algorithms even for small input. The C++ code can be obtained by an anonymous ftp from <archive.fiu.edu>.  Entity 2: title secure transaction processing in firm real-time database systems authors binto george , jayant haritsa venue international conference on management of data year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.  Entity 2: title integrating heterogenous overlapping databases through object-oriented transformations authors vanja josifovski , tore risch venue very large data bases year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Our results show that the policies are effective at achieving user-specified levels of I/O operations and database garbage percentage. We also investigate the sensitivity of the policies over a range of object connectivities. The evaluation demonstrates that semi-automatic, self-adaptive policies are a practical means for flexibly controlling garbage collection rate.  Entity 2: title partition selection policies in object database garbage collection authors jonathan e. cook , alexander l. wolf , benjamin g. zorn venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Relational databases provide the ability to store user-defined functions and predicates which can be invoked in SQL queries. When evaluation of a user-defined predicate is relatively expensive, the traditional method of evaluating predicates as early as possible is no longer a sound heuristic. There are two previous approaches for optimizing such queries. However, neither is able to guarantee the optimal plan over the desired execution space. We present efficient techniques that are able to guarantee the choice of an optimal plan over the desired execution space. The optimization algorithm with complete rank-ordering improves upon the naive optimization algorithm by exploiting the nature of the cost formulas for join methods and is polynomial in the number of user-defined predicates (for a given number of relations.) We also propose pruning rules that significantly reduce the cost of searching the execution space for both the naive algorithm as well as for the optimization algorithm with complete rank-ordering, without compromising optimality. We also propose a conservative local heuristic that is simpler and has low optimization overhead. Although it is not always guaranteed to find the optimal plans, it produces close to optimal plans in most cases.  Entity 2: title optimization techniques for queries with expensive methods authors joseph m. hellerstein venue acm transactions on database systems ( tods ) year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This dissertation describes techniques for speeding up Online Analytical Processing or OLAP queries. OLAP systems allow users to quickly obtain the answers to complex business queries. Quickly answering these queries which aggregate large amounts of data, calls for various specialized techniques. One technique used by OLAP systems to speed up multidimensional data analysis is to precompute aggregates on some subsets of dimensions and their corresponding hierarchies.  Entity 2: title materialized views and data warehouses authors nick roussopoulos venue acm sigmod record year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper, we propose a monitoring service that could be offered by such database servers, and present algorithms for its implementation. In contrast to published view maintenance algorithms, we do not assume that the server has access to the original materialization when computing differential view changes to be notified. We also do not assume any database capabilities on the client side and therefore compute precisely the required differentials rather than just an approximation, as is done by cache coherence techniques in homogeneous clientserver databases.  Entity 2: title efficient maintenance of materialized mediated views authors james j. lu , guido moerkotte , joachim schue , v. s. subrahmanian venue international conference on management of data year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Abstract. Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today's decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing algorithms that operate directly on the wavelet-coefficient synopses of relational tables, allowing us to process arbitrarily complex queries entirely in the wavelet-coefficient domain. This guarantees extremely fast response times since our approximate query execution engine can do the bulk of its processing over compact sets of wavelet coefficients, essentially postponing the expansion into relational tuples until the end-result of the query. We also propose a novel wavelet decomposition algorithm that can build these synopses in an I/O-efficient manner. Finally, we conduct an extensive experimental study with synthetic as well as real-life data sets to determine the effectiveness of our wavelet-based approach compared to sampling and histograms. Our results demonstrate that our techniques: (1) provide approximate answers of better quality than either sampling or histograms; (2) offer query execution-time speedups of more than two orders of magnitude; and (3) guarantee extremely fast synopsis construction times that scale linearly with the size of the data.  Entity 2: title a raster approximation for processing of spatial joins authors geraldo zimbrao , jano moreira de souza venue very large data bases year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The view selection problem is to choose a set of views to materialize over a database schema, such that the cost of evaluating a set of workload queries is minimized and such that the views fit into a prespecified storage constraint. The two main applications of the view selection problem are materializing views in a database to speed up query processing, and selecting views to materialize in a data warehouse to answer decision support queries. In addition, view selection is a core problem for intelligent data placement over a wide-area network for data integration applications and data management for ubiquitous computing. We describe several fundamental results concerning the view selection problem. We consider the problem for views and workloads that consist of equality-selection, project and join queries, and show that the complexity of the problem depends crucially on the quality of the estimates that a query optimizer has on the size of the views it is considering to materialize. When a query optimizer has good estimates of the sizes of the views, we show a somewhat surprising result, namely, that an optimal choice of views may involve a number of views that is exponential in the size of the database schema. On the other hand, when an optimizer uses standard estimation heuristics, we show that the number of necessary views and the expression size of each view are polynomially bounded.  Entity 2: title a formal perspective on the view selection problem authors rada chirkova , alon y. halevy , dan suciu venue very large data bases year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this article, we first describe the XFilter and YFilter approaches and present results of a detailed performance comparison of structure matching for these algorithms as well as a hybrid approach. The results show that the path sharing employed by YFilter can provide order-of-magnitude performance benefits. We then propose two alternative techniques for extending YFilter's shared structure matching with support for value-based predicates, and compare the performance of these two techniques. The results of this latter study demonstrate some key differences between shared XML filtering and traditional database query processing. Finally, we describe how the YFilter approach is extended to handle more complicated queries containing nested path expressions  Entity 2: title the design and performance evaluation of alternative xml storage strategies authors feng tian , david j. dewitt , jianjun chen , chun zhang venue acm sigmod record year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper presents structural recursion as the basis of the syntax and semantics of query languages for semistructured data and XML. We describe a simple and powerful query language based on pattern matching and show that it can be expressed using structural recursion, which is introduced as a top-down, recursive function, similar to the way XSL is defined on XML trees. On cyclic data, structural recursion can be defined in two equivalent ways: as a recursive function which evaluates the data top-down and remembers all its calls to avoid infinite loops, or as a bulk evaluation which processes the entire data in parallel using only traditional relational algebra operators. The latter makes it possible for optimization techniques in relational queries to be applied to structural recursion. We show that the composition of two structural recursion queries can be expressed as a single such query, and this is used as the basis of an optimization method for mediator systems. Several other formal properties are established: structural recursion can be expressed in first-order logic extended with transitive closure; its data complexity is PTIME; and over relational data it is a conservative extension of the relational calculus. The underlying data model is based on value equality, formally defined with bisimulation. Structural recursion is shown to be invariant with respect to value equality.  Entity 2: title distributed query evaluation on semistructured data authors dan suciu venue acm transactions on database systems ( tods ) year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: A radar transmitter apparatus comprising a radar transmitter equipped with a modulator arranged in an oil-filled housing, the modulator being held in spaced relationship with respect to the inner walls of the housing in order to form an intermediate space for the convection flow of the oil. The housing is substantially trough or vat-shaped and covered by a trough or vat-shaped cover member. In the internal chamber or space between the cover member and the modulator, which internal space is wetted by the oil, there is arranged, on the one hand, a magnetron attached at the cover member and, on the other hand, a thyratron which is mounted directly below an opening at the cover member. This opening is closable by means of oil sealed throughpassage means.  Entity 2: title type classification of semi-structured documents authors markus tresch , neal palmer , allen luniewski venue very large data bases year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In many application fields, such as production lines or stock analysis, it is substantial to create and process high amounts of data at high rates. Such continuous data flows with unknown size and end are also called data streams. The processing and analysis of data streams are a challenge for common data management systems as they have to operate and deliver results in real time. Data Stream Management Systems (DSMS), as an advancement of database management systems, have been implemented to deal with these issues. DSMS have to adapt to the notion of data streams on various levels, such as query languages, processing or optimization. In this chapter we give an overview of the basics of data streams, architecture principles of DSMS and the used query languages. Furthermore, we specifically detail data quality aspects in DSMS as these play an important role for various applications based on data streams. Finally, the chapter also includes a list of research and commercial DSMS and their key properties.  Entity 2: title data management issues in electronic commerce authors asuman dogac venue acm sigmod record year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Like any other data, biological data is a very vast one. Due to emergence of system biology it is necessary to develop various platforms and techniques to analyze and organize the biological data in meaning full manner for which it to be mined and processed carefully. As the complexity associated with biological data is high ,it has to be studied considering various criteria\u2019s and also it is mandatory to study all available databases and then has to undergo several processing mining techniques to finally put in a format which is easy to assess and produce the information of interest. There are various techniques and method for mining biological data. Here we will put forth all possible techniques and operations involved in data mining and will compare them in order to find the advantages and disadvantages of different methods  Entity 2: title data analysis and mining in the life sciences authors nam huyn venue acm sigmod record year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: XML is an emerging standard for data representation and exchange on the World-Wide Web. Due to the nature of information on the Web and the inherent flexibility of XML, we expect that much of the data encoded in XML will be semistructured: the data may be irregular or incomplete, and its structure may change rapidly or unpredictably. This paper describes the query processor of Lore, a DBMS for XML-based data supporting an expressive query language. We focus primarily on Lore's cost-based query optimizer. While all of the usual problems associated with cost-based query optimization apply to XML-based query languages, a number of additional problems arise, such as new kinds of indexing, more complicated notions of database statistics, and vastly different query execution strategies for different databases. We define appropriate logical and physical query plans, database statistics, and a cost model, and we describe plan enumeration including heuristics for reducing the large search space. Our optimizer is fully implemented in Lore and preliminary performance results are reported. This is a short version of the paper Query Optimization for Semistructured Data which is available at: http://www-db.stanford.edu/~mchughj/publications/qo.ps  Entity 2: title a query language and optimization techniques for unstructured data authors peter buneman , susan davidson , gerd hillebrand , dan suciu venue international conference on management of data year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: METU Object-Oriented DBMS1 includes the implementation of a database kernel, an object-oriented SQL-like language and a graphical user interface. Kernel functions are divided between a SQL Interpreter and a C++ compiler. Thus the interpretation of functions are avoided increasing the efficiency of the system. The compiled by C++ functions are used by the system through the Function Manager. The system is realized on Exodus Storage Manager (ESM), thus exploiting some of the kernel functions readily provided by ESM. The additional functions provided by the MOOD kernel are the optimization and interpretation of SQL statements, dynamic linking of functions, and catalog management.  Entity 2: title a cost model for clustered object-oriented databases authors georges gardarin , jean-robert gruser , zhao-hui tang venue very large data bases year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In the last few years, many active database models have been proposed. Some of them have been implemented as research prototypes. The use and study of these prototypes shows that it is difficult to get a clear idea of the proposed approaches and to compare them. More generally there are some unquestionable difficulties in understanding, reasoning about and teaching behavior of active database systems. We think there is a need for formal descriptions of the semantics of such systems in order to describe and to understand them with less ambiguities, to compare them and to come up with some progress in defining standard concepts and functionalities for active databases.  Entity 2: title the aditi deductive database system authors jayen vaghani , kotagiri ramamohanarao , david b. kemp , zoltan somogyi , peter j. stuckey , tim s. leask , james harland venue the vldb journal -- the international journal on very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: e consider the execution of multi-join queries in a hierarchical parallel system, i.e., a shared-nothing system whose nodes are shared-memory multiprocessors. In this context, load balancing must be addressed at two levels, locally among the processors of each shared-memory node and globally among all nodes. In this paper, we propose a dynamic execution model that maximizes local load balancing within shared-memory nodes and minimizes the need for load sharing across nodes. This is obtained by allowing each processor to execute any operator that can be processed locally, thereby taking full advantage of inter- and intra-operator parallelism. We conducted a performance evaluation using an implementation on a 72-processor KSR1 computer.  Entity 2: title data partitioning and load balancing in parallel disk systems authors peter scheuermann , gerhard weikum , peter zabback venue the vldb journal -- the international journal on very large data bases year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Abstract.Due to their expressive power, regular expressions (REs) are quickly becoming an integral part of language specifications for several important application scenarios. Many of these applications have to manage huge databases of RE specifications and need to provide an effective matching mechanism that, given an input string, quickly identifies the REs in the database that match it. In this paper, we propose the RE-tree, a novel index structure for large databases of RE specifications. Given an input query string, the RE-tree speeds up the retrieval of matching REs by focusing the search and comparing the input string with only a small fraction of REs in the database. Even though the RE-tree is similar in spirit to other tree-based structures that have been proposed for indexing multidimensional data, RE indexing is significantly more challenging since REs typically represent infinite sets of strings with no well-defined notion of spatial locality. To address these new challenges, our RE-tree index structure relies on novel measures for comparing the relative sizes of infinite regular languages. We also propose innovative solutions for the various RE-tree operations including the effective splitting of RE-tree nodes and computing a \"tight\" bounding RE for a collection of REs. Finally, we demonstrate how sampling-based approximation algorithms can be used to significantly speed up the performance of RE-tree operations. Preliminary experimental results with moderately large synthetic data sets indicate that the RE-tree is effective in pruning the search space and easily outperforms naive sequential search approaches.  Entity 2: title the tv-tree : an index structure for high-dimensional data authors king ip lin , h. v. jagadish , christos faloutsos venue the vldb journal -- the international journal on very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: New telecommunication services and mobility networks have introduced databases in telecommunication networks. Compared with traditional use of databases, telecom databases must fulfill very tough requirements on response time, throughput, and availability. ClustRa is a telecom database prototype developed to run on standard workstations interconnected by an ATM switch. To meet the throughput and real-time response requirements, ClustRa is a main memory database with neighbor main, memory logging. Transactions are executed in parallel. To meet the availability requirements, we use a 2-safe replication scheme over two sites with independent failure modes, a novel declustering strategy, early detection of failures with fast takeover, and by on-line self-repair and maintenance. This paper gives an overview of ClustRa and includes a set of performance measurements.  Entity 2: title a scalable architecture for autonomous heterogeneous database interactions authors steven milliner , athman bouguettaya , mike p. papazoglou venue very large data bases year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Abstract. With respect to the specific requirements of advanced OODB applications, index data structures for type hierarchies in OODBMS have to provide efficient support for multiattribute queries and have to allow index optimization for a particular query profile. We describe the multikey type index and an efficient implementation of this indexing scheme. It meets both requirements: in addition to its multiattribute query capabilities it is designed as a mediator between two standard design alternatives, key-grouping and type-grouping. A prerequisite for the multikey type index is a linearization algorithm which maps type hierarchies to linearly ordered attribute domains in such a way that each subhierarchy is represented by an interval of this domain. The algorithm extends previous results with respect to multiple inheritance. The subsequent evaluation of our proposal focuses on storage space overhead as well as on the number of disk I/O operations needed for query execution. The analytical results for the multikey type index are compared to previously published figures for well-known single-key search structures. The comparison clearly shows the superiority of the multikey type index for a large class of query profiles.  Entity 2: title evaluating functional joins along nested reference sets in object-relational and object-oriented databases authors reinhard braumandl , jens clau &#223; en , alfons kemper venue very large data bases year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Many applications require the management of spatial data. Clustering large spatial databases is an important problem which tries to find the densely populated regions in the feature space to be used in data mining, knowledge discovery, or efficient information retrieval. A good clustering approach should be efficient and detect clusters of arbitrary shape. It must be insensitive to the outliers (noise) and the order of input data. We pro-pose WaveCluster, a novel clustering approach based on wavelet transforms, which satisfies all the above requirements. Using multi-resolution property of wavelet transforms, we can effectively identify arbitrary shape clus-ters at different degrees of accuracy. We also demonstrate that WaveCluster is highly effi-cient in terms of time complexity. Experi-mental results on very large data sets are pre-sented which show the efficiency and effective-ness of the proposed approach compared to the other recent clustering methods  Entity 2: title cure : an efficient clustering algorithm for large databases authors sudipto guha , rajeev rastogi , kyuseok shim venue international conference on management of data year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The MOMIS project (Mediator envirOnment for Multiple Information Sources) developed in the past years allows the integration of data from structured and semi-structured data sources. SI-Designer (Source Integrator Designer) is a designer support tool implemented within the MOMIS project for semi-automatic integration of heterogeneous sources schemata. It is a java application where all modules involved are available as CORBA Object and interact using established IDL interfaces. The goal of this demonstration is to present a new tool: SI-Web (Source Integrator on Web), it offers the same features of SI-Designer but it has got the great advantage of being usable on Internet through a web browser.  Entity 2: title securing xml documents : the author-x project demonstration authors elisa bertino , silvana castano , elena ferrari venue international conference on management of data year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: As XML is emerging as the data format of the internet era, there is an substantial increase of the amount of data in XML format. To better describe such XML data structures and constraints, several XML schema languages have been proposed. In this paper, we present a comparative analysis of six noteworthy XML schema languages.  Entity 2: title comparative analysis of five xml query languages authors angela bonifati , stefano ceri venue acm sigmod record year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We briefly outline the main characteristics of an efficient server-based algorithm for garbage collecting object-oriented databases in a client-server environment. The algorithm is incremental and runs concurrently with client transactions. Unlike previous algorithms, it does not hold any locks on data and does not require callbacks to clients. It is fault tolerant, but performs very little logging. The algorithm has been designed to be integrated into existing OODB systems, and therefore it works with standard implementation techniques such as two-phase locking and write-ahead-logging. In addition, it supports client-server performance optimizations such as client caching and flexible management of client buffers. The algorithm has been implemented in the EXODUS storage manager before being evaluated.  Entity 2: title semi-automatic , self-adaptive control of garbage collection rates in object databases authors jonathan e. cook , artur w. klauser , alexander l. wolf , benjamin g. zorn venue international conference on management of data year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: OdeFS is a file-like interface to the Ode objectoriented database. OdeFS allows database objects to be accessed and manipulated with standard commands, just like files in a traditional file system. No recompilation is required, so proprietary applications can access Ode objects. OdeFS is implemented as a network file server, using the NFS protocol. This paper describes OdeFS and its implementation.  Entity 2: title repositories and object oriented databases authors philip a. bernstein venue acm sigmod record year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Data warehouses store materialized views over base data from external sources. Clients typically perform complex read-only queries on the views. The views are refreshed periodically by maintenance transactions, which propagate large batch updates from the base tables. In current warehousing systems, maintenance transactions usually are isolated from client read activity, limiting availability and/or size of the warehouse. We describe an algorithm called 2VNL that allows warehouse maintenance transactions to run concurrently with readers. By logically maintaining two versions of the database, no locking is required and serializability is guaranteed. We present our algorithm, explain its relationship to other multi-version concurrency control algorithms, and describe how it can be implemented on top of a conventional relational DBMS using a query rewrite approach.  Entity 2: title view maintenance in a warehousing environment authors yue zhuge , h &#233; ctor garc &#237; a-molina , joachim hammer , jennifer widom venue international conference on management of data year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Although research on temporal database systems has been active for about 20 years, implementations have not appeared until recently. This is one reason why current commercial database systems provide only limited temporal functionality. This paper summarizes extant state of the art of temporal database implementations. Rather than being very specific about each system we have attempted to provide an indication of the functionality together with pointers to additional information. It is hoped that this leads to more efforts pushing the implementation of temporal database systems in the near future.  Entity 2: title temporal statement modifiers authors michael h. b &#246; hlen , christian s. jensen , richard thomas snodgrass venue acm transactions on database systems ( tods ) year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: e consider the execution of multi-join queries in a hierarchical parallel system, i.e., a shared-nothing system whose nodes are shared-memory multiprocessors. In this context, load balancing must be addressed at two levels, locally among the processors of each shared-memory node and globally among all nodes. In this paper, we propose a dynamic execution model that maximizes local load balancing within shared-memory nodes and minimizes the need for load sharing across nodes. This is obtained by allowing each processor to execute any operator that can be processed locally, thereby taking full advantage of inter- and intra-operator parallelism. We conducted a performance evaluation using an implementation on a 72-processor KSR1 computer.  Entity 2: title dynamic load balancing for parallel association rule mining on heterogenous pc cluster systems authors masahisa tamura , masaru kitsuregawa venue very large data bases year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We consider the problem of processing top-N queries in a distributed environment with possibly uncooperative local database systems. For a given top-N query, the problem is to find the N tuples that satisfy the query the best but not necessarily completely in an efficient manner. Top-N queries are gaining popularity in relational databases and are expected to be very useful for e-commerce applications. Many companies provide the same type of goods and services to the public on the Web, and relational databases may be employed to manage the data. It is not feasible for a user to query a large number of databases. It is therefore desirable to provide a facility where a user query is accepted at some site, suitable tuples from appropriate sites are retrieved and the results are merged and then presented to the user. In this paper, we present a method for constructing the desired facility. Our method consists of two steps. The first step determines which databases are likely to contain the desired tuples for a given query so that the databases can be ranked based on their desirability with respect to the query. Four different techniques are introduced for this step with one requiring no cooperation from local systems. The second step determines how the ranked databases should be searched and what tuples from the searched databases should be returned. A new algorithm is proposed for this purpose. Experimental results are presented to compare different methods and very promising results are obtained using the method that requires no cooperation from local databases.  Entity 2: title distributed processing over stand-alone systems and applications authors gustavo alonso , claus hagen , hans-j &#246; rg schek , markus tresch venue very large data bases year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The web is highly dynamic in both the content and quantity of the information that it encompasses. In order to fully exploit its enormous potential as a global repository of information, we need to understand how its size, topology, and content are evolving. This then allows the development of new techniques for locating and retrieving information that are better able to adapt and scale to its change and growth. The web's users are highly diverse and can access the it from a variety of devices and interfaces, at different places and times, and for varying purposes. Thus, new techniques are being developed for personalising the presentation and content of web-based information depending on how it is being accessed and on the individual user's requirements and preferences.  Entity 2: title report on the 1995 international workshop on temporal databases authors arie segev , christian s. jensen , richard thomas snodgrass venue acm sigmod record year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Providing content-based video query, retrieval and browsing is the most important goal of a video database management system (VDBMS). Video data is unique not only in terms of its spatial and temporal characteristics, but also in the semantic associations manifested by the entities present in the video. This paper introduces a novel video data model called Logical Hypervideo Data Model. In addition to multilevel video abstractions, the model is capable of representing video entities that users are interested in (defined as hot objects) and their semantic associations with other logical video abstractions, including hot objects themselves.  Entity 2: title supporting periodic authorizations and temporal reasoning in database access control authors elisa bertino , claudio bettini , elena ferrari , pierangela samarati venue very large data bases year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The Grid is an emerging infrastructure for providing coordinated and consistent access to distributed, heterogeneous computational and information storage resources amongst autonomous organizations.    Data grids are being built across the world as the next generation data handling systems for sharing access to data and storage systems within multiple administrative domains. A data grid provides logical name spaces for digital entities and storage resources to create global identifiers that are location independent. Data grid systems provide services on the logical name space for the manipulation, management, and organization of digital entities.    Databases are increasingly being used within Grid applications for data and metadata management, and several groups are now developing services for the access and integration of structured data on the Grid. The service-based approach to making data available on the Grid is being encouraged by the adoption of the Open Grid Services Architecture (OGSA), which is bringing about the integration of the Grid with Web Service technologies.    The tutorial will introduce the Grid, and examine the requirements, issues and possible solutions for integrating data into the Grid. It will take examples from current systems, in particular the SDSC Storage Resource Broker and the OGSA-Database Access and Integration project.  Entity 2: title database management systems and the internet authors susan malaika venue very large data bases year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We present techniques for computing small space representations of massive data streams. These are inspired by traditional wavelet-based approximations that consist of specific linear projections of the underlying data. We present general \u201csketch\u201d based methods for capturing various linear projections of the data and use them to provide pointwise and rangesum estimation of data streams. These methods use small amounts of space and per-item time while streaming through the data, and provide accurate representation as our experiments with real data streams show.  Entity 2: title a robust , optimization-based approach for approximate answering of aggregate queries authors surajit chaudhuri , gautam das , vivek narasayya venue international conference on management of data year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Relational OLAP tools and other database applications generate sequences of SQL statements that are sent to the database server as result of a single information request provided by a user. Unfortunately, these sequences cannot be processed efficiently by current database systems because they typically optimize and process each statement in isolation. We propose a practical approach for this optimization problem, called \"coarse-grained optimization,\" complementing the conventional query optimization phase. This new approach exploits the fact that statements of a sequence are correlated since they belong to the same information request. A lightweight heuristic optimizer modifies a given statement sequence using a small set of rewrite rules.  Entity 2: title fundamental techniques for order optimization authors david simmen , eugene shekita , timothy malkemus venue international conference on management of data year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.  Entity 2: title garbage collection in object oriented databases using transactional cyclic reference counting authors srinivas ashwin , prasan roy , s. seshadri , abraham silberschatz , s. sudarshan venue very large data bases year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The emergence of Big Data has amounted to the complexity of the discussion on data reuse. The benefits of Big Data lie in the possibilities to discover novel trends, patterns and relationships by combining very large amounts of data from different sources. Current personal data protection requirements like data minimization and purpose specification are potentially inimical to Big Data as they limit the size and use of Big Data. Substantial loss of economic and social benefits of Big Data may be the result. In order to avoid this, the reuse of data could be encouraged. Data reuse, when done properly, may be both privacy preserving and economically and socially beneficial. In this paper, we provide a taxonomy of data reuse from both the data controller\u2019s and the data subject\u2019s perspective that may be useful to determine the extent to which data reuse should be allowed and under which conditions. From the data controller\u2019s perspective we distinguish data recycling, data repurposing and data recontextualisation. From the data subject\u2019s perspective, we distinguish data sharing and data portability. It is argued that forms of data reuse that stay close to the awareness and intentions of data subjects should be approached less tight (for instance, by assuming informed consent), whereas forms of data reuse that are \u2018at a distance\u2019, i.e., in which awareness and transparency may be lacking and data subject\u2019s rights may prove more difficult to exercise, more restrictions and additional protection should be considered (for instance, by requiring explicit consent).  Entity 2: title data modelling in the large authors martin bertram venue acm sigmod record year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Internet, Web and distributed computing infrastructures continue to gain in popularity as a means of communication for organizations, groups and individuals alike. In such an environment, characterized by large distributed, autonomous, diverse, and dynamic information sources, access to relevant and accurate information is becoming increasingly complex. This complexity is exacerbated by the evolving system, semantic and structural heterogeneity of these potentially global, cross-disciplinary, multicultural and rich-media technologies. Clearly, solutions to these challenges require addressing directly a variety of interoperability issues.  Entity 2: title optimizing disjunctive queries with expensive predicates authors a. kemper , g. moerkotte , k. peithner , m. steinbrunn venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Semi-structured documents (e.g. journal art,icles, electronic mail, television programs, mail order catalogs, . ..) a.re often not explicitly typed; the only available t,ype information is the implicit structure. An explicit t,ype, however, is needed in order to a.pply objectoriented technology, like type-specific methods. In this paper, we present a.n experimental vector space cla.ssifier for determining the type of semi-structured documents. Our goal was to design a. high-performa.nce classifier in t,erms of accuracy (recall and precision), speed, and extensibility.  Entity 2: title semantic integration of semistructured and structured data sources authors s. bergamaschi , s. castano , m. vincini venue acm sigmod record year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper, we show how probabilistic graphical models can be effectively used for this task as an accurate and compact approximation of the joint frequency distribution of multiple attributes across multiple relations. Probabilistic Relational Models (PRMs) are a recent development that extends graphical statistical models such as Bayesian Networks to relational domains. They represent the statistical dependencies between attributes within a table, and between attributes across foreign-key joins. We provide an efficient algorithm for constructing a PRM front a database, and show how a PRM can be used to compute selectivity estimates for a broad class of queries. One of the major contributions of this work is a unified framework for the estimation of queries involving both select and foreign-key join operations.  Entity 2: title one-dimensional and multi-dimensional substring selectivity estimation authors h. v. jagadish , olga kapitskaia , raymond t. ng , divesh srivastava venue the vldb journal -- the international journal on very large data bases year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Several techniques that compute the join between two spatial datasets have been proposed during the last decade. Among these methods, some consider existing indices for the joined inputs, while others treat datasets with no index, providing solutions for the case where at least one input comes as an intermediate result of another database operator. In this paper we analyze previous work on spatial joins and propose a novel algorithm, called slot index spatial join (SISJ), that efficiently computes the spatial join between two inputs, only one of which is indexed by an R-tree. Going one step further, we show how SISJ and other spatial join algorithms can be implemented as operators in a database environment that joins more than two spatial datasets. We study the differences between relational and spatial multiway joins, and propose a dynamic programming algorithm that optimizes the execution of complex spatial queries.  Entity 2: title incremental distance join algorithms for spatial databases authors g &#237; sli r. hjaltason , hanan samet venue international conference on management of data year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Catalog management in websphere commerce suite. Share on. Author: Thomas Maguire. IBM, Hawthorne. IBM, Hawthorne. Search about this author. Authors Info & Affiliations. Publication: SIGMOD '01: Proceedings of the 2001 ACM SIGMOD international conference on Management of dataMay 2001 https://doi.org/10.1145/375663.375742. 0citation; 268 Downloads. Metrics. Total Citations0. Total Downloads268. Last 12 Months5. Last 6 weeks4. Get Citation Alerts New Citation Alert added! This alert has been successfully added  Entity 2: title data management issues in electronic commerce authors m. tamer &#214; zsu venue international conference on management of data year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper is a retrospective of the Stanford Information Filtering Service (SIFT), a system that as of April 1996 was processing over 40,000 worldwide subscriptions and over 80,000 daily documents. The paper describes some of the indexing mechanisms that were developed for SIFT, as well as the evaluations that were conducted to select a scheme to implement. It also describes the implementation of SIFT, and experimental results for the actual system. Finally, it also discusses and experimentally evaluates techniques for distributing a service such as SIFT for added performance and availability.  Entity 2: title challenges for global information systems authors alon y. levy , abraham silberschatz , divesh srivastava , maria zemankova venue very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Much of business XML data has accompanying XSD specifications. In many scenarios, \"shredding such XML data into a relational storage is a popular paradigm. Optimizing evaluation of XPath queries over such XML data requires paying careful attention to both the logical and physical designs of the relational database where XML data is shredded. None of the existing solutions has taken into account physical design of the generated relational database. In this paper, we study the interplay of logical and physical design and conclude that 1) solving them independently leads to suboptimal performance and 2) there is substantial overlap between logical and physical designs: some well-known logical design transformations generate the same mappings as physical design. Furthermore, existing search algorithms are inefficient to search the extremely large space of logical and physical design combinations. We propose a search algorithm that carefully avoids searching duplicated mappings and utilizes the workload information to further prune the search space. Experimental results confirm the effectiveness of our approach.  Entity 2: title querying xml views of relational data authors jayavel shanmugasundaram , jerry kiernan , eugene j. shekita , catalina fan , john funderburk venue very large data bases year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper we present an efficient algorithm for mining association rules that is fundamentally different from known algorithms. Compared to previous algorithms, our algorithm not only reduces the I/O overhead significantly but also has lower CPU overhead for most cases. We have performed extensive experiments and compared the performance of our algorithm with one of the best existing algorithms. It was found that for large databases, the CPU overhead was reduced by as much as a factor of four and I/O was reduced by almost an order of magnitude. Hence this algorithm is especially suitable for very large size databases.  Entity 2: title cmvf : a novel dimension reduction scheme for efficient indexing in a large image database authors jialie shen , anne h. h. ngu , john shepherd , du q. huynh , quan z. sheng venue international conference on management of data year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Publisher Summary  The Hippocratic Oath has guided the conduct of physicians for centuries. Inspired by its tenet of preserving privacy, it has been argued that future database systems must include responsibility for the privacy of data that they manage as a founding tenet. The explosive progress in networking, storage, and processor technologies is resulting in an unprecedented amount of digitization of information. It is estimated that the amount of information in the world is doubling every 20 months, and the size and number of databases are increasing even faster. In concert with this dramatic and escalating increase in digital data, concerns about the privacy of personal information have emerged globally. Privacy issues have been further exacerbated, now that the Internet makes it easy for new data to be automatically collected and added to databases. Privacy is the fight of individuals to determine for themselves when, how, and to what extent information about them is communicated to others. Privacy concerns are being fueled by an ever-increasing list of privacy violations, ranging from privacy accidents to illegal actions. Lax security for sensitive data is of equal concern.  Entity 2: title petabyte databases authors dirk d &#252; llmann venue international conference on management of data year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight \"quality\" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.  Entity 2: title reminiscences on influential papers authors kenneth a. ross venue acm sigmod record year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study  Entity 2: title data grid management systems authors arun jagatheesan , arcot rajasekar venue international conference on management of data year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Service composition is gaining momentum as the potential silver bullet for the envisioned Semantic Web. It purports to take the Web to unexplored efficiencies and provide a flexible approach for promoting all types of activities in tomorrow\u2019s Web. Applications expected to heavily take advantage of Web service composition include B2B E-commerce and E-government. To date, enabling composite services has largely been an ad hoc, time-consuming, and error-prone process involving repetitive low-level programming. In this paper, we propose an ontology-based framework for the automatic composition of Web services. We present a technique to generate composite services from high-level declarative descriptions. We define formal safeguards for meaningful composition through the use of composability rules. These rules compare the syntactic and semantic features of Web services to determine whether two services are composable. We provide an implementation using an E-government application offering customized services to indigent citizens. Finally, we present an exhaustive performance experiment to assess the scalability of our approach.  Entity 2: title the grid : an application of the semantic web authors carole goble , david de roure venue acm sigmod record year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Active object oriented database management systems (AODBMS) are finding increasing application in different application domains and especially for cooperative and long duration activity management. In this paper, we propose a concurrency control mechanism for open nested transactions in an AODBMS. It exploits the semantics of the transactions to achieve controlled cooperation and concurrency among the transactions. Atomic AODBMS transactions are treated as base transactions. A complex transaction type is formed from a collection of base and complex transactions, a set of detached mode ECA rules and a state transition model. The cooperation semantics of a complex transaction type with other complex transaction types is specified by associating with each state of a complex transaction, a set of cooperating complex transaction types.  Entity 2: title the ores temporal database management system authors babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: XML is becoming the universal format for data exchange between applications. Recently, the emergence of Web services as standard means of publishing and accessing data on the Web introduced a new class of XML documents, which we call intensional documents. These are XML documents where some of the data is given explicitly while other parts are defined only intensionally by means of embedded calls to Web services.When such documents are exchanged between applications, one has the choice of whether or not to materialize the intensional data (i.e., to invoke the embedded calls) before the document is sent. This choice may be influenced by various parameters, such as performance and security considerations. This article addresses the problem of guiding this materialization process.We argue that---like for regular XML data---schemas (\u00e0 la DTD and XML Schema) can be used to control the exchange of intensional data and, in particular, to determine which data should be materialized before sending a document, and which should not. We formalize the problem and provide algorithms to solve it. We also present an implementation that complies with real-life standards for XML data, schemas, and Web services, and is used in the Active XML system. We illustrate the usefulness of this approach through a real-life application for peer-to-peer news exchange.  Entity 2: title querying xml views of relational data authors jayavel shanmugasundaram , jerry kiernan , eugene j. shekita , catalina fan , john funderburk venue very large data bases year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Data mining on large data warehouses is becoming increasingly important. In support of this trend, we consider a spectrum of architectural alternatives for coupling mining with database systems. These alternatives include: loose-coupling through a SQL cursor interface; encapsulation of a mining algorithm in a stored procedure; caching the data to a file system on-the-fly and mining; tight-coupling using primarily user-defined functions; and SQL implementations for processing in the DBMS. We comprehensively study the option of expressing the mining algorithm in the form of SQL queries using Association rule mining as a case in point. We consider four options in SQL-92 and six options in SQL enhanced with object-relational extensions (SQL-OR). Our evaluation of the different architectural alternatives shows that from a performance perspective, the Cache-Mine option is superior, although the performance of the SQL-OR option is within a factor of two. Both the Cache-Mine and the SQL-OR approaches incur a higher storage penalty than the loose-coupling approach which performance-wise is a factor of 3 to 4 worse than Cache-Mine. The SQL-92 implementations were too slow to qualify as a competitive option. We also compare these alternatives on the basis of qualitative factors like automatic parallelization, development ease, portability and inter-operability.  Entity 2: title integration of data mining with database technology authors amir netz , surajit chaudhuri , jeff bernhardt , usama m. fayyad venue very large data bases year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We examine how to apply the hash-join paradigm to spatial joins, and define a new framework for spatial hash-joins. Our spatial partition functions have two components: a set of bucket extents and an assignment function, which may map a data item into multiple buckets. Furthermore, the partition functions for the two input datasets may be different.We have designed and tested a spatial hash-join method based on this framework. The partition function for the inner dataset is initialized by sampling the dataset, and evolves as data are inserted. The partition function for the outer dataset is immutable, but may replicate a data item from the outer dataset into multiple buckets. The method mirrors relational hash-joins in other aspects. Our method needs no pre-computed indices. It is therefore applicable to a wide range of spatial joins.Our experiments show that our method outperforms current spatial join algorithms based on tree matching by a wide margin.  Entity 2: title spatial joins using seeded trees authors ming-ling lo , chinya v. ravishankar venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: QuickStore is a memory-mapped storage system for persistent C++, built on top of the EXODUS Storage Manager. QuickStore provides fast access to in-memory objects by allowing application programs to access objects via normal virtual memory pointers. This article presents the results of a detailed performance study using the OO7 benchmark. The study compares the performance of QuickStore with the latest implementation of the E programming language. The QuickStore and E systems exemplify the two basic approaches (hardware and software) that have been used to implement persistence in object-oriented database systems. In addition, both systems use the same underlying storage manager and compiler, allowing us to make a truly apples-to-apples comparison of the hardware and software techniques.  Entity 2: title database indexing for large dna and protein sequence collections authors ela hunt , malcolm p. atkinson , robert w. irving venue the vldb journal -- the international journal on very large data bases year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We present novel algorithms for the problem of using materialized views to compute answers to SQL queries with grouping and aggregation, in the presence of multiset tables. ln addition to its obvious potential in query optimization, this problem is important in many applications, such as data warehousing, very large transaction recording systems, global information systems and mobile computing, where access to local or cached materialized views may be cheaper than access to the underlying database. Our contributions are the following: First, we show that in the case where the query has grouping and aggregation but the views do not, a view is usable in answering a query only if there is an isomorphism between the view and a portion of the query. Second, when the views also have grouping and aggregation we identify conditions under which the aggregation information present in a view is sufficient to perform the aggregation computations requited in the query. The algorithms we describe for rewriting a query also consider the case in which the rewritten query may be a union of single-block queries. Our approach is a semantic one, in that it detects when the information existing in a view is sufficient to answer a query. In contrast, previous work performed syntactic transformations on the query such that the definition of the view would be a sub-part of the definition of the query. Consequently, these methods can only detect usages of views in limited cases.  Entity 2: title the gmap : a versatile tool for physical data independence authors odysseas g. tsatalos , marvin h. solomon , yannis e. ioannidis venue the vldb journal -- the international journal on very large data bases year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Computing multidimensional aggregates in high dimensions is a performance bottleneck for many OLAP applications. Obtaining the exact answer to an aggregation query can be prohibitively expensive in terms of time and/or storage space in a data warehouse environment. It is advantageous to have fast, approximate answers to OLAP aggregation queries.  Entity 2: title on the computation of multidimensional aggregates authors sameet agarwal , rakesh agrawal , prasad deshpande , ashish gupta , jeffrey f. naughton , raghu ramakrishnan , sunita sarawagi venue very large data bases year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way.  Entity 2: title a cost model for clustered object-oriented databases authors georges gardarin , jean-robert gruser , zhao-hui tang venue very large data bases year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The analysis of time series in financial and scientific applications requires database functionality with complex specialized modeling capabilities and at the same time an easy-to-use interface. We present the time series management system CALANDA which combines both, a powerful dedicated data model and an intuitive GUI. The focus of this paper and the demonstration is to show how CALANDA is accessed by end users.  Entity 2: title dwms : data warehouse management system authors narendra mohan venue very large data bases year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Non-conventional database management systems are used to achieve a better performance when dealing with complex data. One fundamental concept of these systems is object identity (OID), because each object in the database has a unique identifier that is used to access and reference it in relationships to other objects. Two approaches can be used for the implementation of OIDs: physical or logical OIDs. In order to manage complex data, was proposed the Multimedia Data Manager Kernel (NuGeM) that uses a logical technique, named Indirect Mapping. This paper proposes an improvement to the technique used by NuGeM, whose original contribution is management of OIDs with a fewer number of disc accesses and less processing, thus reducing management time from the pages and eliminating the problem with exhaustion of OIDs. Also, the technique presented here can be applied to others OODBMSs.  Entity 2: title an evaluation of generic bulk loading techniques authors jochen van den bercken , bernhard seeger venue very large data bases year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.  Entity 2: title query unnesting in object-oriented databases authors leonidas fegaras venue international conference on management of data year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Content-based retrieval of images is the ability to retrieve images that are similar to a query image. Oracle8i Visual Information Retrieval provides this facility based on technology licensed from Virage, Inc. This product is built on top of Oracle8i interMedia which enables storage, retrieval and management of images, audios and videos. Images are matched using attributes such as color, texture and structure and efficient content-based retrieval is provided using indexes of an image index type. The design of the index type is based on a multi-level filtering algorithm. The filters reduce the search space so that the expensive comparison algorithm operates on a small subset of the data. Bitmap indexes are used to evaluate the first filter resulting in a design which performs well and is scalable. The image index type is built using Oracle8i extensible indexing technology, allowing users to create, use, and drop instances of this index type as they would any other standard index. In this paper we present an overview of the product, the design of the image index type, and some performance results of our product.  Entity 2: title checkpointing in oracle authors ashok joshi , william bridge , juan loaiza , tirthankar lahiri venue very large data bases year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Similarity search in large time series databases has attracted much research interest recently. It is a difficult problem because of the typically high dimensionality of the data. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and the Discrete Wavelet Transform (DWT). In this article, we introduce a new dimensionality reduction technique, which we call Adaptive Piecewise Constant Approximation (APCA). While previous techniques (e.g., SVD, DFT and DWT) choose a common representation for all the items in the database that minimizes the global reconstruction error, APCA approximates each time series by a set of constant value segments of varying lengths such that their individual reconstruction errors are minimal. We show how APCA can be indexed using a multidimensional index structure. We propose two distance measures in the indexed space that exploit the high fidelity of APCA for fast searching: a lower bounding Euclidean distance approximation, and a non-lower-bounding, but very tight, Euclidean distance approximation, and show how they can support fast exact searching and even faster approximate searching on the same index structure. We theoretically and empirically compare APCA to all the other techniques and demonstrate its superiority.  Entity 2: title dimensionality reduction for similarity searching in dynamic databases authors k. v. ravi kanth , divyakant agrawal , ambuj singh venue international conference on management of data year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In a database to which data is continually added, users may wish to issue a permanent query and be notified whenever data matches the query. If such continuous queries examine only single records, this can be implemented by examining each record as it arrives. This is very efficient because only the incoming record needs to be scanned. This simple approach does not work for queries involving joins or time. The Tapestry system allows users to issue such queries over a database of mail and bulletin board messages. The user issues a static query, such as \u201cshow me all messages that have been replied to by Jones,\u201d as though the database were fixed and unchanging. Tapestry converts the query into an incremental query that efficiently finds new matches to the original query as new messages are added to the database. This paper describes the techniques used in Tapestry, which do not depend on triggers and thus be implemented on any commercial database that supports SQL. Although Tapestry is designed for filtering mail and news messages, its techniques are applicable to any append-only database.  Entity 2: title an experimental object-based sharing system for networked databases authors doug fang , shahram ghandeharizadeh , dennis mcleod venue the vldb journal -- the international journal on very large data bases year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In existing relational database systems, processing of group-by and computation of aggregate functions are always postponed until all joins are performed. In this paper, we present transformations that make it possible to push group-by operation past one or more joins and can potentially reduce the cost of processing a query significantly. Therefore, the placement of group-by should be decided based on cost estimation. We explain how the traditional System-R style optimizers can be modified by incorporating the greedy conservative heuristic that we developed. We prove that applications of greedy conservative heuristic produce plans that are better (or no worse) than the plans generated by a traditional optimizer. Our experimental study shows that the extent of improvement in the quality of plans is significant with only a modest increase in optimization cost. Our technique also applies to optimization of Select Distinct queries by pushing down duplicate elimination in a cost-based fashion.  Entity 2: title exploiting statistics on query expressions for optimization authors nicolas bruno , surajit chaudhuri venue international conference on management of data year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: RasDaMan is a universal \u2014 i.e., domain-independent \u2014 array DBMS for multidimensional arrays of arbitrary size and structure. A declarative, SQL-based array query language offers flexible retrieval and manipulation. Efficient server-based query evaluation is enabled by an intelligent optimizer and a streamlined storage architecture based on flexible array tiling and compression.  Entity 2: title the mariposa distributed database management system authors jeff sidell venue acm sigmod record year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The University of Ulm was founded in 1967 with focus on medicine and natural sciences. In 1989 the University established two new faculties: Engineering Sciences and Computer Science. This enlargement took place within the framework of the so-called Science City Ulm. In a joint effort, the State of Baden-W\u00fcrttemberg, industrial companies, the University, and the City of Ulm successfully established a research and development infrastructure at or nearby the university campus consisting of the university's research labs, university-related research institutes like the Research Institute for Applied Knowledge Processing (FAW), and industrial research and development labs, especially a large research center of Daimler-Benz AG.  Entity 2: title research in database engineering at the university of namur authors jean-luc hainaut venue acm sigmod record year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We introduce the problem of mining generalized association rules. Given a large database of transactions, where each transaction consists of a set of items, and a taxonomy (is-a hierarchy) on the items, we find associations between items at any level of the taxonomy. For example, given a taxonomy that says that jackets is-a outerwear is-e clothes, we may infer a rule that \u201cpeople who buy outerwear tend to buy shoes\u201d. This rule may hold even if rules that \u201cpeople who buy jackets tend to buy shoes\u201d, and \u201cpeople who buy clothes tend to buy shoes\u201d do not hold. An obvious solution to the problem is to add all ancestors of each item in a transaction to the transaction, and then run any of the algorithms for mining association rules on these \u201cextended transactions\u201d . However, this \u201cBasic\u201d algorithm is not very fast; we present two algorithms, Cumulate and EstMerge, which run 2 to 5 times faster than Basic (and more than 100 times faster on one real-life dataset). We also present a new interest-measure for rules which uses the information in the taxonomy. Given a user-specified \u201cminimum-interest-level\u201d, this measure prunes a large number of redundant rules; 40% to 60% of all the rules were pruned on two real-life datasets.  Entity 2: title on the discovery of interesting patterns in association rules authors sridhar ramaswamy , sameer mahajan , abraham silberschatz venue very large data bases year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Multi-tier infrastructures have become common practice for implementing high volume web sites. Such infrastructures typically contain TCP load balancers, HTTP servers, application servers, transaction-processing monitors, and databases. Caching has been widely used at different layers of the infrastructure stack to improve scalability and response time of e-business applications. The majority of existing caching mechanisms target only static HTML pages or page fragments. However, as web applications become more dynamic through increased personalization, these caching techniques turn out to be less useful. Consequently, as more application requests result in increased querying and updating of backend database servers, scalability limits are often reached.  Entity 2: title middle-tier database caching for e-business authors qiong luo , sailesh krishnamurthy , c. mohan , hamid pirahesh , honguk woo , bruce g. lindsay , jeffrey f. naughton venue international conference on management of data year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The field of database systems research and development has been enormously successful over its 30 year history. It has led to a $10 billion industry with an installed base that touches virtually every major company in the world. It would be unthinkable to manage the large volume of valuable information that keeps corporations running without support from commercial database management systems (DBMS).  Entity 2: title database systems management and oracle8 authors c. gregory doherty venue international conference on management of data year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.  Entity 2: title olap , relational , and multidimensional database systems authors george colliat venue acm sigmod record year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Global information systems have the potential of providing decision makers with timely spatial information about earth systems. This information will come from diverse sources, including field monitoring, remotely sensed imagery, and environmental models. Of the three the latter has the greatest potential of providing regional and global scale information on the behavior of environmental systems, which may be vital for setting multi-governmental policy and for making decisions that are critical to quality of life. However, environmental models have limited prootocol for quality control and standardization. They tend to have weak or poorly defined semantics and so their output is often difficult to interpret outside a very limited range of applications for which they are designed. This paper considers this issue with respect to spatially distributed environmental models. A method of measuring the semantic proximity between components of large, integrated models is presented, along with an example illustrating its application. It is concluded that many of the issues associated with weak model semantics can be resolved with the addition of self-evaluating logic and context-based tools that present the semantic weaknesses to the end-user.  Entity 2: title uis-management of data and services in the environmental information systems of baden-w &#252; rttemberg authors wolf-fritz riekert , roland mayer-f &#246; ll , gerlinde wiest venue acm sigmod record year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Relational queries on continuous streams of data are the subject of many recent database research projects. In 1998 a small group of people started a similar project with the goal to transform our product, NonStop SQL/MX, into an active RDBMS. This project tried to integrate functionality of transactional queuing systems with relational tables and with SQL, using simple extensions to the SQL syntax and guaranteeing clearly defined query and transactional semantics. The result is the first commercially available RDBMS that incorporates streams. All data flowing through the system is contained in relational tables and is protected by ACID transactions. Insert and update operations on any NonStop SQL table can be considered publishing of data and can therefore be transparent to the (legacy) applications performing them.  Entity 2: title telegraphcq : continuous dataflow processing authors sirish chandrasekaran , owen cooper , amol deshpande , michael j. franklin , joseph m. hellerstein , wei hong , sailesh krishnamurthy , samuel r. madden , fred reiss , mehul a. shah venue international conference on management of data year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper, we introduce an approach that supports querying for Semantic Associations on the Semantic Web. Semantic Associations capture complex relationships between entities involving sequences of predicates, and sets of predicate sequences that interact in complex ways. Detecting such associations is at the heart of many research and analytical activities that are crucial to applications in national security and business intelligence. This in combination with the improving ability to identify entities in documents as part of automatic semantic annotation, gives a very powerful capability for semantic analysis of large amounts of heterogeneous content.  Entity 2: title managing multiple and distributed ontologies on the semantic web authors a. maedche , b. motik , l. stojanovic venue the vldb journal -- the international journal on very large data bases year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Integrating data and knowledge from multiple heterogeneous sources -- like databases, knowledge bases or specific software packages -- is often required for answering certain queries. Recently, a powerful framework for defining mediated views spanning multiple knowledge bases by a set of constrained rules was proposed [24, 4, 16]. We investigate the materialization of these views by unfolding the view definition and the efficient maintenance of the resulting materialized mediated view in case of updates. Thereby, we consider two kinds of updates: updates to the view and updates to the underlying sources. For each of these two cases several efficient algorithms maintaining materialized mediated views are given. We improve on previous algorithms like the DRed algorithm [12] and introduce a new fixpoint operator WP which -- opposed to the standard fixpoint operator TP [9] -- allows us to correctly capture the update's semantics without any recomputation of the materialized view.  Entity 2: title incremental maintenance of views with duplicates authors timothy griffin , leonid libkin venue international conference on management of data year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Authors and publishers who wish their publications to be considered for review in Computational Linguistics should send a copy to the book review editor, Graeme Hirst, Department of Computer Science, University of Toronto, Toronto, Canada M5S 3G4. All relevant books received will be listed, but not all can be reviewed. Technical reports (other than dissertations) will not be listed or reviewed. Authors should be aware that some publishers will not send books for review (even when instructed to do so); authors wishing to inquire as to whether their book has been received for review may contact the book review editor.  Entity 2: title combining fuzzy information : an overview authors ronald fagin venue acm sigmod record year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Querying large numbers of data sources is gaining importance due to increasing numbers of independent data providers. One of the key challenges is executing queries on all relevant information sources in a scalable fashion and retrieving fresh results. The key to scalability is to send queries only to the relevant servers and avoid wasting resources on data sources which will not provide any results. Thus, a catalog service, which would determine the relevant data sources given a query, is an essential component in efficiently processing queries in a distributed environment. This paper proposes a catalog framework which is distributed across the data sources themselves and does not require any central infrastructure. As new data sources become available, they automatically become part of the catalog service infrastructure, which allows scalability to large numbers of nodes. Furthermore, we propose techniques for workload adaptability. Using simulation and real-world data we show that our approach is valid and can scale to thousands of data sources.  Entity 2: title query caching and optimization in distributed mediator systems authors s. adali , k. s. candan , y. papakonstantinou , v. s. subrahmanian venue international conference on management of data year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The enhanced pay-per-view (EPPV) model for providing continuous-media-on-demand (CMOD) services associates with each continuous media clip a display frequency that depends on the clip\u2019s popularity. The aim is to increase the number of clients that can be serviced concurrently beyond the capacity limitations of available resources, while guaranteeing a constraint on the response time. This is achieved by sharing periodic continuous media streams among multiple clients. In this paper, we provide a comprehensive study of the resource scheduling problems associated with supporting EPPV for continuous media clips with (possibly) different display rates, frequencies, and lengths. Our main objective is to maximize the amount of disk bandwidth that is effectively scheduled under the given data layout and storage constraints. This formulation gives rise to -hard combinatorial optimization problems that fall within the realm of hard real-time scheduling theory. Given the intractability of the problems, we propose novel heuristic solutions with polynomial-time complexity. Preliminary results from an experimental evaluation of the proposed schemes are also presented.  Entity 2: title real-time database - similarity and resource scheduling authors tei-wei kuo , aloysius k. mok venue acm sigmod record year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The commoditization of high-performance networking has sparked research interest in the RDMA capability of this hardware. One-sided RDMA primitives, in particular, have generated substantial excitement due to the ability to directly access remote memory from within an application without involving the TCP/IP stack or the remote CPU. This article considers how to leverage RDMA to improve the analytical performance of parallel database systems. To shuffle data efficiently using RDMA, one needs to consider a complex design space  Entity 2: title towards self-tuning data placement in parallel database systems authors mong li lee , masaru kitsuregawa , beng chin ooi , kian-lee tan , anirban mondal venue international conference on management of data year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases, that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.  Entity 2: title opt + + : an object-oriented implementation for extensible database query optimization authors navin kabra , david j. dewitt venue the vldb journal -- the international journal on very large data bases year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The goal of STRUDEL project is to extend and adapt these concepts to the problem of Web-site management. Consider several tasks required of a Web-site manager. Site managers often want to manage a single repository of site data, but present different browsable \u201cviews\u201d of the site based on criteria such as the type of user accessing the site, e.g., external or internal, expert or novice. Morever, a manager might want to modify the data repository by editing simple text files or by updating external databases, to reorganize the structure of the pages by manipulating graphs that represent the linked pages, or to design multiple presentations of a single page by editing HTML files or by using a WYSIWYG HTML generator.  Entity 2: title open object database management systems authors jos &#233; a. blakeley venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases, that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.  Entity 2: title accessing a relational database through an object-oriented database interface authors jack a. orenstein , d. n. kamber venue very large data bases year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The goal of the Paradise project is to apply object-oriented and parallel database technology to the task of implementing a parallel GIS system capable of managing extremely large (multi-terabyte) data sets such as those that will be produced by the upcoming NASA EOSDIS project [Car92]. The project is focusing its resources on algorithms, processing, and storage techniques, and not on making new contributions to the data modeling, query language, or user interface domains.  Entity 2: title delaunay : a database visualization system authors isabel f. cruz , m. averbuch , wendy t. lucas , melissa radzyminski , kirby zhang venue international conference on management of data year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Over the last decades, improvements in CPU speed have outpaced improvements in main memory and disk access rates by orders of magnitude, enabling the use of data compression techniques to improve the performance of database systems. Previous work describes the benefits of compression for numerical attributes, where data is stored in compressed format on disk. Despite the abundance of string-valued attributes in relational schemas there is little work on compression for string attributes in a database context. Moreover, none of the previous work suitably addresses the role of the query optimizer: During query execution, data is either eagerly decompressed when it is read into main memory, or data lazily stays compressed in main memory and is decompressed on demand only  Entity 2: title revisiting commit processing in distributed database systems authors ramesh gupta , jayant haritsa , krithi ramamritham venue international conference on management of data year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We propose and evaluate two indexing schemes for improving the efficiency of data retrieval in high-dimensional databases that are incomplete. These schemes are novel in that the search keys may contain missing attribute values. The first is a multi-dimensional index structure, called the Bitstring-augmented R-tree (BR-tree), whereas the second comprises a family of multiple one-dimensional one-attribute (MOSAIC) indexes. Our results show that both schemes can be superior over exhaustive search.  Entity 2: title obtaining complete answers from incomplete databases authors alon y. levy venue very large data bases year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We investigate the problem of incremental maintenance of an SQL view in the face of database updates, and show that it is possible to reduce the total time cost of view maintenance by materializing (and maintaining) additional views. We formulate the problem of determining the optimal set of additional views to materialize as an optimization problem over the space of possible view sets (which includes the empty set). The optimization problem is harder than query optimization since it has to deal with multiple view sets, updates of multiple relations, and multiple ways of maintaining each view set for each updated relation.We develop a memoing solution for the problem; the solution can be implemented using the expression DAG representation used in rule-based optimizers such as Volcano. We demonstrate that global optimization cannot, in general, be achieved by locally optimizing each materialized subview, because common subexpressions between different materialized subviews can allow nonoptimal local plans to be combined into an optimal global plan. We identify conditions on materialized subviews in the expression DAG when local optimization is possible. Finally, we suggest heuristics that can be used to efficiently determine a useful set of additional views to materialize.Our results are particularly important for the efficient checking of assertions (complex integrity constraints) in the SQL-92 standard, since the incremental checking of such integrity constraints is known to be essentially equivalent to the view maintenance problem.  Entity 2: title incremental maintenance for materialized views over semistructured data authors serge abiteboul , jason mchugh , michael rys , vasilis vassalos , janet l. wiener venue very large data bases year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The discussions during the panel stayed largely but not entirely focused on the question of active database research issues from the application perspective. There were nine panelists. Each panelist was asked to prepare brief answers to a set of questions. The sets of answers were discussed by all participants, and finally a number of more general issues were discussed.  Entity 2: title advances in real-time database systems research authors azer bestavros venue acm sigmod record year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper we present a prototype system for the management of earth science data which is novel in that it takes a DBMS centric view of the the task. Our prototype -called \"BigSur\" -is shown in the context of its use by two geographically distributed scientific groups with demanding data storage and processing requirements. BigSur currently stores 1 Terabyte of data, about one thousandth of the volume EOSDIS must store. We claim that the design principles embodied in BigSur provide sufficient flexibility to achieve the difficult scientific and technical objectives of Mission to Planet Earth.  Entity 2: title aurora : a data stream management system authors d. abadi , d. carney , u. &#199; etintemel , m. cherniack , c. convey , c. erwin , e. galvez , m. hatoun , a. maskey , a. rasin , a. singer , m. stonebraker , n. tatbul , y. xing , r. yan , s. zdonik venue international conference on management of data year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Together with techniques developed for relational databases, this basis in logic means that deductive databases are capable of handling large amounts of information as well as performing reasoning based on that information. There are many application areas for deductive database technology. One area is that of decision support systems. In particular, the exploitation of an organization's resources requires fi~tbniy sufficient information about the current and future status of the resources themselves, but also a way of reasoning effectively about plans for the future. The present generation of decision support systems are severely deficient when it comes to reasoning about future plans. Deductive database technology is an appropriate solution to this problem. Another fruitful application area is that of expert systems. There are many computing applications in which there are large amounts of information, from which the important facts may be distilled by a simple yet tedious analysis. For example, medical analysis and monitoring can generate a large amount of data, and an error can have disastrous consequences.We demonstrate the COUGAR System, a new distributed data management infrastructure that scales with the growth of sensor interconnectivity and computational power on the sensors over the next decades. Our system resides directly on the sensor nodes and creates the abstraction of a single processing node without centralizing data or computation.  Entity 2: title introduction to constraint databases authors bart kuijpers venue acm sigmod record year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The rate of increase in database size and response-time requirements has outpaced advancements in processor and mass storage technology. One way to satisfy the increasing demand for processing power and input/output bandwidth in database applications is to have a number of processors, loosely or tightly coupled, serving database requests concurrently. Technologies developed during the last decade have made commercial parallel database systems a reality, and these systems have made an inroad into the stronghold of traditionally mainframe-based large database applications. This paper describes the DB2\u00ae Parallel Edition product that evolved from a prototype developed at IBM Research in Hawthorne, New York, and now is being jointly developed with the IBM Toronto laboratory.  Entity 2: title an overview of db2 parallel edition authors chaitanya baru , gilles fecteau venue international conference on management of data year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Content placement algorithm: An ACDN must decide which applications to deploy where and when. Content placement is solved trivially in traditional CDNs by cache replacement algorithms.  Entity 2: title influential papers authors ken ross venue acm sigmod record year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Conceptual data modeling for complex applications, such as multimedia and spatiotemporal applications, often results in large, complicated and difficult-to-comprehend diagrams. One reason for this is that these diagrams frequently involve repetition of autonomous, semantically meaningful parts that capture similar situations and characteristics. By recognizing such parts and treating them as units, it is possible to simplify the diagrams, as well as the conceptual modeling process. We propose to capture autonomous and semantically meaningful excerpts of diagrams that occur frequently as modeling patterns. Specifically, the paper concerns modeling patterns for conceptual design of spatiotemporal databases. Based on requirements drawn from real applications, it presents a set of modeling patterns that capture spatial, temporal, and spatiotemporal aspects. To facilitate the conceptual design process, these patterns are abbreviated by corresponding spatial, temporal, and spatiotemporal pattern abstractions, termed components. The result is more elegant and less-detailed diagrams that are easier to comprehend, but yet semantically rich. The Entity-Relationship model serves as the context for this study. An extensive example from a real cadastral application illustrates the benefits of using a component-based conceptual model.  Entity 2: title special issue on spatial database systems authors h. j. schek venue the vldb journal -- the international journal on very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We discovered a surprising law governing the spatial join selectivity across two sets of points. An example of such a spatial join is \u201cfind the libraries that are within 10 miles of schools\u201d. Our law dictates that the number of such qualifying pairs follows a power law, whose exponent we call \u201cpair-count exponent\u201d (PC). We show that this law also holds for self-spatial-joins (\u201cfind schools within 5 miles of other schools\u201d) in addition to the general case that the two point-sets are distinct. Our law holds for many real datasets, including diverse environments (geographic datasets, feature vectors from biology data, galaxy data from astronomy). In addition, we introduce the concept of the Box-Occupancy-Product-Sum (BOPS) plot, and we show that it can compute the pair-count exponent in a timely manner, reducing the run time by orders of magnitude, from quadratic to linear. Due to the pair-count exponent and our analysis (Law 1), we can achieve accurate selectivity estimates in constant time (O(1)) without the need for sampling or other expensive operations. The relative error in selectivity is about 30% with our fast BOPS method, and even better (about 10%), if we use the slower, quadratic method.  Entity 2: title selectivity estimation using probabilistic models authors lise getoor , benjamin taskar , daphne koller venue international conference on management of data year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Deductive databases generalize relational databases by providing support for recursive views and non-atomic data. Aditi is a deductive system based on the client-server model; it is inherently multi-user and capable of exploiting parallelism on shared-memory multiprocessors. The back-end uses relational technology for efficiency in the management of disk-based data and uses optimization algorithms especially developed for the bottom-up evaluation of logical queries involving recursion. The front-end interacts with the user in a logical language that has more expressive power than relational query languages. We present the structure of Aditi, discuss its components in some detail, and present performance figures.  Entity 2: title special issue on prototypes of deductive database systems authors k. ramamohanarao venue the vldb journal -- the international journal on very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Recently, similarity queries on feature vectors have been widely used to perform content-based retrieval of images. To apply this technique to large databases, it is required to develop multidimensional index structures supporting nearest neighbor queries efficiently. The SS-tree had been proposed for this purpose and is known to outperform other index structures such as the R*-tree and the K-D-B-tree. One of its most important features is that it employs bounding spheres rather than bounding rectangles for the shape of regions. However, we demonstrate in this paper that bounding spheres occupy much larger volume than bounding rectangles with high-dimensional data and that this reduces search efficiency. To overcome this drawback, we propose a new index structure called the SR-tree (Sphere/Rectangle-tree) which integrates bounding spheres and bounding rectangles. A region of the SR-tree is specified by the intersection of a bounding sphere and a bounding rectangle. Incorporating bounding rectangles permits neighborhoods to be partitioned into smaller regions than the SS-tree and improves the disjointness among regions. This enhances the performance on nearest neighbor queries especially for high-dimensional and non-uniform data which can be practical in actual image/video similarity indexing. We include the performance test results the verify this advantage of the SR-tree and show that the SR-tree outperforms both the SS-tree and the R*-tree.  Entity 2: title modeling high-dimensional index structures using sampling authors christian a. lang , ambuj k. singh venue international conference on management of data year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper provides an overview of the current database research activities within the Intelligent Information Systems Group in the Department of Computer Science and Engineering at Arizona State University. The focus of our research is on the integration of data and knowledge management issues, with specific emphasis on multimedia systems, object-oriented databases, active databases, deductive databases, and heterogeneous, distributed database environments.  Entity 2: title database research at nthu and itri authors arbee l. p. chen venue acm sigmod record year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.  Entity 2: title book reviews venue acm sigmod record year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The Multimedia Description Standard MPEG-7 is an International Standard since February 2002. It defines a huge set of description classes for multimedia content, for its creation and its communication. This article investigates what MPEG-7 means to Multimedia Database Systems (MMDBSs) and vice versa. We argue that MPEG-7 has to be considered complementary to, rather than competing with, data models employed in MMDBSs. Finally we show by an example scenario how these technologies can reasonably complement one another.  Entity 2: title concurrency control in hierarchical multidatabase systems authors sharad mehrotra , henry f. korth , avi silberschatz venue the vldb journal -- the international journal on very large data bases year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The Laboratory of Database Applications Engineering (LIBD) is devoted to the development of models, techniques, methods and tools to support all the engineering activities related to databases and their applications. It also develops material and activities to transfer database knowledge towards industry. This report describes the main activities of the laboratory during the last ten years. It first discusses general resources and processes that form the baselines for the other research activities. The latter will be classified into reverse engineering, interoperability, advanced processes and CASE technology.  Entity 2: title research in information managment at dublin city university authors mark roantree , alan f. smeaton venue acm sigmod record year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We present novel algorithms for the problem of using materialized views to compute answers to SQL queries with grouping and aggregation, in the presence of multiset tables. ln addition to its obvious potential in query optimization, this problem is important in many applications, such as data warehousing, very large transaction recording systems, global information systems and mobile computing, where access to local or cached materialized views may be cheaper than access to the underlying database. Our contributions are the following: First, we show that in the case where the query has grouping and aggregation but the views do not, a view is usable in answering a query only if there is an isomorphism between the view and a portion of the query. Second, when the views also have grouping and aggregation we identify conditions under which the aggregation information present in a view is sufficient to perform the aggregation computations requited in the query. The algorithms we describe for rewriting a query also consider the case in which the rewritten query may be a union of single-block queries. Our approach is a semantic one, in that it detects when the information existing in a view is sufficient to answer a query. In contrast, previous work performed syntactic transformations on the query such that the definition of the view would be a sub-part of the definition of the query. Consequently, these methods can only detect usages of views in limited cases.  Entity 2: title generating efficient plans for queries using views authors foto n. afrati , chen li , jeffrey d. ullman venue international conference on management of data year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Searching a database of 3D-volume objects for objects which are similar to a given 3D search object is an important problem which arises in number of database applications- for example, in Medicine and CAD. In this paper, we present a new geometrybased solution to the problem of searching for similar 3D-volume objects. The problem is motivated from a real application in the medical domain where volume similarity is used as a basis for surgery decisions. Our solution for an efficient similarity search on large databases of 3D volume objects is based on a new geometric index structure. The basic idea of our new approach is to use the concept of hierarchical approximations of the 3D objects to speed up the search process. We formally show the correctness of our new approach and introduce two instantiations of our general idea, which are based on cuboid and octree approximations. We finally provide a performance evaluation of our new index structure revealing significant performance improvements over existing approaches.  Entity 2: title fast parallel similarity search in multimedia databases authors stefan berchtold , christian b &#246; hm , bernhard braunm &#252; ller , daniel a. keim , hans-peter kriegel venue international conference on management of data year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper, we present a unifying framework called Rain Forest for classification tree construction that separates the scalability aspects of algorithms for constructing a tree from the central features that determine the quality of the tree. The generic algorithm is easy to instantiate with specific split selection methods from the literature (including C4.5, CART, CHAID, FACT, ID3 and extensions, SLIQ, SPRINT and QUEST).In addition to its generality, in that it yields scalable versions of a wide range of classification algorithms, our approach also offers performance improvements of over a factor of three over the SPRINT algorithm, the fastest scalable classification algorithm proposed previously. In contrast to SPRINT, however, our generic algorithm requires a certain minimum amount of main memory, proportional to the set of distinct values in a column of the input relation.  Entity 2: title boat-optimistic decision tree construction authors johannes gehrke , venkatesh ganti , raghu ramakrishnan , wei-yin loh venue international conference on management of data year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: While the number of database management systems (DBMSs) increases and the various DBMSs get more and more complex, no uniform method for DBMS construction exists. As a result, developers are forced to start more or less from scratch again for every desired system, resulting in a waste of time, effort, and cost. Hence, the database community is challenged with the development of an appropriate method, i.e. the time-saving application of engineering principles (e.g., reuse). Problems related to a construction method are described, as well as approaches towards solutions.  Entity 2: title on supporting containment queries in relational database management systems authors chun zhang , jeffrey naughton , david dewitt , qiong luo , guy lohman venue international conference on management of data year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The idea of building data warehouses as central data collections made available for decision support applications in a company is widely accepted. The concrete design and management of a data warehouse from a technical as well as from an organizational point of view, however, turns out to be far from trivial but requires sophisticated and time consuming efforts. The DMDW workshop was held at the CAiSE\u201999 conference in Heidelberg on June 14-15, 1999. It had the intention to bring together practitioners and researchers to discuss the design and management of data warehouses. The various presentations gave a broad view on the data warehouse life cycle covering aspects relevant at design time, at build time and at run time.  Entity 2: title dwms : data warehouse management system authors narendra mohan venue very large data bases year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This article presents a database programming language, Th\u00e9mis, which supports subtyping and class hierarchies, and allows for the definition of integrity constraints in a global and declarative way. We first describe the salient features of the language: types, names, classes, integrity constraints (including methods), and transactions. The inclusion of methods into integrity constraints allows an increase of the declarative power of these constraints. Indeed, the information needed to define a constraint is not always stored in the database through attributes, but is sometimes computed or derived data. Then, we address the problem of efficiently checking constraints.  Entity 2: title safe query languages for constraint databases authors peter z. revesz venue acm transactions on database systems ( tods ) year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: HYPERQUERY is a hypertext query language for object-oriented pictorial database systems. First, we discuss object calculus based on term rewriting. Then, example queries are used to illustrate language facilities. This query language has been designed with a flavor similar to QBE as the highly nonprocedural and conversational language for object-oriented pictorial database management system OISDBS.  Entity 2: title safe query languages for constraint databases authors peter z. revesz venue acm transactions on database systems ( tods ) year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O\u2019s state is loaded, similar state for other objects in O\u2019s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.  Entity 2: title structures for manipulating proposed updates in object-oriented databases authors michael doherty , richard hull , mohammed rupawalla venue international conference on management of data year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: QuickStore is a memory-mapped storage system for persistent C++, built on top of the EXODUS Storage Manager. QuickStore provides fast access to in-memory objects by allowing application programs to access objects via normal virtual memory pointers. This article presents the results of a detailed performance study using the OO7 benchmark. The study compares the performance of QuickStore with the latest implementation of the E programming language. The QuickStore and E systems exemplify the two basic approaches (hardware and software) that have been used to implement persistence in object-oriented database systems. In addition, both systems use the same underlying storage manager and compiler, allowing us to make a truly apples-to-apples comparison of the hardware and software techniques.  Entity 2: title quickstore : a high performance mapped object store authors seth j. white , david j. dewitt venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this article we consider a number of different possible options in the behavior of an active DBMS, based on a broad analysis of some of the best known implemented systems and prototypes. We encode these options in a user-readable form, called Extended ECA. A rule from any existing system can be rewritten in this formalism making all the semantic choices apparent. Then an EECA rule can be automatically translated into an internal (less readable) format, based on a logical style, which is called core format: the execution semantics of core rules is specified as the fixpoint of a simple transformation involving core rules. As an important premise to this research, a semantics for database updates and transactions has also been established, with respect to a notion of state that comprises both data and events. The article also presents an extensive bibliography on the subject of active databases.  Entity 2: title an approach for building secure database federations authors dirk jonscher , klaus r. dittrich venue very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The Grid is an emerging infrastructure for providing coordinated and consistent access to distributed, heterogeneous computational and information storage resources amongst autonomous organizations.    Data grids are being built across the world as the next generation data handling systems for sharing access to data and storage systems within multiple administrative domains. A data grid provides logical name spaces for digital entities and storage resources to create global identifiers that are location independent. Data grid systems provide services on the logical name space for the manipulation, management, and organization of digital entities.    Databases are increasingly being used within Grid applications for data and metadata management, and several groups are now developing services for the access and integration of structured data on the Grid. The service-based approach to making data available on the Grid is being encouraged by the adoption of the Open Grid Services Architecture (OGSA), which is bringing about the integration of the Grid with Web Service technologies.    The tutorial will introduce the Grid, and examine the requirements, issues and possible solutions for integrating data into the Grid. It will take examples from current systems, in particular the SDSC Storage Resource Broker and the OGSA-Database Access and Integration project.  Entity 2: title using the calanda time series management system authors werner dreyer , angelika kotz dittrich , duri schmidt venue international conference on management of data year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Currently gene expression data are being produced at a phenomenal rate. The general objective is to try to gain a better understanding of the functions of cellular tissues. In particular, one specific goal is to relate gene expression to cancer diagnosis, prognosis and treatment. However, a key obstacle is that the availability of analysis tools or lack thereof, impedes the use of the data, making it difficult for cancer researchers to perform analysis efficiently and effectively.  Entity 2: title an annotated bibliography on real-time database systems authors &#214; zg &#252; r ulusoy venue acm sigmod record year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The problem of indexing path queries in semistructured/XML databases has received considerable attention recently, and several proposals have advocated the use of structure indexes as supporting data structures for this problem. In this paper, we investigate efficient update algorithms for structure indexes. We study two kinds of updates -- the addition of a subgraph, intended to represent the addition of a new file to the database, and the addition of an edge, to represent a small incremental change. We focus on three instances of structure indexes that are based on the notion of graph bisimilarity. We propose algorithms to update the bisimulation partition for both kinds of updates and show how they extend to these indexes. Our experiments on two real world data sets show that our update algorithms are an order of magnitude faster than dropping and rebuilding the index. To the best of our knowledge, no previous work has addressed updates for structure indexes based on graph bisimilarity.  Entity 2: title a fast index for semistructured data authors brian cooper , neal sample , michael j. franklin , gisli r. hjaltason , moshe shadmon venue very large data bases year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Formulating queries on networked information systems is laden with problems: data diversity, data complexity, network growth, varied user base, and slow network access. This paper proposes a new approach to a network query user interface which consists of two phases: query preview and query refinement. This new approach is based on dynamic queries and tight coupling, guiding users to rapidly and dynamically eliminate undesired items, reduce the data volume to a manageable size, and refine queries locally before submission over a network. A two-phase dynamic query system for NASA's Earth Observing Systems--Data Information Systems (EOSDIS) is presented. The prototype was well received by the team of scientists who evaluated the interface.  Entity 2: title environmental information systems authors oliver g &#252; nther venue acm sigmod record year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Replication is often used in many distributed systems to provide a higher level of performance, reliability and availability. Lazy replica update protocols, which propagate updates to replicas through independent transactions after the original transaction commits, have become popular with database vendors due to their superior performance characteristics. However, if lazy protocols are used indiscriminately, they can result in non-serializable executions. In this paper, we propose two new lazy update protocols that guarantee serializability but impose a much weaker requirement on data placement than earlier protocols. Further, many naturally occurring distributed systems, like distributed data warehouses, satisfy this requirement. We also extend our lazy update protocols to eliminate all requirements on data placement. The extension is a hybrid protocol that propagates as many updates as possible in a lazy fashion. We implemented our protocols on the Datablitz database system product developed at Bell Labs. We also conducted an extensive performance study which shows that our protocols outperform existing protocols over a wide range of workloads.  Entity 2: title fast algorithms for maintaining replica consistency in lazy master replicated databases authors esther pacitti , pascale minet , eric simon venue very large data bases year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Recent results in the Rio project at the University of Michigan show that it is possible to create an area of main memory that is as safe as disk from operating system crashes. This paper explores how to integrate the reliable memory provided by the Rio file cache into a database system. Prior studies have analyzed the performance benefits of reliable memory; we focus instead on how different designs affect reliability. We propose three designs for integrating reliable memory into databases: non-persistent database buffer cache, persistent database buffer cache, and persistent database buffer cache with protection. Non-persistent buffer caches use an I/O interface to reliable memory and require the fewest modifications to existing databases. However, they waste memory capacity and bandwidth due to double buffering. Persistent buffer caches use a memory interface to reliable memory by mapping it into the database address space. This places reliable memory under complete database control and eliminates double buffering, but it may expose the buffer cache to database errors. Our third design reduces this exposure by write protecting the buffer pages. Extensive fault tests show that mapping reliable memory into the database address space does not significantly hurt reliability. This is because wild stores rarely touch dirty, committed pages written by previous transactions. As a result, we believe that databases should use a memory interface to reliable memory.  Entity 2: title query processing in tertiary memory databases authors sunita sarawagi venue very large data bases year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Two new algorithms, \u201cJive join\u201d and \u201cSlam join,\u201d are proposed for computing the join of two relations using a join index. The algorithms are duals: Jive join range-partitions input relation tuple ids and then processes each partition, while Slam join forms ordered runs of input relation tuple ids and then merges the results. Both algorithms make a single sequential pass through each input relation, in addition to one pass through the join index and two passes through a temporary file, whose size is half that of the join index. Both algorithms require only that the number of blocks in main memory is of the order of the square root of the number of blocks in the smaller relation.  Entity 2: title spatial joins using seeded trees authors ming-ling lo , chinya v. ravishankar venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Materialized views have been found to be very effective at speeding up queries, and are increasingly being supported by commercial databases and data warehouse systems. However, whereas the amount of data entering a warehouse and the number of materialized views are rapidly increasing, the time window available for maintaining materialized views is shrinking. These trends necessitate efficient techniques for the maintenance of materialized views.  Entity 2: title materialized view and index selection tool for microsoft sql server 2000 authors sanjay agrawal , surajit chaudhuri , vivek narasayya venue international conference on management of data year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: A warehouse is a repository of integrated information drawn from remote data sources. Since a warehouse effectively implements materialized views, we must maintain the views as the data sources are updated. This view maintenance problem differs from the traditional one in that the view definition and the base data are now decoupled. We show that this decoupling can result in anomalies if traditional algorithms are applied. We introduce a new algorithm, ECA (for \"Eager Compensating Algorithm\"), that eliminates the anomalies. ECA is based on previous incremental view maintenance algorithms, but extra \"compensating\" queries are used to eliminate anomalies. We also introduce two streamlined versions of ECA for special cases of views and updates, and we present an initial performance study that compares ECA to a view recomputation algorithm in terms of messages transmitted, data transferred, and I/O costs.  Entity 2: title performance issues in incremental warehouse maintenance authors wilburt labio , jun yang , yingwei cui , hector garcia-molina , jennifer widom venue very large data bases year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Many database applications need accountability and trace-ability that necessitate retaining previous database states. For a transaction-time database supporting this, the choice of times used to timestamp database records, to establish when records are or were current, needs to be consistent with a committed transaction serialization order. Previous solutions have chosen timestamps at commit time, selecting a time that agrees with commit order. However, SQL standard databases can require an earlier choice because a statement within a transaction may request \u201ccurrent time.\u201d Managing timestamps chosen before a serialization order is established is the challenging problem we solve here. By building on two-phase locking concurrency control, we can delay a transaction\u2019s choice of a timestamp, reducing the chance that transactions may need to be aborted in order keep timestamps consistent with a serialization order. Also, while timestamps stored with records in a transaction-time database make it possible to directly identify write-write and write-read conflicts, handling read-write conflicts requires more. Our simple auxiliary structure conservatively detects read-write conflicts, and hence provides transaction timestamps that are consistent with a serialization order.  Entity 2: title semantic assumptions and query evaluation in temporal databases authors claudio bettini , x. sean wang , elisa bertino , sushil jajodia venue international conference on management of data year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Information becomes a more and more valuable asset in today\u2019s organizations. Therefore the need of creating an integrated view over all available data sources arises. Several technical problems must be overcome in the design and implementation of a system for integrating different data sources. To the main obstacles count autonomy, data heterogeneity and different query capabilities of the repositories. This thesis presents the data integration system AMOS II , which is based on the wrapper-mediator approach. The main focus of this work lies on data model transformation and query processing. The following extensions to the AMOS II system are described in this thesis: \u2022 A framework for transforming various data models into the objectoriented model of AMOS II is presented. \u2022 The roles and tasks of wrappers are described. In particular their participation in query processing and query optimization is discussed. \u2022 A way for describing and utilizing the query capabilities of the different data sources is proposed. \u2022 Two different approaches to query processing over external data sources are developed and analyzed. All the proposed techniques are implemented in the AMOS II system, which runs on a Windows NT platform.  Entity 2: title answering xml queries on heterogeneous data sources authors ioana manolescu , daniela florescu , donald kossmann venue very large data bases year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper describes GnatDb, which is an embedded database system that provides protection against both accidental and malicious corruption of data. GnatDb is designed to run on a wide range of appliances, some of which have very limited resources. Therefore, its design is heavily driven by the need to reduce resource consumption. GnatDb employs atomic and durable updates to protect the data against accidental corruption. It prevents malicious corruption of the data using standard cryptographic techniques that leverage the underlying log-structured storage model. We show that the total memory consumption of GnatDb, which includes the code footprint, the stack and the heap, does not exceed 11 KB, while its performance on a typical appliance platform remains at an acceptable level.  Entity 2: title metu interoperable database system authors a. dogac , c. dengi , e. kilic , g. ozhan , f. ozcan , s. nural , c. evrendilek , u. halici , b. arpinar , p. koksal , n. kesim , s. mancuhan venue acm sigmod record year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.  Entity 2: title repositories and object oriented databases authors philip a. bernstein venue acm sigmod record year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The Relational On-Line Analytical Processing (ROLAP) is emerging as the dominant approach in data warehousing. In order to enhance query performance, the ROLAP approach relies on selecting and materializing in summary tables appropriate subsets of aggregate views which are then engaged in speeding up OLAP queries. However, a straight forward relational storage implementation of materialized ROLAP views is immensely wasteful on storage and incredibly inadequate on query performance and incremental update speed.  Entity 2: title an alternative storage organization for rolap aggregate views based on cubetrees authors yannis kotidis , nick roussopoulos venue international conference on management of data year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensiona...  Entity 2: title vxmlr : a visual xml-relational database system authors aoying zhou , hongjun lu , shihui zheng , yuqi liang , long zhang , wenyun ji , zengping tian venue very large data bases year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Some significant progress related to multidimensional data analysis has been achieved in the past few years, including the design of fast algorithms for computing datacubes, selecting some precomputed group-bys to materialize, and designing efficient storage structures for multidimensional data. However, little work has been carried out on multidimensional query optimization issues. Particularly the response time (or evaluation cost) for answering several related dimensional queries simultaneously is crucial to the OLAP applications. Recently, Zhao et al. first exploited this problem by presenting three heuristic algorithms. In this paper we first consider in detail two cases of the problem in which all the queries are either hash-based star joins or index-based star joins only. In the case of the hash-based star join, we devise a polynomial approximation algorithm which delivers a plan whose evaluation cost is $ O(n^{\\epsilon }$) times the optimal, where n is the number of queries and $ \\epsilon $ is a fixed constant with $0<\\epsilon \\leq 1$. We also present an exponential algorithm which delivers a plan with the optimal evaluation cost. In the case of the index-based star join, we present a heuristic algorithm which delivers a plan whose evaluation cost is n times the optimal, and an exponential algorithm which delivers a plan with the optimal evaluation cost. We then consider a general case in which both hash-based star-join and index-based star-join queries are included. For this case, we give a possible improvement on the work of Zhao et al., based on an analysis of their solutions. We also develop another heuristic and an exact algorithm for the problem. We finally conduct a performance study by implementing our algorithms. The experimental results demonstrate that the solutions delivered for the restricted cases are always within two times of the optimal, which confirms our theoretical upper bounds. Actually these experiments produce much better results than our theoretical estimates. To the best of our knowledge, this is the only development of polynomial algorithms for the first two cases which are able to deliver plans with deterministic performance guarantees in terms of the qualities of the plans generated. The previous approaches including that of [ZDNS98] may generate a feasible plan for the problem in these two cases, but they do not provide any performance guarantee, i.e., the plans generated by their algorithms can be arbitrarily far from the optimal one.  Entity 2: title a foundation for multi-dimensional databases authors marc gyssens , laks v. s. lakshmanan venue very large data bases year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: I first came across the AMS paper when I started getting interested in the data-streaming area, in the spring of 2001. Reading this paper was a real eye-opener for me. It was just amazing to see how simple randomization ideas and basic probabilistic tools (like the Chebyshev inequality and the Chernoff bound) can come together to provide elegant, space-efficient randomized approximation algorithms for estimation problems that, at first glance, would seem impossible to solve. The second-moment method described in the AMS paper is essentially the father of all \u201csketch-based\u201d techniques for data-stream management. Even though the idea of randomized linear projections (a.k.a., sketches) was known for some time in the domain of functional analysis (dating back to the famous Johnson-Lindenstrauss Lemma), Alon, Matias, and Szegedy were the first to exploit sketches for small-space data-stream computation, through the use of limited-independence random variates that can be constructed in small space and time. Of course, in addition to small-space sketching, the AMS paper also makes a number of other fundamental contributions in data streaming, including practical approximation algorithms for other frequency moments (e.g., the number of distinct values in a stream), as well as several inapproximability results (i.e., lower bounds) based on beautiful communication-complexity arguments.  Entity 2: title reminiscences on influential papers authors richard snodgrass venue acm sigmod record year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Events are at the core of reactive and proactive applications, which have become popular in many domains.  Entity 2: title conceptual modeling and specification generation for b2b business processes based on ebxml authors hyoungdo kim venue acm sigmod record year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We introduce a rich language of descriptions for semistructured tree-like data, and we explain how such descriptions relate to the data they describe. Various query languages and data schemas can b...  Entity 2: title storing semistructured data with stored authors alin deutsch , mary fernandez , dan suciu venue international conference on management of data year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Many real-time database applications arise in electronic financial services, safety-critical installations and military systems where enforcing is crucial to the success of the enterprise. We investigate here the performance implications, in terms of killed transactions, of guaranteeing multi-level secrecy in a real-time database system supporting applications with firm deadlines. In particular, we focus on the buffer management aspects of this issue.Our main contributions are the following. First, we identify the importance and difficulties of providing secure buffer management in the real-time database environment. Second, we present , a novel buffer management algorithm that provides covert-channel-free security. SABRE employs a fully dynamic one-copy allocation policy for efficient usage of buffer resources. It also incorporates several optimizations for reducing the overall number of killed transactions and for decreasing the unfairness in the distribution of killed transactions across security levels. Third, using a detailed simulation model, the real-time performance of SABRE is evaluated against unsecure conventional and real-time buffer management policies for a variety of security-classified transaction workloads and system configurations. Our experiments show that SABRE provides security with only a modest drop in real-time performance. Finally, we evaluate SABRE's performance when augmented with the GUARD adaptive admission control policy. Our experiments show that this combination provides close to ideal fairness for real-time applications that can tolerate covert-channel bandwidths of up to one bit per second (a limit specified in military standards).  Entity 2: title improving timeliness in real-time secure database systems authors sang h. son , rasikan david , bhavani thuraisingham venue acm sigmod record year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study  Entity 2: title the persistent cache : improving oid indexing in temporal object-oriented database systems authors kjetil n &#248; rv &#229; g venue very large data bases year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This is a beautifully simple paper that I feel encompasses many ideas that keep reappearing in different guises every decade or so! The paper proposes the replication of a dictionary (basically a set of key and value pairs) to all relevant sites in a distributed system. Updates and deletes are propagated in a lazy manner through the system as sites communicate with each other using a simple notion of a log. Queries are answered based on the local copy.  Entity 2: title reminiscences on influential papers authors richard snodgrass venue acm sigmod record year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Multiversion access methods have been emerged in the literature primarily to support queries on a transaction-time database where records are never physically deleted. For a popular class of efficient methods (including the multiversion Btree), data records and index entries are occasionally duplicated to separate data according to time. In this paper, we present techniques for improving query processing in multiversion access methods. In particular, we address the problem of avoiding duplicates in the response sets. We first discuss traditional approaches that eliminate duplicates using hashing and sorting. Next, we propose two new algorithms for avoiding duplicates without using additional data structures. The one performs queries in a depth-first order starting from a root, whereas the other exploits links between data pages. These methods are discussed in full details and their main properties are identitied. Preliminary performance results confirm the advantages of these methods in comparison to traditional ones according to CPU-time, disk accesses and storage.  Entity 2: title optimization techniques for queries with expensive methods authors joseph m. hellerstein venue acm transactions on database systems ( tods ) year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: It is striking that the optimization of disjunctive queries-i.e. those which contain at least one OR-connective in the query predicate-has been vastly neglected in the literature, as well as in commercial systems. In this paper, we propose a novel technique, called bypass processing, for evaluating such disjunctive queries. The bypass processing technique is based on new selection and join operators that produce two output streams: the TRUE-stream with tuples satisfying the selection (join) predicate and the FALSE-stream with tuples not satisfying the corresponding predicate. Splitting the tuple streams in this way enables us to \"bypass\" costly predicates whenever the \"fate\" of the corresponding tuple (stream) can be determined without evaluating this predicate. In the paper, we show how to systematically generate bypass evaluation plans utilizing a bottom-up building-block approach. We show that our evaluation technique allows us to incorporate the standard SQL semantics of null values. For this, we devise two different approaches: one is based on explicitly incorporating three-valued logic into the evaluation plans; the other one relies on two-valued logic by \"moving\" all negations to atomic conditions of the selection predicate. We describe how to extend an iterator-based query engine to support bypass evaluation with little extra overhead. This query engine was used to quantitatively evaluate the bypass evaluation plans against the traditional evaluation techniques utilizing a CNFor DNF-based query predicate.  Entity 2: title optimization techniques for queries with expensive methods authors joseph m. hellerstein venue acm transactions on database systems ( tods ) year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: XML data is likely to be widely used as a data exchange format but users also need to store and query XML data. The purpose of this panel is to explore whether and how to best provide this functionality.  Entity 2: title relational data sharing in peer-based data management systems authors beng chin ooi , yanfeng shu , kian-lee tan venue acm sigmod record year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: On the Semantic Web, data will inevitably come from many different ontologies, and information processing across ontologies is not possible without knowing the semantic mappings between them. Manually finding such mappings is tedious, error-prone, and clearly not possible on the Web scale. Hence the development of tools to assist in the ontology mapping process is crucial to the success of the Semantic Web. We describe GLUE, a system that employs machine learning techniques to find such mappings. Given two ontologies, for each concept in one ontology GLUE finds the most similar concept in the other ontology.  Entity 2: title agents , trust , and information access on the semantic web authors tim finin , anupam joshi venue acm sigmod record year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Knowledge based applications require linguistic, terminological and ontological resources. These applications are used to fulfill a set of tasks such as semantic indexing, knowledge extraction from text, information retrieval, etc. Using these resources and combining them for the same application is a tedious task with different levels of complexity. This requires their representation in a common language, extracting the required knowledge and designing effective large scale storage structures offering operators for resources management. For instance, ontology repositories were created to address these issues by collecting heterogeneous ontologies. They generally offer a more effective indexing of these resources than general search engines by generating alignments and annotations to ensure their interoperability.  Entity 2: title extracting schema from semistructured data authors svetlozar nestorov , serge abiteboul , rajeev motwani venue international conference on management of data year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Bioinformatics, the discipline concerned with biological information management is essential in the post-genome era, where the complexity of data processing allows for contemporaneous multi level research including that at the genome level, transcriptome level, proteome level, the metabolome level, and the integration of these -omic studies towards gaining an understanding of biology at the systems level. This research is also having a major impact on disease research and drug discovery, particularly through pharmacogenomics studies.  Entity 2: title opportunities in information management and assurance authors xiaolei qian venue acm sigmod record year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper introduces the Generalized Search Tree (GiST), an index structure supporting an extensible set of queries and data types. The GiST allows new data types to be indexed in a manner supporting queries natural to the types; this is in contrast to previous work on tree extensibility which only supported the traditional set of equality and range predicates. In a single data structure, the GiST provides all the basic search tree logic required by a database system, thereby unifying disparate structures such as B+-trees and R-trees in a single piece of code, and opening the application of search trees to general extensibility. To illustrate the flexibility of the GiST, we provide simple method implementations that allow it to behave like a B+-tree, an R-tree, and an RD-tree, a new index for data with set-valued attributes. We also present a preliminary performance analysis of RD-trees, which leads to discussion on the nature of tree indices and how they behave for various datasets.  Entity 2: title research issues for data communication in mobile ad-hoc network database systems authors leslie d. fife , le gruenwald venue acm sigmod record year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We confront the promises of active database systems with the result of their use by application developers. The main problems encountered are iusufficient methodological support in analysis and design, the lack of standardization, missing development and administration tools for triggers, and weak performance. We concentrate on performance because we discovered it is one the maiu reasons that makes users reluctant to use active rules iu the development of large applications. We show, using simple concrete examples, that optimizing large applications is rendered difficult by the separation of transactions and triggers and the misunderstanding of their subtle interactions. We argue that tools, which provide assistance to programmers, database administrators, and database designers to optimize their applications and master application evolution is strongly needed.  Entity 2: title naos - efficient and modular reactive capabilities in an object-oriented database system authors christine collet , thierry coupaye , t. svensen venue very large data bases year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Some significant progress related to multidimensional data analysis has been achieved in the past few years, including the design of fast algorithms for computing datacubes, selecting some precomputed group-bys to materialize, and designing efficient storage structures for multidimensional data. However, little work has been carried out on multidimensional query optimization issues. Particularly the response time (or evaluation cost) for answering several related dimensional queries simultaneously is crucial to the OLAP applications. Recently, Zhao et al. first exploited this problem by presenting three heuristic algorithms. In this paper we first consider in detail two cases of the problem in which all the queries are either hash-based star joins or index-based star joins only. In the case of the hash-based star join, we devise a polynomial approximation algorithm which delivers a plan whose evaluation cost is $ O(n^{\\epsilon }$) times the optimal, where n is the number of queries and $ \\epsilon $ is a fixed constant with $0<\\epsilon \\leq 1$. We also present an exponential algorithm which delivers a plan with the optimal evaluation cost. In the case of the index-based star join, we present a heuristic algorithm which delivers a plan whose evaluation cost is n times the optimal, and an exponential algorithm which delivers a plan with the optimal evaluation cost. We then consider a general case in which both hash-based star-join and index-based star-join queries are included. For this case, we give a possible improvement on the work of Zhao et al., based on an analysis of their solutions. We also develop another heuristic and an exact algorithm for the problem. We finally conduct a performance study by implementing our algorithms. The experimental results demonstrate that the solutions delivered for the restricted cases are always within two times of the optimal, which confirms our theoretical upper bounds. Actually these experiments produce much better results than our theoretical estimates. To the best of our knowledge, this is the only development of polynomial algorithms for the first two cases which are able to deliver plans with deterministic performance guarantees in terms of the qualities of the plans generated. The previous approaches including that of [ZDNS98] may generate a feasible plan for the problem in these two cases, but they do not provide any performance guarantee, i.e., the plans generated by their algorithms can be arbitrarily far from the optimal one.  Entity 2: title simultaneous optimization and evaluation of multiple dimensional queries authors yihong zhao , prasad m. deshpande , jeffrey f. naughton , amit shukla venue international conference on management of data year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The Capability Maturity Model [4] is an orderly way for organizations to determine the capabilities of their current processes for developing software and to establish priorities for improvement [2]. It defines five levels of progressively more mature process capability [3].  Entity 2: title book reviews authors karl aberer venue acm sigmod record year 2003 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We investigate the problem of using materialized views to answer SQL queries. We focus on modern decision-support queries, which involve joins, arithmetic operations and other (possibly user-defined) functions, aggregation (often along multiple dimensions), and nested subqueries. Given the complexity of such queries, the vast amounts of data upon which they operate, and the requirement for interactive response times, the use of materialized views (MVs) of similar complexity is often mandatory for acceptable performance. We present a novel algorithm that is able to rewrite a user query so that it will access one or more of the available MVs instead of the base tables. The algorithm extends prior work by addressing the new sources of complexity mentioned above, that is, complex expressions, multidimensional aggregation, and nested subqueries. It does so by relying on a graphical representation of queries and a bottom-up, pair-wise matching of nodes from the query and MV graphs. This approach offers great modularity and extensibility, allowing for the rewriting of a large class of queries.  Entity 2: title answering queries using views : a survey authors alon y. halevy venue the vldb journal -- the international journal on very large data bases year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: A well-known challenge in data warehousing is the efficient incremental maintenance of warehouse data in the presence of source data updates. In this paper, we identify several critical data representation and algorithmic choices that must be made when developing the machinery of an incrementally maintained data warehouse. For each decision area, we identify various alternatives and evaluate them through extensive experiments. We show that picking the right alternative can lead to dramatic performance gains, and we propose guidelines for making the right decisions under different scenarios. All of the issues addressed in this paper arose in our development of WHIPS, a prototype data warehousing system supporting incremental maintenance.  Entity 2: title maintenance of data cubes and summary tables in a warehouse authors inderpal singh mumick , dallan quass , barinderpal singh mumick venue international conference on management of data year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study  Entity 2: title bigsur : a system for the management of earth science data authors paul brown , michael stonebraker venue very large data bases year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this article we carefully define a query cost framework that incorporates both selectivity and cost estimates for selections. We develop an algorithm called Predicate Migration, and prove that it produces optimal plans for queries with expensive methods. We then describe our implementation of Predicate Migration in the commercial object-relational database management system Illustra, and discuss practical issues that affect our earlier assumptions. We compare Predicate Migration to a variety of simplier optimization techniques, and demonstrate that Predicate Migration is the best general solution to date. The alternative techniques we present may be useful for constrained workloads.  Entity 2: title query processing techniques for multiversion access methods authors jochen van den bercken , bernhard seeger venue very large data bases year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Data in relational databases is frequently stored and retrieved using B-Trees. In &cis,ion isugprt applications the key of the B-Tree frequently involves the concatenation of several fields of the relationdl\u2019 table. During retrieval, it is desirable to be able to access a small subset of the table based\u2019 on partial key information, where some fields of the key may either not be present, involve ranges, or lists \u2018of values. It is also advantageous to altow. this type, of access-with gen&il expressions involving any combination of disjuncts on key columns. This paper &scribes a method whereby BTrees can be eficiently used to retrieve small subsets, thus avoiding large scans of potentially huge tables. Another benefit is the ability of this method to reduce the need for additional secondary indexes, thus saving space, maintenance cost, and random accesses.  Entity 2: title efficient concurrency control in multidimensional access methods authors kaushik chakrabarti , sharad mehrotra venue international conference on management of data year 1999 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Deductive databases generalize relational databases by providing support for recursive views and non-atomic data. Aditi is a deductive system based on the client-server model; it is inherently multi-user and capable of exploiting parallelism on shared-memory multiprocessors. The back-end uses relational technology for efficiency in the management of disk-based data and uses optimization algorithms especially developed for the bottom-up evaluation of logical queries involving recursion. The front-end interacts with the user in a logical language that has more expressive power than relational query languages. We present the structure of Aditi, discuss its components in some detail, and present performance figures.  Entity 2: title xsb as an efficient deductive database engine authors konstantinos sagonas , terrance swift , david s. warren venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In the last few years, many active database models have been proposed. Some of them have been implemented as research prototypes. The use and study of these prototypes shows that it is difficult to get a clear idea of the proposed approaches and to compare them. More generally there are some unquestionable difficulties in understanding, reasoning about and teaching behavior of active database systems. We think there is a need for formal descriptions of the semantics of such systems in order to describe and to understand them with less ambiguities, to compare them and to come up with some progress in defining standard concepts and functionalities for active databases.  Entity 2: title the ecrc multi database system authors willem jonker , heribert sch &#252; tz venue international conference on management of data year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: The goal of the Paradise project is to apply object-oriented and parallel database technology to the task of implementing a parallel GIS system capable of managing extremely large (multi-terabyte) data sets such as those that will be produced by the upcoming NASA EOSDIS project [Car92]. The project is focusing its resources on algorithms, processing, and storage techniques, and not on making new contributions to the data modeling, query language, or user interface domains.  Entity 2: title dbcache : database caching for web application servers authors mehmet altinel , qiong luo , sailesh krishnamurthy , c. mohan , hamid pirahesh , bruce g. lindsay , honguk woo , larry brown venue international conference on management of data year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We consider data to be semistructured when there is no schema fixed or known in advance and when the data may be incomplete or irregular. For example, HTML files on the World-Wide Web usually contain some structure, but often the data is irregular or In addition, data integrated from multiple, heterogeneous information sources often is semistructured. Storing and querying semistructured data poses considerably different problems and requirements than those for traditional databases, where data storage and query processing are dependent upon structured data. Relational, nested-relational, and object-oriented database systems, for example, all depend upon the data having a known and regular schema.  Entity 2: title a fast index for semistructured data authors brian cooper , neal sample , michael j. franklin , gisli r. hjaltason , moshe shadmon venue very large data bases year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper we describe the design and implementation of OPT++, a tool for extensible database query optimization that uses an object-oriented design to simplify the task of implementing, extending, and modifying an optimizer. Building an optimizer using OPT++ makes it easy to extend the query algebra (to add new query algebra operators and physical implementation algorithms to the system), easy to change the search space, and also to change the search strategy. Furthermore, OPT++ comes equipped with a number of search strategies that are available for use by an optimizer-implementor. OPT++ considerably simplifies both, the task of implementing an optimizer for a new database system, and the task of evaluating alternative optimization techniques and strategies to decide what techniques are best suited for that database system. We present the results of a series of performance studies. These results validate our design and show that, in spite of its flexibility, OPT++ can be used to build efficient optimizers.  Entity 2: title cost-based optimization for magic : algebra and implementation authors praveen seshadri , joseph m. hellerstein , hamid pirahesh , t. y. cliff leung , raghu ramakrishnan , divesh srivastava , peter j. stuckey , s. sudarshan venue international conference on management of data year 1996 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We present in this paper a fully automatic content-based approach to organizing and indexing video data. Our methodology involves three steps:<ul><li>Step 1: We segment each video into shots using a Camera-Tracking technique. This process also extracts the feature vector for each shot, which consists of two statistical variances. The above three inter-related techniques offer an integrated framework for modeling, browsing, and searching large video databases. Our experimental results indicate that they have many advantages over existing methods.  Entity 2: title effective timestamping in databases authors kristian torp , christian s. jensen , richard thomas snodgrass venue the vldb journal -- the international journal on very large data bases year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: For as long as there have been DBMS's and applications that use them, there has been interest in the performance characteristics that these systems exhibit. This month's column describes some of the recent work that has taken place in TPC, the Transaction Processing Performance Council.TPC-A and TPC-B are obsolete benchmarks that you might have heard about in the past. TPC-C V3.5 is the current benchmark for OLTP systems. Introduced in 1992, it has been run on many hardware platforms and DBMS's. Indeed, the TPC web site currently lists 202 TPC-C benchmark results. Due to its maturity, TPC-C will not be discussed in this article.We've asked two very knowledgeable individuals to write this article. Meikel Poess is the chair of the TPC H and TPC-R Subcommittees and Chris Floyd is the chair of the TPC-W Subcommittee. We greatly appreciate their efforts.A wealth of information can be found at the TPC web site [ 1 ]. This information includes the benchmark specifications themselves, TPC membership information, and benchmark results.  Entity 2: title tpc-ds , taking decision support benchmarking to the next level authors meikel poess , bryan smith , lubor kollar , paul larson venue international conference on management of data year 2002 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: This paper describes the design and implementation of PEST0 (Portable Explorer of Snuctured Objects), a user interface that supports browsing and querying of object databases. PEST0 allows users to navigate the relationships that exist among objects. In addition, users can formulate complex object queries through an integrated query paradigm (\u201cquery-in-place\u201d) that presents querying as a natural extension of browsing. PEST0 is designed to be portable to any object database system that supports a high-level query language; in addition, PEST0 is extensible, providing hooks for specialized predicate formation and object display tools for new data types (e.g., images or text). uniformly and manipulated using an object-oriented dialect of SQL. One component of this project, which is joint work between IBM Almaden and the University of Wisconsin, is the development of a graphical user interface called PEST0 (Portable Explorer of STructured Objects). We refer to the PEST0 interface as a query/browser, as it marries navigational object browsing\u2019 with declarative querying; it integrates browsing and querying via a \u201cquery-in-place\u201d paradigm that provides a powerful yet natural user interface for exploring the contents of object databases.  Entity 2: title repositories and object oriented databases authors philip a. bernstein venue acm sigmod record year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: A frequently encountered type of query in Geographic Information Systems is to find the k nearest neighbor objects to a given point in space. Processing such queries requires substantially different search algorithms than those for location or range queries. In this paper we present an efficient branch-and-bound R-tree traversal algorithm to find the nearest neighbor object to a point, and then generalize it to finding the k nearest neighbors. We also discuss metrics for an optimistic and a pessimistic search ordering strategy as well as for pruning. Finally, we present the results of several experiments obtained using the implementation of our algorithm and examine the behavior of the metrics and the scalability of the algorithm.  Entity 2: title near neighbor search in large metric spaces authors sergey brin venue very large data bases year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensiona...  Entity 2: title materialized view selection for multidimensional datasets authors amit shukla , prasad deshpande , jeffrey f. naughton venue very large data bases year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: We show how careful interspersing of queries and informed cache management can achieve rema.rkable reductions in access time compared 1x1 conventional methods. Our algoril(hms use a few model pa.rameters for each tertiary memory device and are thus designed to be portable across a wide variety of tert,ia.ry memory devices and da,tnhase t,ypes. We arc extending the PoS\u2019TGR.ES database system to implements the new query processing strategics. Jnit,ial mea.surements on the prototype yield impressive results.  Entity 2: title query processing over object views of relational data authors gustav fahl , tore risch venue the vldb journal -- the international journal on very large data bases year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: A methodology of reengineering existing extended Entity-Relationship(EER) model to Object Modeling Technique(OMT) model is described. A set of translation rules from EER model to a generic Object-Oriented(OO) model of OMT methodology is devised. Such reengineering practices not only can provide us with significant insight to the \"interoperability\" between the OO and the traditional semantic modelling techniques, but also can lead us to the development of a practical design methodology for object-oriented databases(OODB).  Entity 2: title an extended entity-relationship model for geographic applications authors thanasis hadzilacos , nectaria tryfona venue acm sigmod record year 1997 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this paper, we consider a dynamic self-tuning approach to reorganization in a shared nothing system. We introduce a new index-based method that faciliates fast and efficient migration of data. Our solution incorporates a globally height-balanced structure and load tracking at different levels of granularity. We conducted an extensive performance study, and implemented the methods on the Fujitsu AP3000 machine. Both the simulation and empirical results demonstratic that our proposed method is indeed scalable and effective in correcting any deterioration in system throughput.  Entity 2: title rethinking database system architecture : towards a self-tuning risc-style database system authors surajit chaudhuri , gerhard weikum venue very large data bases year 2000 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Database researchers have made significant progress on several research issues related to multidimensional data analysis, including the development of fast cubing algorithms, efficient schemes for creating and maintaining precomputed group-bys, and the design of efficient storage structures for multidimensional data. However, to date there has been little or no work on multidimensional query optimization. Recently, Microsoft has proposed \u201cOLE DB for OLAP\u201d as a standard multidimensional interface for databases. OLE DB for OLAP defines Multi-Dimensional Expressions (MDX), which have the interesting and challenging feature of allowing clients to ask several related dimensional queries in a single MDX expression. In this paper, we present three algorithms to optimize multiple related dimensional queries. Two of the algorithms focus on how to generate a global plan from several related local plans. The third algorithm focuses on generating a good global plan without first generating local plans. We also present three new query evaluation primitives that allow related query plans to share portions of their evaluation. Our initial performance results suggest that the exploitation of common subtask evaluation and global optimization can yield substantial performance improvements when relational database systems are used as data sources for multidimensional analysis.  Entity 2: title orthogonal optimization of subqueries and aggregation authors c &#233; sar galindo-legaria , milind joshi venue international conference on management of data year 2001 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In this first article of the regular column on data base standardization activities, I give an overview of topic areas under active development in the formal national and international standardization bodies. I solicit contributions on these active topics so that standardizers and researchers can cooperate in the near term, before irreversible decisions are made, to produce the most useful and highest quality database standards.  Entity 2: title a language based multidatabase system authors eva k &#252; hn , thomas tschernko , konrad schwarz venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Although there have been many studies on data mining, to date there have been few research prototypes or commercial systems supporting comprehensive query-driven mining, which encourages interactive exploration of the data. Our thesis is that constraint constructs and the optimization they induce play a pivotal role in mining queries, thus substantially enhancing the usefulness and performance of the mining system. This is based on the analogy of declarative query languages like SQL and query optimization which have made relational databases so successful. To this end, our proposed demo is not yet another data mining system, but of a new paradigm in data mining - mining with constraints, as the important first step towards supporting ad-hoc mining in DBMS.  Entity 2: title exploratory mining and pruning optimizations of constrained associations rules authors raymond t. ng , laks v. s. lakshmanan , jiawei han , alex pang venue international conference on management of data year 1998 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: e consider the execution of multi-join queries in a hierarchical parallel system, i.e., a shared-nothing system whose nodes are shared-memory multiprocessors. In this context, load balancing must be addressed at two levels, locally among the processors of each shared-memory node and globally among all nodes. In this paper, we propose a dynamic execution model that maximizes local load balancing within shared-memory nodes and minimizes the need for load sharing across nodes. This is obtained by allowing each processor to execute any operator that can be processed locally, thereby taking full advantage of inter- and intra-operator parallelism. We conducted a performance evaluation using an implementation on a 72-processor KSR1 computer.  Entity 2: title dynamic multi-resource load balancing in parallel database systems authors erhard rahm , robert marek venue very large data bases year 1995 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: Research and development of multidatabase systems was triggered by the need to integrate data from heterogeneous and physically distributed information sources Issues on multidatabase architectures and semantic heterogeneity have been explored. This paper presents the multidatabase systems done and currently being done at De La Salle University. These research projects focus on multidatabase architectures, and on identifying methods in resolving various forms of schema conflicts.  Entity 2: title parallel database systems in the 1990 's authors michael j. carey venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    },
    {
        "instruction": "Do the following entities match? Answer with \"yes\" if they do and \"no\" if they do not.\n",
        "input": "Entity 1: In a mobile computing system, caching data items at the mobile clients is important to reduce the data access delay in an unreliable and low bandwidth mobile network. However, efficient methods must be used to ensure the coherence between the cached items and the data items at the database server. By exploring the real time properties of the data items, we propose a cache invalidation scheme called: Invalidation by Absolute Validity Interval (IAVI). We define an absolute validate interval (AVI) for each data item based on its real time property, e.g. update interval. A mobile client can verify the validity of a cached item by comparing the last update time and its AVI. A cached item is invalidated if the current time is greater than the last update time by its AVI. With this self-invalidation mechanism, the IAVI scheme uses the invalidation report to inform the mobile clients about the change of AVI rather than the update event of the data item. As a result, the size of invalidation report can be reduced significantly. Performance studies show that the IAVI scheme can significantly reduce the mean response time and invalidation report size under various system parameters.  Entity 2: title data replication for mobile computers authors yixiu huang , prasad sistla , ouri wolfson venue international conference on management of data year 1994 ",
        "output": "The answer is no",
        "answer": "no",
        "domain": "citation"
    }
]